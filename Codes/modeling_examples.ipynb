{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e613346-c162-4bf8-908d-8e6a5cff0746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, matthews_corrcoef, average_precision_score\n",
    "from pyod.utils.data import precision_n_scores\n",
    "from pyod.models.iforest import IForest\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Per l'uso della memoria degli algoritmi\n",
    "from memory_profiler import memory_usage\n",
    "# Per la metrica sul tempo di Addestramento e Inferenza\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba180484-282d-4605-990e-bf354cf53bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(y_test, y_pred, y_proba=None, digits=3):\n",
    "    res = {\"Accuracy\": round(accuracy_score(y_test, y_pred), digits),\n",
    "           \"Precision\": precision_score(y_test, y_pred).round(digits),\n",
    "           \"Recall\": recall_score(y_test, y_pred).round(digits),\n",
    "           \"F1\": f1_score(y_test, y_pred).round(digits),\n",
    "           \"MCC\": round(matthews_corrcoef(y_test, y_pred), ndigits=digits)}\n",
    "    if y_proba is not None:\n",
    "        res[\"AUC_PR\"] = average_precision_score(y_test, y_proba).round(digits)\n",
    "        res[\"AUC_ROC\"] = roc_auc_score(y_test, y_proba).round(digits)\n",
    "        res[\"PREC_N_SCORES\"] = precision_n_scores(y_test, y_proba).round(digits)\n",
    "    return res\n",
    "\n",
    "\n",
    "def set_seed_numpy(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc90bc09-5125-4641-bf52-cd0d4cc7a122",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"mean\", \"var\", \"std\", \"len\", \"duration\", \"len_weighted\", \"gaps_squared\", \"n_peaks\",\n",
    "    \"smooth10_n_peaks\", \"smooth20_n_peaks\", \"var_div_duration\", \"var_div_len\",\n",
    "    \"diff_peaks\", \"diff2_peaks\", \"diff_var\", \"diff2_var\", \"kurtosis\", \"skew\",\n",
    "]\n",
    "SEED = 2137"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fffebd2-36a4-4c5f-82e0-3b9e4f6c3ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/dataset.csv\", index_col=\"segment\")\n",
    "\n",
    "X_train, y_train = df.loc[df.train==1, features], df.loc[df.train==1, \"anomaly\"]\n",
    "X_test, y_test = df.loc[df.train==0, features], df.loc[df.train==0, \"anomaly\"]\n",
    "X_train_nominal = df.loc[(df.anomaly==0)&(df.train==1), features]\n",
    "\n",
    "prep = StandardScaler()\n",
    "X_train_nominal2 = prep.fit_transform(X_train_nominal)\n",
    "X_train2 = prep.transform(X_train)\n",
    "X_test2 = prep.transform(X_test)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5cb2f65-dba5-431f-a7c1-dc4db5d1fb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed_numpy(SEED) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938333f9",
   "metadata": {},
   "source": [
    "# Supervised Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "396d74e0-6e0c-40c2-beaf-ea856b6a22d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(random_state=2137) \n",
      " {'Accuracy': 0.934, 'Precision': 0.89, 'Recall': 0.788, 'F1': 0.836, 'MCC': 0.797, 'AUC_PR': 0.923, 'AUC_ROC': 0.962, 'PREC_N_SCORES': 0.841}\n"
     ]
    }
   ],
   "source": [
    "model = AdaBoostClassifier(random_state=SEED)\n",
    "model.fit(X_train2, y_train)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb2e8e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=3, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=50, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=2137, ...) \n",
      " {'Accuracy': 0.957, 'Precision': 0.959, 'Recall': 0.832, 'F1': 0.891, 'MCC': 0.867, 'AUC_PR': 0.961, 'AUC_ROC': 0.986, 'PREC_N_SCORES': 0.876}\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "y_train_np = y_train\n",
    "\n",
    "model = xgb.XGBClassifier (\n",
    "    n_estimators=50,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    random_state=SEED\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "# Previsioni e probabilità di previsione\n",
    "y_predicted = model.predict(X_test)\n",
    "y_predicted_score = model.predict_proba(X_test)[:, 1]  # Probabilità per la classe positiva\n",
    "# Questa è la probabilità che la classificazione sia corretta\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8aa6aa90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=3, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=50, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=2137, ...) \n",
      " {'Accuracy': 0.953, 'Precision': 0.94, 'Recall': 0.832, 'F1': 0.883, 'MCC': 0.856, 'AUC_PR': 0.949, 'AUC_ROC': 0.976, 'PREC_N_SCORES': 0.867}\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "y_train_np = y_train\n",
    "\n",
    "model = xgb.XGBClassifier (\n",
    "    n_estimators=50,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    random_state=SEED\n",
    ")\n",
    "model.fit(X_train_scaled, y_train)\n",
    "# Previsioni e probabilità di previsione\n",
    "y_predicted = model.predict(X_test_scaled)\n",
    "y_predicted_score = model.predict_proba(X_test_scaled)[:, 1]  # Probabilità per la classe positiva\n",
    "# Questa è la probabilità che la classificazione sia corretta\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f258319c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': 0.926, 'Precision': 0.911, 'Recall': 0.726, 'F1': 0.808, 'MCC': 0.771, 'AUC_PR': 0.73, 'AUC_ROC': 0.806, 'PREC_N_SCORES': 0.628}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\sklearn\\svm\\_classes.py:32: FutureWarning: The default value of `dual` will change from `True` to `'auto'` in 1.5. Set the value of `dual` explicitly to suppress the warning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\sklearn\\svm\\_base.py:1250: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Inizializza e addestra il modello\n",
    "model = LinearSVC()\n",
    "model.fit(X_train2, y_train)\n",
    "\n",
    "# Predizione\n",
    "y_test_scores = model.decision_function(X_test2)\n",
    "# Previsioni e probabilità di previsione\n",
    "y_predicted = model.predict(X_test2)\n",
    "\n",
    "# Questa è la probabilità che la classificazione sia corretta\n",
    "print(evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d90807a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': 0.924, 'Precision': 0.92, 'Recall': 0.708, 'F1': 0.8, 'MCC': 0.764, 'AUC_PR': 0.73, 'AUC_ROC': 0.806, 'PREC_N_SCORES': 0.628}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Inizializza e addestra il modello\n",
    "model = LogisticRegression(max_iter=500)\n",
    "model.fit(X_train2, y_train)\n",
    "\n",
    "# Predizione\n",
    "y_test_scores = model.decision_function(X_test2)\n",
    "# Previsioni e probabilità di previsione\n",
    "y_predicted = model.predict(X_test2)\n",
    "\n",
    "# Questa è la probabilità che la classificazione sia corretta\n",
    "print(evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c5f6a6",
   "metadata": {},
   "source": [
    "## Unsupervised Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b5d242",
   "metadata": {},
   "source": [
    "MO_GAAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "76d742e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 of 60\n",
      "Epoch 2 of 60\n",
      "Epoch 3 of 60\n",
      "Epoch 4 of 60\n",
      "Epoch 5 of 60\n",
      "Epoch 6 of 60\n",
      "Epoch 7 of 60\n",
      "Epoch 8 of 60\n",
      "Epoch 9 of 60\n",
      "Epoch 10 of 60\n",
      "Epoch 11 of 60\n",
      "Epoch 12 of 60\n",
      "Epoch 13 of 60\n",
      "Epoch 14 of 60\n",
      "Epoch 15 of 60\n",
      "Epoch 16 of 60\n",
      "Epoch 17 of 60\n",
      "Epoch 18 of 60\n",
      "Epoch 19 of 60\n",
      "Epoch 20 of 60\n",
      "Epoch 21 of 60\n",
      "Epoch 22 of 60\n",
      "Epoch 23 of 60\n",
      "Epoch 24 of 60\n",
      "Epoch 25 of 60\n",
      "Epoch 26 of 60\n",
      "Epoch 27 of 60\n",
      "Epoch 28 of 60\n",
      "Epoch 29 of 60\n",
      "Epoch 30 of 60\n",
      "Epoch 31 of 60\n",
      "Epoch 32 of 60\n",
      "Epoch 33 of 60\n",
      "Epoch 34 of 60\n",
      "Epoch 35 of 60\n",
      "Epoch 36 of 60\n",
      "Epoch 37 of 60\n",
      "Epoch 38 of 60\n",
      "Epoch 39 of 60\n",
      "Epoch 40 of 60\n",
      "Epoch 41 of 60\n",
      "Epoch 42 of 60\n",
      "Epoch 43 of 60\n",
      "Epoch 44 of 60\n",
      "Epoch 45 of 60\n",
      "Epoch 46 of 60\n",
      "Epoch 47 of 60\n",
      "Epoch 48 of 60\n",
      "Epoch 49 of 60\n",
      "Epoch 50 of 60\n",
      "Epoch 51 of 60\n",
      "Epoch 52 of 60\n",
      "Epoch 53 of 60\n",
      "Epoch 54 of 60\n",
      "Epoch 55 of 60\n",
      "Epoch 56 of 60\n",
      "Epoch 57 of 60\n",
      "Epoch 58 of 60\n",
      "Epoch 59 of 60\n",
      "Epoch 60 of 60\n",
      "MO_GAAL(contamination=0.1, k=10, lr_d=0.01, lr_g=0.0001, momentum=0.9,\n",
      "    stop_epochs=20) \n",
      " {'Accuracy': 0.9, 'Precision': 0.941, 'Recall': 0.566, 'F1': 0.707, 'MCC': 0.682, 'AUC_PR': 0.73, 'AUC_ROC': 0.806, 'PREC_N_SCORES': 0.628}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.mo_gaal import MO_GAAL\n",
    "import os\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = 'True'\n",
    "\n",
    "model = MO_GAAL(k=10, stop_epochs=20, lr_d=0.01, lr_g=0.0001, momentum=0.9, contamination=0.1)\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5629a030",
   "metadata": {},
   "source": [
    "ANO-GAAL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c80cf4",
   "metadata": {},
   "source": [
    "Non funzionante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5af5e4c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m AnoGAN()\n\u001b[1;32m----> 9\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m y_predicted \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test2)\n\u001b[0;32m     12\u001b[0m y_predicted_score \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdecision_function(X_test2)\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\pyod\\models\\anogan.py:333\u001b[0m, in \u001b[0;36mAnoGAN.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    331\u001b[0m optimizer_g\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    332\u001b[0m loss_G\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m--> 333\u001b[0m \u001b[43moptimizer_g\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhist_loss_discriminator\u001b[38;5;241m.\u001b[39mappend(loss_D\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m    336\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhist_loss_generator\u001b[38;5;241m.\u001b[39mappend(loss_G\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:487\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    483\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    484\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    485\u001b[0m             )\n\u001b[1;32m--> 487\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[0;32m    490\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[1;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\torch\\optim\\adam.py:223\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    211\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    213\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[0;32m    214\u001b[0m         group,\n\u001b[0;32m    215\u001b[0m         params_with_grad,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    220\u001b[0m         state_steps,\n\u001b[0;32m    221\u001b[0m     )\n\u001b[1;32m--> 223\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    224\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\torch\\optim\\optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\torch\\optim\\adam.py:784\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    782\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[1;32m--> 784\u001b[0m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mamsgrad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcapturable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdifferentiable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrad_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfound_inf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\torch\\optim\\adam.py:430\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[0;32m    428\u001b[0m         denom \u001b[38;5;241m=\u001b[39m (max_exp_avg_sqs[i]\u001b[38;5;241m.\u001b[39msqrt() \u001b[38;5;241m/\u001b[39m bias_correction2_sqrt)\u001b[38;5;241m.\u001b[39madd_(eps)\n\u001b[0;32m    429\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 430\u001b[0m         denom \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mexp_avg_sq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbias_correction2_sqrt\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    432\u001b[0m     param\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[0;32m    434\u001b[0m \u001b[38;5;66;03m# Lastly, switch back to complex view\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"True\"\n",
    "\n",
    "# Ora importa PyOD e usa AnoGAN come prima\n",
    "from pyod.models.anogan import AnoGAN\n",
    "import tensorflow as tf\n",
    "\n",
    "model = AnoGAN()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34682324",
   "metadata": {},
   "source": [
    "SO_GAAL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af6ef79",
   "metadata": {},
   "source": [
    "Non funzionante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "81e3d631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensione X_train: (500, 2)\n",
      "Dimensione y_train: (500,)\n",
      "Dimensione X_test: (100, 2)\n",
      "Dimensione y_test: (100,)\n",
      "Epoch 1 of 60\n",
      "Epoch 2 of 60\n",
      "Epoch 3 of 60\n",
      "Epoch 4 of 60\n",
      "Epoch 5 of 60\n",
      "Epoch 6 of 60\n",
      "Epoch 7 of 60\n",
      "Epoch 8 of 60\n",
      "Epoch 9 of 60\n",
      "Epoch 10 of 60\n",
      "Epoch 11 of 60\n",
      "Epoch 12 of 60\n",
      "Epoch 13 of 60\n",
      "Epoch 14 of 60\n",
      "Epoch 15 of 60\n",
      "Epoch 16 of 60\n",
      "Epoch 17 of 60\n",
      "Epoch 18 of 60\n",
      "Epoch 19 of 60\n",
      "Epoch 20 of 60\n",
      "Epoch 21 of 60\n",
      "Epoch 22 of 60\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Using a target size (torch.Size([94, 1])) that is different to the input size (torch.Size([500, 1])) is deprecated. Please ensure they have the same size.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDimensione y_test:\u001b[39m\u001b[38;5;124m\"\u001b[39m, y_test\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m SO_GAAL()\n\u001b[1;32m---> 10\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m y_predicted \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test2)\n\u001b[0;32m     13\u001b[0m y_predicted_score \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdecision_function(X_test2)\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\pyod\\models\\so_gaal.py:196\u001b[0m, in \u001b[0;36mSO_GAAL.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    194\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerator_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(g_loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 196\u001b[0m         g_loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiscriminator\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoise\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrick_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerator_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(g_loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_epochs:\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:697\u001b[0m, in \u001b[0;36mBCELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    696\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 697\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbinary_cross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:3545\u001b[0m, in \u001b[0;36mbinary_cross_entropy\u001b[1;34m(input, target, weight, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3543\u001b[0m     reduction_enum \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[0;32m   3544\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m target\u001b[38;5;241m.\u001b[39msize() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize():\n\u001b[1;32m-> 3545\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3546\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a target size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) that is different to the input size (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) is deprecated. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3547\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure they have the same size.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3548\u001b[0m     )\n\u001b[0;32m   3550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3551\u001b[0m     new_size \u001b[38;5;241m=\u001b[39m _infer_size(target\u001b[38;5;241m.\u001b[39msize(), weight\u001b[38;5;241m.\u001b[39msize())\n",
      "\u001b[1;31mValueError\u001b[0m: Using a target size (torch.Size([94, 1])) that is different to the input size (torch.Size([500, 1])) is deprecated. Please ensure they have the same size."
     ]
    }
   ],
   "source": [
    "from pyod.models.so_gaal import SO_GAAL\n",
    "\n",
    "# Verifica le dimensioni dei dati generati\n",
    "print(\"Dimensione X_train:\", X_train.shape)\n",
    "print(\"Dimensione y_train:\", y_train.shape)\n",
    "print(\"Dimensione X_test:\", X_test.shape)\n",
    "print(\"Dimensione y_test:\", y_test.shape)\n",
    "\n",
    "model = SO_GAAL()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "# Valutazione del modello\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fb4017",
   "metadata": {},
   "source": [
    "RF+ICCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b046e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': 0.964, 'Precision': 0.98, 'Recall': 0.85, 'F1': 0.91, 'MCC': 0.891, 'AUC_PR': 0.73, 'AUC_ROC': 0.806, 'PREC_N_SCORES': 0.628}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Inizializza e addestra il modello\n",
    "model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Previsioni e probabilità di previsione\n",
    "y_predicted = model.predict(X_test)\n",
    "# Predizione\n",
    "y_test_scores = model.predict_proba(X_test)\n",
    "\n",
    "# Questa è la probabilità che la classificazione sia corretta\n",
    "print(evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ae849e",
   "metadata": {},
   "source": [
    "Linear+L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ece267d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': 0.902, 'Precision': 0.969, 'Recall': 0.558, 'F1': 0.708, 'MCC': 0.69, 'AUC_PR': 0.889, 'AUC_ROC': 0.95, 'PREC_N_SCORES': 0.814}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "# Inizializza e addestra il modello Ridge Classifier (Linear + L2)\n",
    "model = RidgeClassifier(alpha=1.0)  # 'alpha' è il parametro di regolarizzazione L2\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predizione delle etichette di classe\n",
    "y_predicted = model.predict(X_test)\n",
    "\n",
    "# Ottieni le probabilità della classe positiva per AUC (si utilizza decision_function per ottenere punteggi di decisione)\n",
    "y_test_scores = model.decision_function(X_test)\n",
    "\n",
    "# Calcola e stampa le metriche\n",
    "metrics = evaluate_metrics(y_test, y_predicted, y_test_scores)\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21ed48a",
   "metadata": {},
   "source": [
    "Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62a69cca-f485-4810-b146-d00d216c01cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IForest(behaviour='old', bootstrap=False, contamination=0.2, max_features=1.0,\n",
      "    max_samples='auto', n_estimators=100, n_jobs=1, random_state=2137,\n",
      "    verbose=0) \n",
      " {'Accuracy': 0.701, 'Precision': 0.297, 'Recall': 0.292, 'F1': 0.295, 'MCC': 0.105, 'AUC_PR': 0.347, 'AUC_ROC': 0.635, 'PREC_N_SCORES': 0.301}\n"
     ]
    }
   ],
   "source": [
    "model = IForest(random_state=SEED, contamination=.2)\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7f31c8",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a3608e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN(algorithm='auto', contamination=0.1, leaf_size=30, method='largest',\n",
      "  metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "  radius=1.0) \n",
      " {'Accuracy': 0.849, 'Precision': 0.78, 'Recall': 0.407, 'F1': 0.535, 'MCC': 0.489, 'AUC_PR': 0.658, 'AUC_ROC': 0.852, 'PREC_N_SCORES': 0.593}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.knn import KNN\n",
    "\n",
    "model = KNN()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28682eb3",
   "metadata": {},
   "source": [
    "OCSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c77bba73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCSVM(cache_size=200, coef0=0.0, contamination=0.1, degree=3, gamma='auto',\n",
      "   kernel='rbf', max_iter=-1, nu=0.5, shrinking=True, tol=0.001,\n",
      "   verbose=False) \n",
      " {'Accuracy': 0.837, 'Precision': 0.721, 'Recall': 0.389, 'F1': 0.506, 'MCC': 0.447, 'AUC_PR': 0.659, 'AUC_ROC': 0.788, 'PREC_N_SCORES': 0.655}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.ocsvm import OCSVM\n",
    "\n",
    "model = OCSVM()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f40d2e",
   "metadata": {},
   "source": [
    "ABOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df3e3a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABOD(contamination=0.1, method='fast', n_neighbors=5) \n",
      " {'Accuracy': 0.845, 'Precision': 0.782, 'Recall': 0.381, 'F1': 0.512, 'MCC': 0.472, 'AUC_PR': 0.644, 'AUC_ROC': 0.843, 'PREC_N_SCORES': 0.584}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.abod import ABOD\n",
    "\n",
    "model = ABOD()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03191a05",
   "metadata": {},
   "source": [
    "INNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19321ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INNE(contamination=0.1, max_samples='auto', n_estimators=200,\n",
      "   random_state=None) \n",
      " {'Accuracy': 0.832, 'Precision': 0.694, 'Recall': 0.381, 'F1': 0.491, 'MCC': 0.427, 'AUC_PR': 0.636, 'AUC_ROC': 0.805, 'PREC_N_SCORES': 0.655}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.inne import INNE\n",
    "\n",
    "model = INNE()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b5366a",
   "metadata": {},
   "source": [
    "ALAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "671e0764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALAD(activation_hidden_disc='tanh', activation_hidden_gen='tanh',\n",
      "   add_disc_zz_loss=True, add_recon_loss=False, batch_size=32,\n",
      "   contamination=0.1, dec_layers=[5, 10, 25], device=device(type='cpu'),\n",
      "   disc_xx_layers=[25, 10, 5], disc_xz_layers=[25, 10, 5],\n",
      "   disc_zz_layers=[25, 10, 5], dropout_rate=0.2, enc_layers=[25, 10, 5],\n",
      "   epochs=200, lambda_recon_loss=0.1, latent_dim=2,\n",
      "   learning_rate_disc=0.0001, learning_rate_gen=0.0001,\n",
      "   output_activation=None, preprocessing=False,\n",
      "   spectral_normalization=False, verbose=0) \n",
      " {'Accuracy': 0.83, 'Precision': 0.725, 'Recall': 0.327, 'F1': 0.451, 'MCC': 0.408, 'AUC_PR': 0.633, 'AUC_ROC': 0.759, 'PREC_N_SCORES': 0.549}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.alad import ALAD\n",
    "\n",
    "model = ALAD()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff966da",
   "metadata": {},
   "source": [
    "LMDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f10b166f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LMDD(contamination=0.1, dis_measure='aad', n_iter=50, random_state=None) \n",
      " {'Accuracy': 0.822, 'Precision': 1.0, 'Recall': 0.168, 'F1': 0.288, 'MCC': 0.37, 'AUC_PR': 0.624, 'AUC_ROC': 0.765, 'PREC_N_SCORES': 0.663}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.lmdd import LMDD\n",
    "\n",
    "model = LMDD()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfca5e0",
   "metadata": {},
   "source": [
    "SOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "27f82998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOD(alpha=0.8, contamination=0.1, n_neighbors=20, ref_set=10) \n",
      " {'Accuracy': 0.826, 'Precision': 0.611, 'Recall': 0.513, 'F1': 0.558, 'MCC': 0.453, 'AUC_PR': 0.621, 'AUC_ROC': 0.797, 'PREC_N_SCORES': 0.549}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.sod import SOD\n",
    "\n",
    "model = SOD()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42df12fb",
   "metadata": {},
   "source": [
    "COF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "578deac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COF(contamination=0.1, method='fast', n_neighbors=20) \n",
      " {'Accuracy': 0.834, 'Precision': 0.667, 'Recall': 0.442, 'F1': 0.532, 'MCC': 0.449, 'AUC_PR': 0.603, 'AUC_ROC': 0.774, 'PREC_N_SCORES': 0.593}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.cof import COF\n",
    "\n",
    "model = COF()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35130602",
   "metadata": {},
   "source": [
    "LODA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "12782922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LODA(contamination=0.1, n_bins=10, n_random_cuts=100) \n",
      " {'Accuracy': 0.83, 'Precision': 0.689, 'Recall': 0.372, 'F1': 0.483, 'MCC': 0.418, 'AUC_PR': 0.549, 'AUC_ROC': 0.692, 'PREC_N_SCORES': 0.522}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.loda import LODA\n",
    "\n",
    "model = LODA()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b9f73c",
   "metadata": {},
   "source": [
    "LUNAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f7b6391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LUNAR(contamination=0.1, epsilon=0.1, lr=0.001, model_type='WEIGHT',\n",
      "   n_epochs=200, n_neighbours=5, negative_sampling='MIXED', proportion=1.0,\n",
      "   scaler=MinMaxScaler(), val_size=0.1, verbose=0, wd=0.1) \n",
      " {'Accuracy': 0.813, 'Precision': 0.719, 'Recall': 0.204, 'F1': 0.317, 'MCC': 0.313, 'AUC_PR': 0.542, 'AUC_ROC': 0.799, 'PREC_N_SCORES': 0.46}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.lunar import LUNAR\n",
    "\n",
    "model = LUNAR()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134d43ae",
   "metadata": {},
   "source": [
    "CBLOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "17d31d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1416: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBLOF(alpha=0.9, beta=5, check_estimator=False, clustering_estimator=None,\n",
      "   contamination=0.1, n_clusters=8, n_jobs=None, random_state=None,\n",
      "   use_weights=False) \n",
      " {'Accuracy': 0.8, 'Precision': 0.556, 'Recall': 0.31, 'F1': 0.398, 'MCC': 0.307, 'AUC_PR': 0.493, 'AUC_ROC': 0.642, 'PREC_N_SCORES': 0.425}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.cblof import CBLOF\n",
    "\n",
    "model = CBLOF()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e78d538",
   "metadata": {},
   "source": [
    "DIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "664e4d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIF(batch_size=1000, contamination=0.1, device=device(type='cpu'),\n",
      "  hidden_activation='tanh', hidden_neurons=[500, 100], max_samples=256,\n",
      "  n_ensemble=50, n_estimators=6, random_state=None, representation_dim=20,\n",
      "  skip_connection=False) \n",
      " {'Accuracy': 0.788, 'Precision': 1.0, 'Recall': 0.009, 'F1': 0.018, 'MCC': 0.084, 'AUC_PR': 0.512, 'AUC_ROC': 0.826, 'PREC_N_SCORES': 0.513}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.dif import DIF\n",
    "\n",
    "model = DIF()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d12a8b",
   "metadata": {},
   "source": [
    "VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "322caf2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 30/30 [00:22<00:00,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE(batch_norm=False, batch_size=32, beta=1.0, capacity=0.0,\n",
      "  compile_mode='default', contamination=0.1,\n",
      "  decoder_neuron_list=[32, 64, 128], device=device(type='cpu'),\n",
      "  dropout_rate=0.2, encoder_neuron_list=[128, 64, 32], epoch_num=30,\n",
      "  hidden_activation_name='relu', latent_dim=2, lr=0.001,\n",
      "  optimizer_name='adam', optimizer_params={'weight_decay': 1e-05},\n",
      "  output_activation_name='sigmoid', preprocessing=True, random_state=42,\n",
      "  use_compile=False, verbose=1) \n",
      " {'Accuracy': 0.794, 'Precision': 0.532, 'Recall': 0.292, 'F1': 0.377, 'MCC': 0.283, 'AUC_PR': 0.446, 'AUC_ROC': 0.687, 'PREC_N_SCORES': 0.513}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.vae import VAE\n",
    "\n",
    "model = VAE()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2842c38a",
   "metadata": {},
   "source": [
    "GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b603e181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GMM(contamination=0.1, covariance_type='full', init_params='kmeans',\n",
      "  max_iter=100, means_init=None, n_components=1, n_init=1,\n",
      "  precisions_init=None, random_state=None, reg_covar=1e-06, tol=0.001,\n",
      "  warm_start=False, weights_init=None) \n",
      " {'Accuracy': 0.783, 'Precision': 0.482, 'Recall': 0.239, 'F1': 0.32, 'MCC': 0.225, 'AUC_PR': 0.426, 'AUC_ROC': 0.713, 'PREC_N_SCORES': 0.389}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.gmm import GMM\n",
    "\n",
    "model = GMM()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c8a4e4",
   "metadata": {},
   "source": [
    "DeepSVDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f094a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 36.17359483242035\n",
      "Epoch 2/100, Loss: 36.19166633486748\n",
      "Epoch 3/100, Loss: 36.2466336786747\n",
      "Epoch 4/100, Loss: 36.13528761267662\n",
      "Epoch 5/100, Loss: 36.165921211242676\n",
      "Epoch 6/100, Loss: 36.13916572928429\n",
      "Epoch 7/100, Loss: 36.189294904470444\n",
      "Epoch 8/100, Loss: 36.17238187789917\n",
      "Epoch 9/100, Loss: 36.2117395401001\n",
      "Epoch 10/100, Loss: 36.185857594013214\n",
      "Epoch 11/100, Loss: 36.13321906328201\n",
      "Epoch 12/100, Loss: 36.1584706902504\n",
      "Epoch 13/100, Loss: 36.17630282044411\n",
      "Epoch 14/100, Loss: 36.17380636930466\n",
      "Epoch 15/100, Loss: 36.25334322452545\n",
      "Epoch 16/100, Loss: 36.1712027490139\n",
      "Epoch 17/100, Loss: 36.12485006451607\n",
      "Epoch 18/100, Loss: 36.4436274766922\n",
      "Epoch 19/100, Loss: 36.22374951839447\n",
      "Epoch 20/100, Loss: 36.2115415930748\n",
      "Epoch 21/100, Loss: 36.16678577661514\n",
      "Epoch 22/100, Loss: 36.20809951424599\n",
      "Epoch 23/100, Loss: 36.228652626276016\n",
      "Epoch 24/100, Loss: 36.154085248708725\n",
      "Epoch 25/100, Loss: 36.138443648815155\n",
      "Epoch 26/100, Loss: 36.5161928832531\n",
      "Epoch 27/100, Loss: 36.136161506175995\n",
      "Epoch 28/100, Loss: 36.181707948446274\n",
      "Epoch 29/100, Loss: 36.141745775938034\n",
      "Epoch 30/100, Loss: 36.1334473490715\n",
      "Epoch 31/100, Loss: 36.193426355719566\n",
      "Epoch 32/100, Loss: 36.15622678399086\n",
      "Epoch 33/100, Loss: 36.199489802122116\n",
      "Epoch 34/100, Loss: 36.11734637618065\n",
      "Epoch 35/100, Loss: 36.160643100738525\n",
      "Epoch 36/100, Loss: 36.1936252117157\n",
      "Epoch 37/100, Loss: 36.16784855723381\n",
      "Epoch 38/100, Loss: 36.19024500250816\n",
      "Epoch 39/100, Loss: 36.2072534263134\n",
      "Epoch 40/100, Loss: 36.19248494505882\n",
      "Epoch 41/100, Loss: 36.18511536717415\n",
      "Epoch 42/100, Loss: 36.156825214624405\n",
      "Epoch 43/100, Loss: 36.18466040492058\n",
      "Epoch 44/100, Loss: 36.14989456534386\n",
      "Epoch 45/100, Loss: 36.18341547250748\n",
      "Epoch 46/100, Loss: 36.13255634903908\n",
      "Epoch 47/100, Loss: 36.44247457385063\n",
      "Epoch 48/100, Loss: 36.20795226097107\n",
      "Epoch 49/100, Loss: 36.16933789849281\n",
      "Epoch 50/100, Loss: 36.155869632959366\n",
      "Epoch 51/100, Loss: 36.17461675405502\n",
      "Epoch 52/100, Loss: 36.14994007349014\n",
      "Epoch 53/100, Loss: 36.176823407411575\n",
      "Epoch 54/100, Loss: 36.16330271959305\n",
      "Epoch 55/100, Loss: 36.18516033887863\n",
      "Epoch 56/100, Loss: 36.17514684796333\n",
      "Epoch 57/100, Loss: 36.11868315935135\n",
      "Epoch 58/100, Loss: 36.16933134198189\n",
      "Epoch 59/100, Loss: 36.193585991859436\n",
      "Epoch 60/100, Loss: 36.30585631728172\n",
      "Epoch 61/100, Loss: 36.124624133110046\n",
      "Epoch 62/100, Loss: 36.41590037941933\n",
      "Epoch 63/100, Loss: 36.16250681877136\n",
      "Epoch 64/100, Loss: 36.13125276565552\n",
      "Epoch 65/100, Loss: 36.290554732084274\n",
      "Epoch 66/100, Loss: 36.19485479593277\n",
      "Epoch 67/100, Loss: 36.192596822977066\n",
      "Epoch 68/100, Loss: 36.19311338663101\n",
      "Epoch 69/100, Loss: 36.15330824255943\n",
      "Epoch 70/100, Loss: 36.15977245569229\n",
      "Epoch 71/100, Loss: 36.17040690779686\n",
      "Epoch 72/100, Loss: 36.20549160242081\n",
      "Epoch 73/100, Loss: 36.14463156461716\n",
      "Epoch 74/100, Loss: 36.23132652044296\n",
      "Epoch 75/100, Loss: 36.14879962801933\n",
      "Epoch 76/100, Loss: 36.246677339076996\n",
      "Epoch 77/100, Loss: 36.14988797903061\n",
      "Epoch 78/100, Loss: 36.13583964109421\n",
      "Epoch 79/100, Loss: 36.220797061920166\n",
      "Epoch 80/100, Loss: 36.12322652339935\n",
      "Epoch 81/100, Loss: 36.13682180643082\n",
      "Epoch 82/100, Loss: 36.136348247528076\n",
      "Epoch 83/100, Loss: 36.21580085158348\n",
      "Epoch 84/100, Loss: 36.154904037714005\n",
      "Epoch 85/100, Loss: 36.17132344841957\n",
      "Epoch 86/100, Loss: 36.27107375860214\n",
      "Epoch 87/100, Loss: 36.149519234895706\n",
      "Epoch 88/100, Loss: 36.13255423307419\n",
      "Epoch 89/100, Loss: 36.17941099405289\n",
      "Epoch 90/100, Loss: 36.24904045462608\n",
      "Epoch 91/100, Loss: 36.19086942076683\n",
      "Epoch 92/100, Loss: 36.16237214207649\n",
      "Epoch 93/100, Loss: 36.12625986337662\n",
      "Epoch 94/100, Loss: 36.16925394535065\n",
      "Epoch 95/100, Loss: 36.25732374191284\n",
      "Epoch 96/100, Loss: 36.15719136595726\n",
      "Epoch 97/100, Loss: 36.21809810400009\n",
      "Epoch 98/100, Loss: 36.19173404574394\n",
      "Epoch 99/100, Loss: 36.41532385349274\n",
      "Epoch 100/100, Loss: 36.2065212726593\n",
      "DeepSVDD(batch_size=32, c=0.0, contamination=0.1, dropout_rate=0.2,\n",
      "     epochs=100, hidden_activation='relu', hidden_neurons=[64, 32],\n",
      "     l2_regularizer=0.1, n_features=18, optimizer='adam',\n",
      "     output_activation='sigmoid', preprocessing=True, random_state=None,\n",
      "     use_ae=False, validation_size=0.1, verbose=1) \n",
      " {'Accuracy': 0.76, 'Precision': 0.394, 'Recall': 0.23, 'F1': 0.291, 'MCC': 0.166, 'AUC_PR': 0.333, 'AUC_ROC': 0.598, 'PREC_N_SCORES': 0.319}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.deep_svdd import DeepSVDD\n",
    "\n",
    "# Determina il numero di feature\n",
    "n_features = X_train2.shape[1]\n",
    "\n",
    "model = DeepSVDD(n_features=n_features)\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba48687c",
   "metadata": {},
   "source": [
    "PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "711d99bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA(contamination=0.1, copy=True, iterated_power='auto', n_components=None,\n",
      "  n_selected_components=None, random_state=None, standardization=True,\n",
      "  svd_solver='auto', tol=0.0, weighted=True, whiten=False) \n",
      " {'Accuracy': 0.777, 'Precision': 0.456, 'Recall': 0.23, 'F1': 0.306, 'MCC': 0.206, 'AUC_PR': 0.373, 'AUC_ROC': 0.612, 'PREC_N_SCORES': 0.363}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.pca import PCA\n",
    "\n",
    "model = PCA()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9980f4ce",
   "metadata": {},
   "source": [
    "COPOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ac4e4963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COPOD(contamination=0.1, n_jobs=1) \n",
      " {'Accuracy': 0.767, 'Precision': 0.4, 'Recall': 0.177, 'F1': 0.245, 'MCC': 0.147, 'AUC_PR': 0.328, 'AUC_ROC': 0.627, 'PREC_N_SCORES': 0.257}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.copod import COPOD\n",
    "\n",
    "model = COPOD()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b78adc",
   "metadata": {},
   "source": [
    "SOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9aea353c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOS(contamination=0.1, eps=1e-05, metric='euclidean', perplexity=4.5) \n",
      " {'Accuracy': 0.758, 'Precision': 0.364, 'Recall': 0.177, 'F1': 0.238, 'MCC': 0.125, 'AUC_PR': 0.308, 'AUC_ROC': 0.524, 'PREC_N_SCORES': 0.274}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.sos import SOS\n",
    "\n",
    "model = SOS()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d203a345",
   "metadata": {},
   "source": [
    "ECOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e31e207f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECOD(contamination=0.1, n_jobs=1) \n",
      " {'Accuracy': 0.767, 'Precision': 0.396, 'Recall': 0.168, 'F1': 0.236, 'MCC': 0.14, 'AUC_PR': 0.34, 'AUC_ROC': 0.637, 'PREC_N_SCORES': 0.345}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.ecod import ECOD\n",
    "\n",
    "model = ECOD()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140c03c4",
   "metadata": {},
   "source": [
    "## Prove ESN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e2ca192f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Reservoir-0: 100%|██████████| 1594/1594 [00:00<00:00, 2797.32it/s]\n",
      "Running Reservoir-0: 100%|██████████| 529/529 [00:00<00:00, 2798.52it/s]\n",
      "Running Ridge-0: 100%|██████████| 529/529 [00:00<00:00, 6901.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESN \n",
      " {'Accuracy': 0.767, 'Precision': 0.396, 'Recall': 0.168, 'F1': 0.236, 'MCC': 0.14, 'AUC_PR': 0.34, 'AUC_ROC': 0.637, 'PREC_N_SCORES': 0.345}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from reservoirpy.nodes import Reservoir, Ridge\n",
    "\n",
    "# Creazione del reservoir\n",
    "reservoir = Reservoir(units=1000, sr=0.95)  # sr: raggio spettrale\n",
    "\n",
    "# Creazione del nodo di output per il readout\n",
    "readout = Ridge(ridge=1e-5)\n",
    "\n",
    "# Connessione del reservoir al readout per creare l'ESN\n",
    "reservoir >> readout\n",
    "\n",
    "# Addestramento del modello\n",
    "readout.fit(reservoir.run(X_train_scaled), X_train_scaled)  # Si allena il readout sugli stati del reservoir\n",
    "\n",
    "# Predizione per il rilevamento di anomalie\n",
    "reservoir_states = reservoir.run(X_test_scaled)\n",
    "predictions = readout.run(reservoir_states)\n",
    "errors = np.abs(predictions - X_test_scaled)\n",
    "\n",
    "# Definisci una soglia per identificare le anomalie\n",
    "threshold = np.percentile(errors, 95)  # Prendi il 95° percentile degli errori come soglia\n",
    "anomalies = errors > threshold\n",
    "\n",
    "print(\"ESN\", '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6992aa41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Reservoir-0: 100%|██████████| 1594/1594 [00:00<00:00, 2742.70it/s]\n",
      "Running ESN-0: 100%|██████████| 1/1 [00:00<00:00,  2.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting node ESN-0...\n",
      "'ESN-0': ESN('Reservoir-1', 'Ridge-1') {'Accuracy': 0.767, 'Precision': 0.396, 'Recall': 0.168, 'F1': 0.236, 'MCC': 0.14, 'AUC_PR': 0.34, 'AUC_ROC': 0.637, 'PREC_N_SCORES': 0.345}\n"
     ]
    }
   ],
   "source": [
    "from reservoirpy.nodes import ESN\n",
    "\n",
    "# Caricamento del dataset\n",
    "# Esempio: X_train, X_test, y_train, y_test sono già disponibili\n",
    "\n",
    "# Creazione dell'ESN\n",
    "#model = ESN(n_inputs=X_train.shape[1], n_outputs=1, units=100, random_state=42)\n",
    "model = ESN(n_inputs=X_train_scaled.shape[1], n_outputs=X_train_scaled.shape[1], units=500, spectral_radius=0.95, sparsity=0.5, leaking_rate=0.5, random_state=42)\n",
    "\n",
    "\n",
    "# Addestramento dell'ESN\n",
    "model.fit(reservoir.run(X_train_scaled), X_train_scaled)\n",
    "\n",
    "\n",
    "# Eseguiamo la valutazione delle metriche\n",
    "metrics = evaluate_metrics(y_test, y_predicted, y_predicted_score)\n",
    "\n",
    "# Stampa i risultati\n",
    "print(model, metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdf6c7c",
   "metadata": {},
   "source": [
    "### ESN con \"grid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d03e89df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_sequences shape: (1495, 100, 18)\n",
      "X_test_sequences shape: (430, 100, 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-1: 100%|██████████| 1/1 [00:00<00:00,  3.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting node ESN-1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-1: 100%|██████████| 1/1 [00:00<00:00,  3.01it/s]\n",
      "Running ESN-1: 100%|██████████| 1/1 [00:00<00:00, 11.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_sequences shape: (1495, 100, 18)\n",
      "X_test_sequences shape: (430, 100, 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-2: 100%|██████████| 1/1 [00:00<00:00,  3.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting node ESN-2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-2: 100%|██████████| 1/1 [00:00<00:00,  2.38it/s]\n",
      "Running ESN-2: 100%|██████████| 1/1 [00:00<00:00,  8.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_sequences shape: (1495, 100, 18)\n",
      "X_test_sequences shape: (430, 100, 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-3: 100%|██████████| 1/1 [00:00<00:00,  3.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting node ESN-3...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-3: 100%|██████████| 1/1 [00:00<00:00,  2.59it/s]\n",
      "Running ESN-3: 100%|██████████| 1/1 [00:00<00:00,  9.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_sequences shape: (1495, 100, 18)\n",
      "X_test_sequences shape: (430, 100, 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-4: 100%|██████████| 1/1 [00:00<00:00,  1.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting node ESN-4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-4: 100%|██████████| 1/1 [00:00<00:00,  2.10it/s]\n",
      "Running ESN-4: 100%|██████████| 1/1 [00:00<00:00, 10.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_sequences shape: (1495, 100, 18)\n",
      "X_test_sequences shape: (430, 100, 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-5: 100%|██████████| 1/1 [00:00<00:00,  1.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting node ESN-5...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-5: 100%|██████████| 1/1 [00:00<00:00,  2.08it/s]\n",
      "Running ESN-5: 100%|██████████| 1/1 [00:00<00:00,  9.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_sequences shape: (1495, 100, 18)\n",
      "X_test_sequences shape: (430, 100, 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-6: 100%|██████████| 1/1 [00:00<00:00,  3.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting node ESN-6...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-6: 100%|██████████| 1/1 [00:00<00:00,  2.77it/s]\n",
      "Running ESN-6: 100%|██████████| 1/1 [00:00<00:00,  9.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_sequences shape: (1495, 100, 18)\n",
      "X_test_sequences shape: (430, 100, 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-7: 100%|██████████| 1/1 [00:00<00:00,  3.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting node ESN-7...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-7: 100%|██████████| 1/1 [00:00<00:00,  1.31it/s]\n",
      "Running ESN-7: 100%|██████████| 1/1 [00:00<00:00,  4.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_sequences shape: (1495, 100, 18)\n",
      "X_test_sequences shape: (430, 100, 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-8: 100%|██████████| 1/1 [00:00<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting node ESN-8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-8: 100%|██████████| 1/1 [00:01<00:00,  1.03s/it]\n",
      "Running ESN-8: 100%|██████████| 1/1 [00:00<00:00,  3.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_sequences shape: (1495, 100, 18)\n",
      "X_test_sequences shape: (430, 100, 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-9: 100%|██████████| 1/1 [00:00<00:00,  1.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting node ESN-9...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-9: 100%|██████████| 1/1 [00:01<00:00,  1.22s/it]\n",
      "Running ESN-9: 100%|██████████| 1/1 [00:00<00:00,  3.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_sequences shape: (1495, 100, 18)\n",
      "X_test_sequences shape: (430, 100, 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-10: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting node ESN-10...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-10: 100%|██████████| 1/1 [00:01<00:00,  1.01s/it]\n",
      "Running ESN-10: 100%|██████████| 1/1 [00:00<00:00,  3.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_sequences shape: (1495, 100, 18)\n",
      "X_test_sequences shape: (430, 100, 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-11: 100%|██████████| 1/1 [00:00<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting node ESN-11...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-11: 100%|██████████| 1/1 [00:00<00:00,  1.72it/s]\n",
      "Running ESN-11: 100%|██████████| 1/1 [00:00<00:00,  5.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_sequences shape: (1495, 100, 18)\n",
      "X_test_sequences shape: (430, 100, 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-12: 100%|██████████| 1/1 [00:00<00:00,  2.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting node ESN-12...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-12: 100%|██████████| 1/1 [00:00<00:00,  1.97it/s]\n",
      "Running ESN-12: 100%|██████████| 1/1 [00:00<00:00,  6.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_sequences shape: (1495, 100, 18)\n",
      "X_test_sequences shape: (430, 100, 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-13: 100%|██████████| 1/1 [00:00<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting node ESN-13...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-13: 100%|██████████| 1/1 [00:00<00:00,  1.69it/s]\n",
      "Running ESN-13: 100%|██████████| 1/1 [00:00<00:00,  6.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_sequences shape: (1495, 100, 18)\n",
      "X_test_sequences shape: (430, 100, 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-14: 100%|██████████| 1/1 [00:00<00:00,  2.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting node ESN-14...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-14: 100%|██████████| 1/1 [00:00<00:00,  1.99it/s]\n",
      "Running ESN-14: 100%|██████████| 1/1 [00:00<00:00,  9.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_sequences shape: (1495, 100, 18)\n",
      "X_test_sequences shape: (430, 100, 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-15: 100%|██████████| 1/1 [00:00<00:00,  3.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting node ESN-15...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-15: 100%|██████████| 1/1 [00:00<00:00,  2.82it/s]\n",
      "Running ESN-15: 100%|██████████| 1/1 [00:00<00:00,  9.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_sequences shape: (1495, 100, 18)\n",
      "X_test_sequences shape: (430, 100, 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-16: 100%|██████████| 1/1 [00:00<00:00,  3.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting node ESN-16...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-16: 100%|██████████| 1/1 [00:00<00:00,  2.75it/s]\n",
      "Running ESN-16: 100%|██████████| 1/1 [00:00<00:00,  9.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_sequences shape: (1495, 100, 18)\n",
      "X_test_sequences shape: (430, 100, 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-17: 100%|██████████| 1/1 [00:00<00:00,  3.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting node ESN-17...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-17: 100%|██████████| 1/1 [00:00<00:00,  2.68it/s]\n",
      "Running ESN-17: 100%|██████████| 1/1 [00:00<00:00,  8.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_sequences shape: (1495, 100, 18)\n",
      "X_test_sequences shape: (430, 100, 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-18: 100%|██████████| 1/1 [00:00<00:00,  3.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting node ESN-18...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-18: 100%|██████████| 1/1 [00:00<00:00,  2.92it/s]\n",
      "Running ESN-18: 100%|██████████| 1/1 [00:00<00:00,  9.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_sequences shape: (1495, 100, 18)\n",
      "X_test_sequences shape: (430, 100, 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-19: 100%|██████████| 1/1 [00:00<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting node ESN-19...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-19: 100%|██████████| 1/1 [00:00<00:00,  2.55it/s]\n",
      "Running ESN-19: 100%|██████████| 1/1 [00:00<00:00,  7.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_sequences shape: (1495, 100, 18)\n",
      "X_test_sequences shape: (430, 100, 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-20: 100%|██████████| 1/1 [00:00<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting node ESN-20...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-20: 100%|██████████| 1/1 [00:00<00:00,  2.60it/s]\n",
      "Running ESN-20: 100%|██████████| 1/1 [00:00<00:00,  8.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_sequences shape: (1495, 100, 18)\n",
      "X_test_sequences shape: (430, 100, 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-21: 100%|██████████| 1/1 [00:00<00:00,  3.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting node ESN-21...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-21: 100%|██████████| 1/1 [00:00<00:00,  2.57it/s]\n",
      "Running ESN-21: 100%|██████████| 1/1 [00:00<00:00,  7.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_sequences shape: (1495, 100, 18)\n",
      "X_test_sequences shape: (430, 100, 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-22: 100%|██████████| 1/1 [00:00<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting node ESN-22...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-22: 100%|██████████| 1/1 [00:00<00:00,  1.67it/s]\n",
      "Running ESN-22: 100%|██████████| 1/1 [00:00<00:00,  7.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_sequences shape: (1495, 100, 18)\n",
      "X_test_sequences shape: (430, 100, 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-23: 100%|██████████| 1/1 [00:00<00:00,  2.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting node ESN-23...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-23: 100%|██████████| 1/1 [00:00<00:00,  2.29it/s]\n",
      "Running ESN-23: 100%|██████████| 1/1 [00:00<00:00,  7.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_sequences shape: (1495, 100, 18)\n",
      "X_test_sequences shape: (430, 100, 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-24: 100%|██████████| 1/1 [00:00<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting node ESN-24...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-24: 100%|██████████| 1/1 [00:00<00:00,  2.26it/s]\n",
      "Running ESN-24: 100%|██████████| 1/1 [00:00<00:00,  7.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_sequences shape: (1495, 100, 18)\n",
      "X_test_sequences shape: (430, 100, 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-25: 100%|██████████| 1/1 [00:00<00:00,  2.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting node ESN-25...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-25: 100%|██████████| 1/1 [00:00<00:00,  2.19it/s]\n",
      "Running ESN-25: 100%|██████████| 1/1 [00:00<00:00,  8.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_sequences shape: (1495, 100, 18)\n",
      "X_test_sequences shape: (430, 100, 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-26: 100%|██████████| 1/1 [00:00<00:00,  2.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting node ESN-26...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-26: 100%|██████████| 1/1 [00:00<00:00,  2.37it/s]\n",
      "Running ESN-26: 100%|██████████| 1/1 [00:00<00:00,  7.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_sequences shape: (1495, 100, 18)\n",
      "X_test_sequences shape: (430, 100, 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-27: 100%|██████████| 1/1 [00:00<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting node ESN-27...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-27: 100%|██████████| 1/1 [00:00<00:00,  1.95it/s]\n",
      "Running ESN-27: 100%|██████████| 1/1 [00:00<00:00,  7.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_sequences shape: (1495, 100, 18)\n",
      "X_test_sequences shape: (430, 100, 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-28: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting node ESN-28...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-28: 100%|██████████| 1/1 [00:00<00:00,  1.27it/s]\n",
      "Running ESN-28: 100%|██████████| 1/1 [00:00<00:00,  4.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errore con parametri 500, 0.8, 0.1: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].\n",
      "X_train_sequences shape: (1495, 100, 18)\n",
      "X_test_sequences shape: (430, 100, 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-29: 100%|██████████| 1/1 [00:00<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting node ESN-29...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-29: 100%|██████████| 1/1 [00:00<00:00,  1.20it/s]\n",
      "Running ESN-29: 100%|██████████| 1/1 [00:00<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_sequences shape: (1495, 100, 18)\n",
      "X_test_sequences shape: (430, 100, 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-30: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting node ESN-30...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-30: 100%|██████████| 1/1 [00:00<00:00,  1.33it/s]\n",
      "Running ESN-30: 100%|██████████| 1/1 [00:00<00:00,  4.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_sequences shape: (1495, 100, 18)\n",
      "X_test_sequences shape: (430, 100, 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-31: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting node ESN-31...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-31: 100%|██████████| 1/1 [00:00<00:00,  1.29it/s]\n",
      "Running ESN-31: 100%|██████████| 1/1 [00:00<00:00,  5.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errore con parametri 500, 0.9, 0.1: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].\n",
      "X_train_sequences shape: (1495, 100, 18)\n",
      "X_test_sequences shape: (430, 100, 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-32: 100%|██████████| 1/1 [00:00<00:00,  1.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting node ESN-32...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-32: 100%|██████████| 1/1 [00:00<00:00,  1.33it/s]\n",
      "Running ESN-32: 100%|██████████| 1/1 [00:00<00:00,  4.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train_sequences shape: (1495, 100, 18)\n",
      "X_test_sequences shape: (430, 100, 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-33: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting node ESN-33...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-33: 100%|██████████| 1/1 [00:00<00:00,  1.36it/s]\n",
      "Running ESN-33: 100%|██████████| 1/1 [00:00<00:00,  4.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errore con parametri 500, 0.9, 0.5: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].\n",
      "X_train_sequences shape: (1495, 100, 18)\n",
      "X_test_sequences shape: (430, 100, 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-34: 100%|██████████| 1/1 [00:00<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting node ESN-34...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-34: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s]\n",
      "Running ESN-34: 100%|██████████| 1/1 [00:00<00:00,  5.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errore con parametri 500, 0.95, 0.1: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].\n",
      "X_train_sequences shape: (1495, 100, 18)\n",
      "X_test_sequences shape: (430, 100, 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-35: 100%|██████████| 1/1 [00:00<00:00,  1.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting node ESN-35...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-35: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s]\n",
      "Running ESN-35: 100%|██████████| 1/1 [00:00<00:00,  5.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Errore con parametri 500, 0.95, 0.3: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].\n",
      "X_train_sequences shape: (1495, 100, 18)\n",
      "X_test_sequences shape: (430, 100, 18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-36: 100%|██████████| 1/1 [00:00<00:00,  1.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting node ESN-36...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-36: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s]\n",
      "Running ESN-36: 100%|██████████| 1/1 [00:00<00:00,  4.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'units': 500, 'spectral_radius': 0.9, 'leaking_rate': 0.3}\n",
      "Best F1 score: 0.406015037593985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running ESN-32: 100%|██████████| 1/1 [00:00<00:00,  5.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: 'ESN-32': ESN('Reservoir-33', 'Ridge-33')\n",
      "Metrics: {'Accuracy': 0.814, 'Precision': 0.676, 'Recall': 0.269, 'F1': 0.385, 'MCC': 0.342, 'AUC_PR': 0.526, 'AUC_ROC': 0.731, 'PREC_N_SCORES': 0.452}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from itertools import product\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import Ridge\n",
    "import numpy as np\n",
    "\n",
    "# Funzione per creare sequenze temporali dai dati\n",
    "def create_sequences(data, seq_length):\n",
    "    sequences = []\n",
    "    for i in range(len(data) - seq_length + 1):\n",
    "        seq = data[i:i + seq_length]\n",
    "        sequences.append(seq)\n",
    "    return np.array(sequences)\n",
    "\n",
    "# Parametri da ottimizzare manualmente\n",
    "param_grid = {\n",
    "    'units': [50, 100, 150, 500],\n",
    "    'spectral_radius': [0.8, 0.9, 0.95],\n",
    "    'leaking_rate': [ 0.1, 0.3, 0.5]\n",
    "}\n",
    "\n",
    "best_score = -1\n",
    "best_params = {}\n",
    "best_model = None\n",
    "\n",
    "# Imposta la lunghezza delle sequenze\n",
    "seq_length = 100  # Modifica la lunghezza della sequenza in base alle tue esigenze\n",
    "\n",
    "# Creare le sequenze per X_train e X_test\n",
    "X_train_sequences = create_sequences(X_train_scaled, seq_length)\n",
    "X_test_sequences = create_sequences(X_test_scaled, seq_length)\n",
    "\n",
    "# Ciclo su tutte le combinazioni di parametri\n",
    "for units, spectral_radius, leaking_rate in product(param_grid['units'], \n",
    "                                                    param_grid['spectral_radius'], \n",
    "                                                    param_grid['leaking_rate']):\n",
    "    # Inizializza il modello ESN con i parametri correnti\n",
    "    model = ESN(n_inputs=X_train_sequences.shape[2], n_outputs=1, units=units, \n",
    "                spectral_radius=spectral_radius, leaking_rate=leaking_rate, random_state=42)\n",
    "    \n",
    "    readout = Ridge()\n",
    "\n",
    "    print(f\"X_train_sequences shape: {X_train_sequences.shape}\")  # Verifica che la forma dei dati di training sia corretta\n",
    "    print(f\"X_test_sequences shape: {X_test_sequences.shape}\")    # Verifica che la forma dei dati di test sia corretta\n",
    "    \n",
    "    try:\n",
    "        # Trasponi X_train_sequences se necessario per avere (num_samples, num_features, seq_length)\n",
    "        X_train_seq_transposed = X_train_sequences.reshape(X_train_sequences.shape[0], -1)  # Forma: (num_samples, seq_length * num_features)\n",
    "        X_test_seq_transposed = X_test_sequences.reshape(X_test_sequences.shape[0], -1)    # Forma: (num_samples, seq_length * num_features)\n",
    "        \n",
    "        # Addestra il modello con un warmup esteso\n",
    "        model.fit(X_train_seq_transposed, X_train_scaled[:X_train_seq_transposed.shape[0]])\n",
    "\n",
    "        # Predizione per il rilevamento di anomalie (stati del serbatoio sui dati di test)\n",
    "        reservoir_states_train = model.run(X_train_seq_transposed)\n",
    "        readout.fit(reservoir_states_train, y_train[:X_train_seq_transposed.shape[0]])\n",
    "\n",
    "        # Predizione sui dati di test\n",
    "        reservoir_states_test = model.run(X_test_seq_transposed)\n",
    "        predictions = readout.predict(reservoir_states_test)\n",
    "\n",
    "        # Calcola la metrica F1\n",
    "        score = f1_score(y_test[:X_test_seq_transposed.shape[0]], predictions.round())  # Arrotonda le predizioni per il rilevamento di anomalie\n",
    "\n",
    "        # Aggiorna i migliori parametri e modello se il punteggio è migliore\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_params = {'units': units, 'spectral_radius': spectral_radius, 'leaking_rate': leaking_rate}\n",
    "            best_model = model\n",
    "\n",
    "    except ValueError as e:\n",
    "        print(f\"Errore con parametri {units}, {spectral_radius}, {leaking_rate}: {e}\")\n",
    "\n",
    "# Stampa i migliori parametri e punteggio\n",
    "print(\"Best parameters:\", best_params)\n",
    "print(\"Best F1 score:\", best_score)\n",
    "\n",
    "# Ora, con il miglior modello, possiamo fare una predizione finale sui dati di test\n",
    "\n",
    "# Utilizza il miglior modello per fare una predizione finale sui dati di test\n",
    "X_test_seq_transposed = X_test_sequences.reshape(X_test_sequences.shape[0], -1)  # Trasponi X_test per il modello\n",
    "reservoir_states_test = best_model.run(X_test_seq_transposed)  # Ottieni gli stati del serbatoio sui dati di test\n",
    "\n",
    "# Predizione finale\n",
    "predictions = readout.predict(reservoir_states_test)  # Predizione finale con il modello readout\n",
    "\n",
    "# Calcola le metriche\n",
    "metrics = evaluate_metrics(y_test[:X_test_seq_transposed.shape[0]], predictions.round(), predictions)\n",
    "\n",
    "# Stampa i risultati\n",
    "print(\"Best model:\", best_model)\n",
    "print(\"Metrics:\", metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140e0d61",
   "metadata": {},
   "source": [
    "## XGBOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "31cf9dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [17:08:41] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBOD(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "   colsample_bytree=1,\n",
      "   estimator_list=[KNN(algorithm='auto', contamination=0.1, leaf_size=30, method='largest',\n",
      "  metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
      "  radius=1.0), LOF(algorithm='auto', contamination=0.1, leaf_size=30, metric='minkowski',\n",
      "  metric_params=None, n_jobs=1, n_neighbors=1, no...ax_features=1.0,\n",
      "    max_samples='auto', n_estimators=200, n_jobs=1, random_state=0,\n",
      "    verbose=0)],\n",
      "   gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "   min_child_weight=1, n_estimators=100, n_jobs=1, nthread=None,\n",
      "   objective='binary:logistic', random_state=0, reg_alpha=0, reg_lambda=1,\n",
      "   scale_pos_weight=1, silent=True,\n",
      "   standardization_flag_list=[True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False],\n",
      "   subsample=1) {'Accuracy': 0.968, 'Precision': 0.953, 'Recall': 0.894, 'F1': 0.922, 'MCC': 0.903, 'AUC_PR': 0.969, 'AUC_ROC': 0.99, 'PREC_N_SCORES': 0.912}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.xgbod import XGBOD\n",
    "\n",
    "# Inizializza e addestra XGBOD\n",
    "model = XGBOD()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Prevedi gli outlier nel dataset di test\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_predicted_score = model.decision_function(X_test_scaled)\n",
    "# Eseguiamo la valutazione delle metriche\n",
    "metrics = evaluate_metrics(y_test, y_pred, y_predicted_score)\n",
    "\n",
    "# Stampa i risultati\n",
    "print(model, metrics)\n",
    "\n",
    "#n_estimators=50,\n",
    "#max_depth=3,\n",
    "#learning_rate=0.1,\n",
    "#random_state=SEED"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3705d61e",
   "metadata": {},
   "source": [
    "#### Con metiche di Memoria e Tempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a82d45b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from memory_profiler import memory_usage\n",
    "from pyod.models.xgbod import XGBOD\n",
    "\n",
    "# Inizializza e addestra XGBOD\n",
    "model = XGBOD(n_estimators=50, max_depth=3, learning_rate=0.1, random_state=SEED)\n",
    "\n",
    "def train_model():\n",
    "    start_time = time.time()\n",
    "    mem_usage = memory_usage((model.fit, (X_train_scaled, y_train)))\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"\\n Tempo di addestramento: {training_time} secondi\")\n",
    "    print(f\"Uso della memoria durante l'addestramento: {max(mem_usage)} MiB\")\n",
    "    return training_time, mem_usage\n",
    "\n",
    "def inference_model():\n",
    "    start_time = time.time()\n",
    "    mem_usage_inference = memory_usage((model.predict, (X_test_scaled,)))\n",
    "    inference_time = time.time() - start_time\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    print(f\"\\n Tempo di inferenza: {inference_time} secondi\")\n",
    "    print(f\"Uso della memoria durante l'inferenza: {max(mem_usage_inference)} MiB\")\n",
    "    return y_pred, inference_time, mem_usage_inference\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5595e73b",
   "metadata": {},
   "source": [
    "### XGBOD più modelli unsupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5b09d455",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [17:08:48] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBOD(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "   colsample_bytree=1,\n",
      "   estimator_list=[KNN(algorithm='auto', contamination=0.1, leaf_size=30, method='largest',\n",
      "  metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "  radius=1.0), LOF(algorithm='auto', contamination=0.1, leaf_size=30, metric='minkowski',\n",
      "  metric_params=None, n_jobs=1, n_neighbors=20, n...3, gamma='auto',\n",
      "   kernel='rbf', max_iter=-1, nu=0.5, shrinking=True, tol=0.001,\n",
      "   verbose=False)],\n",
      "   gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "   min_child_weight=1, n_estimators=100, n_jobs=1, nthread=None,\n",
      "   objective='binary:logistic', random_state=0, reg_alpha=0, reg_lambda=1,\n",
      "   scale_pos_weight=1, silent=True,\n",
      "   standardization_flag_list=[True, True, True, True], subsample=1) {'Accuracy': 0.968, 'Precision': 0.944, 'Recall': 0.903, 'F1': 0.923, 'MCC': 0.903, 'AUC_PR': 0.974, 'AUC_ROC': 0.991, 'PREC_N_SCORES': 0.92}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.xgbod import XGBOD\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.iforest import IForest\n",
    "from pyod.models.lof import LOF\n",
    "from pyod.models.abod import ABOD\n",
    "from pyod.models.ocsvm import OCSVM\n",
    "\n",
    "# Definizione dei modelli unsupervised\n",
    "unsupervised_models = [ KNN(),\n",
    "                       LOF(),\n",
    "                       ABOD(),\n",
    "                        OCSVM()\n",
    "                    ]\n",
    "# Inizializza e addestra XGBOD\n",
    "model = XGBOD(estimator_list=unsupervised_models)\n",
    "\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Prevedi gli outlier nel dataset di test\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_predicted_score = model.decision_function(X_test_scaled)\n",
    "# Eseguiamo la valutazione delle metriche\n",
    "metrics = evaluate_metrics(y_test, y_pred, y_predicted_score)\n",
    "\n",
    "# Stampa i risultati\n",
    "print(model, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c6d685",
   "metadata": {},
   "source": [
    "#### Con Metriche di Tempo e Memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f00d2966",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [17:08:51] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Tempo di addestramento: 2.184173345565796 secondi\n",
      "Uso della memoria durante l'addestramento: 611.49609375 MiB\n",
      "\n",
      " Tempo di inferenza: 1.5959651470184326 secondi\n",
      "Uso della memoria durante l'inferenza: 602.36328125 MiB\n",
      "XGBOD(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "   colsample_bytree=1,\n",
      "   estimator_list=[KNN(algorithm='auto', contamination=0.1, leaf_size=30, method='largest',\n",
      "  metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "  radius=1.0), LOF(algorithm='auto', contamination=0.1, leaf_size=30, metric='minkowski',\n",
      "  metric_params=None, n_jobs=1, n_neighbors=20, n...3, gamma='auto',\n",
      "   kernel='rbf', max_iter=-1, nu=0.5, shrinking=True, tol=0.001,\n",
      "   verbose=False)],\n",
      "   gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "   min_child_weight=1, n_estimators=100, n_jobs=1, nthread=None,\n",
      "   objective='binary:logistic', random_state=0, reg_alpha=0, reg_lambda=1,\n",
      "   scale_pos_weight=1, silent=True,\n",
      "   standardization_flag_list=[True, True, True, True], subsample=1) {'Accuracy': 0.968, 'Precision': 0.944, 'Recall': 0.903, 'F1': 0.923, 'MCC': 0.903, 'AUC_PR': 0.974, 'AUC_ROC': 0.991, 'PREC_N_SCORES': 0.92}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from memory_profiler import memory_usage\n",
    "from pyod.models.xgbod import XGBOD\n",
    "\n",
    "# Definizione dei modelli unsupervised\n",
    "unsupervised_models = [ KNN(),\n",
    "                       LOF(),\n",
    "                       ABOD(),\n",
    "                        OCSVM()\n",
    "                    ]\n",
    "# Inizializza e addestra XGBOD\n",
    "model = XGBOD(estimator_list=unsupervised_models)\n",
    "\n",
    "def train_model():\n",
    "    start_time = time.time()\n",
    "    mem_usage = memory_usage((model.fit, (X_train_scaled, y_train)))\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"\\n Tempo di addestramento: {training_time} secondi\")\n",
    "    print(f\"Uso della memoria durante l'addestramento: {max(mem_usage)} MiB\")\n",
    "    return training_time, mem_usage\n",
    "\n",
    "def inference_model():\n",
    "    start_time = time.time()\n",
    "    mem_usage_inference = memory_usage((model.predict, (X_test_scaled,)))\n",
    "    inference_time = time.time() - start_time\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    print(f\"\\n Tempo di inferenza: {inference_time} secondi\")\n",
    "    print(f\"Uso della memoria durante l'inferenza: {max(mem_usage_inference)} MiB\")\n",
    "    return y_pred, inference_time, mem_usage_inference\n",
    "\n",
    "# Addestramento del modello e monitoraggio delle metriche di efficientamento\n",
    "training_time, mem_usage = train_model()\n",
    "\n",
    "# Inferenza del modello e monitoraggio delle metriche di efficientamento\n",
    "y_pred, inference_time, mem_usage_inference = inference_model()\n",
    "\n",
    "# Calcola i punteggi di decisione\n",
    "y_predicted_score = model.decision_function(X_test_scaled)\n",
    "\n",
    "# Eseguiamo la valutazione delle metriche con le nuove metriche di efficientamento\n",
    "metrics = evaluate_metrics(y_test, y_pred, y_predicted_score)\n",
    "\n",
    "# Stampa i risultati\n",
    "print(model, metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a13a44",
   "metadata": {},
   "source": [
    "### XGBOD più modelli unsupervised e Parametri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7899936c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [17:08:54] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'Accuracy': 0.97, 'Precision': 0.945, 'Recall': 0.912, 'F1': 0.928, 'MCC': 0.909, 'AUC_PR': 0.973, 'AUC_ROC': 0.992, 'PREC_N_SCORES': 0.92}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.xgbod import XGBOD\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.lof import LOF\n",
    "from pyod.models.abod import ABOD\n",
    "from pyod.models.ocsvm import OCSVM\n",
    "\n",
    "# Definizione dei modelli unsupervised\n",
    "unsupervised_models = [ KNN(),\n",
    "                       LOF(),\n",
    "                       ABOD(),\n",
    "                        OCSVM()\n",
    "                    ]\n",
    "\n",
    "# Inizializza e addestra XGBOD\n",
    "model = XGBOD(estimator_list=unsupervised_models,\n",
    "              n_estimators=100,\n",
    "              max_depth=3,\n",
    "              learning_rate=0.2,\n",
    "              n_jobs=-1,\n",
    "              random_state=SEED\n",
    "            )\n",
    "\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Prevedi gli outlier nel dataset di test\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_predicted_score = model.decision_function(X_test_scaled)\n",
    "\n",
    "# Eseguiamo la valutazione delle metriche\n",
    "metrics = evaluate_metrics(y_test, y_pred, y_predicted_score)\n",
    "print(\"\")\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796179ff",
   "metadata": {},
   "source": [
    "#### Con Metriche di Tempo e Memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "790f2d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [17:08:57] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Tempo di addestramento: 2.3739891052246094 secondi\n",
      "Uso della memoria durante l'addestramento: 611.7890625 MiB\n",
      "\n",
      " Tempo di inferenza: 1.7508814334869385 secondi\n",
      "Uso della memoria durante l'inferenza: 603.37890625 MiB\n",
      "XGBOD(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "   colsample_bytree=1,\n",
      "   estimator_list=[KNN(algorithm='auto', contamination=0.1, leaf_size=30, method='largest',\n",
      "  metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "  radius=1.0), LOF(algorithm='auto', contamination=0.1, leaf_size=30, metric='minkowski',\n",
      "  metric_params=None, n_jobs=1, n_neighbors=20, n...3, gamma='auto',\n",
      "   kernel='rbf', max_iter=-1, nu=0.5, shrinking=True, tol=0.001,\n",
      "   verbose=False)],\n",
      "   gamma=0, learning_rate=0.2, max_delta_step=0, max_depth=3,\n",
      "   min_child_weight=1, n_estimators=100, n_jobs=1, nthread=None,\n",
      "   objective='binary:logistic', random_state=2137, reg_alpha=0,\n",
      "   reg_lambda=1, scale_pos_weight=1, silent=True,\n",
      "   standardization_flag_list=[True, True, True, True], subsample=1) {'Accuracy': 0.97, 'Precision': 0.945, 'Recall': 0.912, 'F1': 0.928, 'MCC': 0.909, 'AUC_PR': 0.973, 'AUC_ROC': 0.992, 'PREC_N_SCORES': 0.92}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from memory_profiler import memory_usage\n",
    "from pyod.models.xgbod import XGBOD\n",
    "\n",
    "# Definizione dei modelli unsupervised\n",
    "unsupervised_models = [ KNN(),\n",
    "                       LOF(),\n",
    "                       ABOD(),\n",
    "                        OCSVM()\n",
    "                    ]\n",
    "# Inizializza e addestra XGBOD\n",
    "model = XGBOD(estimator_list=unsupervised_models, n_estimators=100, max_depth=3, learning_rate=0.2, random_state=SEED)\n",
    "\n",
    "def train_model():\n",
    "    start_time = time.time()\n",
    "    mem_usage = memory_usage((model.fit, (X_train_scaled, y_train)))\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"\\n Tempo di addestramento: {training_time} secondi\")\n",
    "    print(f\"Uso della memoria durante l'addestramento: {max(mem_usage)} MiB\")\n",
    "    return training_time, mem_usage\n",
    "\n",
    "def inference_model():\n",
    "    start_time = time.time()\n",
    "    mem_usage_inference = memory_usage((model.predict, (X_test_scaled,)))\n",
    "    inference_time = time.time() - start_time\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    print(f\"\\n Tempo di inferenza: {inference_time} secondi\")\n",
    "    print(f\"Uso della memoria durante l'inferenza: {max(mem_usage_inference)} MiB\")\n",
    "    return y_pred, inference_time, mem_usage_inference\n",
    "\n",
    "# Addestramento del modello e monitoraggio delle metriche di efficientamento\n",
    "training_time, mem_usage = train_model()\n",
    "\n",
    "# Inferenza del modello e monitoraggio delle metriche di efficientamento\n",
    "y_pred, inference_time, mem_usage_inference = inference_model()\n",
    "\n",
    "# Calcola i punteggi di decisione\n",
    "y_predicted_score = model.decision_function(X_test_scaled)\n",
    "\n",
    "# Eseguiamo la valutazione delle metriche con le nuove metriche di efficientamento\n",
    "metrics = evaluate_metrics(y_test, y_pred, y_predicted_score)\n",
    "\n",
    "# Stampa i risultati\n",
    "print(model, metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39459cc5",
   "metadata": {},
   "source": [
    "### Early Stopping\n",
    "Termina l'esecuzione anticipatamente se per un numero prestabilito di round non migliorano più i parametri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f0157b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [17:09:02] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [17:09:03] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [17:09:05] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [17:09:08] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [17:09:09] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [17:09:11] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [17:09:13] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [17:09:15] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [17:09:17] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [17:09:18] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [17:09:19] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [17:09:20] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [17:09:22] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at iteration 12\n",
      "\n",
      "{'Accuracy': 0.97, 'Precision': 0.971, 'Recall': 0.885, 'F1': 0.926, 'MCC': 0.909, 'AUC_PR': 0.969, 'AUC_ROC': 0.99, 'PREC_N_SCORES': 0.912}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pyod.models.xgbod import XGBOD\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.iforest import IForest\n",
    "from pyod.models.lof import LOF\n",
    "from pyod.models.abod import ABOD\n",
    "from pyod.models.ocsvm import OCSVM\n",
    "\n",
    "# Definizione dei modelli unsupervised\n",
    "unsupervised_models = [ KNN(),\n",
    "                       LOF(),\n",
    "                       ABOD(),\n",
    "                        OCSVM()\n",
    "                    ]\n",
    "\n",
    "# Divisione del dataset di allenamento per avere un set di validazione\n",
    "X_train_sub, X_val, y_train_sub, y_val = train_test_split(X_train_scaled, y_train, test_size=0.2, random_state=SEED)\n",
    "\n",
    "# Inizializzazione del modello\n",
    "model = XGBOD(estimator_list=unsupervised_models, n_estimators=50, max_depth=3, learning_rate=0.2, n_jobs=-1, random_state=SEED)\n",
    "\n",
    "best_score = -np.inf\n",
    "patience = 10       # Numero di volte che il modello cercherà di migliorarsi\n",
    "patience_counter = 0\n",
    "n_iterations = 100      # Numero massimo di cicli del'allenamento\n",
    "\n",
    "for i in range(n_iterations):  # Numero massimo di iterazioni\n",
    "    model.fit(X_train_sub, y_train_sub)\n",
    "    \n",
    "    # Predizione sul set di validazione\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    val_score = accuracy_score(y_val, y_val_pred)\n",
    "    \n",
    "    # Controllo early stopping\n",
    "    if val_score > best_score:\n",
    "        best_score = val_score\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at iteration {i}\")\n",
    "            break\n",
    "    model.n_estimators += 1  # Incrementa il numero di stimatori per la prossima iterazione\n",
    "\n",
    "# Predizione sul set di test\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_predicted_score = model.decision_function(X_test_scaled)\n",
    "\n",
    "# Eseguiamo la valutazione delle metriche\n",
    "metrics = evaluate_metrics(y_test, y_pred, y_predicted_score)\n",
    "print(\"\")\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "91184e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [17:09:25] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Tempo di addestramento: 1.9182665348052979 secondi\n",
      "Uso della memoria durante l'addestramento: 613.16796875 MiB\n",
      "\n",
      " Tempo di inferenza: 1.47544527053833 secondi\n",
      "Uso della memoria durante l'inferenza: 613.171875 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [17:09:29] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Tempo di addestramento: 1.9029278755187988 secondi\n",
      "Uso della memoria durante l'addestramento: 613.0390625 MiB\n",
      "\n",
      " Tempo di inferenza: 1.5100088119506836 secondi\n",
      "Uso della memoria durante l'inferenza: 613.046875 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [17:09:33] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Tempo di addestramento: 2.14433217048645 secondi\n",
      "Uso della memoria durante l'addestramento: 612.80859375 MiB\n",
      "\n",
      " Tempo di inferenza: 1.6270020008087158 secondi\n",
      "Uso della memoria durante l'inferenza: 612.82421875 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [17:09:37] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Tempo di addestramento: 2.1460025310516357 secondi\n",
      "Uso della memoria durante l'addestramento: 613.1796875 MiB\n",
      "\n",
      " Tempo di inferenza: 1.6065373420715332 secondi\n",
      "Uso della memoria durante l'inferenza: 613.18359375 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [17:09:41] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Tempo di addestramento: 2.1586568355560303 secondi\n",
      "Uso della memoria durante l'addestramento: 613.46875 MiB\n",
      "\n",
      " Tempo di inferenza: 1.68288254737854 secondi\n",
      "Uso della memoria durante l'inferenza: 613.4765625 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [17:09:45] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Tempo di addestramento: 2.3993046283721924 secondi\n",
      "Uso della memoria durante l'addestramento: 613.5703125 MiB\n",
      "\n",
      " Tempo di inferenza: 1.4842274188995361 secondi\n",
      "Uso della memoria durante l'inferenza: 613.5703125 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [17:09:49] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Tempo di addestramento: 2.049354076385498 secondi\n",
      "Uso della memoria durante l'addestramento: 613.578125 MiB\n",
      "\n",
      " Tempo di inferenza: 1.5631005764007568 secondi\n",
      "Uso della memoria durante l'inferenza: 613.5859375 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [17:09:53] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Tempo di addestramento: 1.8952484130859375 secondi\n",
      "Uso della memoria durante l'addestramento: 614.625 MiB\n",
      "\n",
      " Tempo di inferenza: 1.4641797542572021 secondi\n",
      "Uso della memoria durante l'inferenza: 614.6328125 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [17:09:56] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Tempo di addestramento: 1.9484944343566895 secondi\n",
      "Uso della memoria durante l'addestramento: 614.5625 MiB\n",
      "\n",
      " Tempo di inferenza: 2.032728433609009 secondi\n",
      "Uso della memoria durante l'inferenza: 614.56640625 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [17:10:02] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Tempo di addestramento: 4.249924182891846 secondi\n",
      "Uso della memoria durante l'addestramento: 611.890625 MiB\n",
      "\n",
      " Tempo di inferenza: 2.6989939212799072 secondi\n",
      "Uso della memoria durante l'inferenza: 607.4921875 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [17:10:09] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Tempo di addestramento: 2.9948065280914307 secondi\n",
      "Uso della memoria durante l'addestramento: 614.16015625 MiB\n",
      "\n",
      " Tempo di inferenza: 1.9658229351043701 secondi\n",
      "Uso della memoria durante l'inferenza: 607.90234375 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [17:10:13] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Tempo di addestramento: 2.2505972385406494 secondi\n",
      "Uso della memoria durante l'addestramento: 615.23046875 MiB\n",
      "\n",
      " Tempo di inferenza: 1.774336338043213 secondi\n",
      "Uso della memoria durante l'inferenza: 615.234375 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [17:10:18] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Tempo di addestramento: 1.9607925415039062 secondi\n",
      "Uso della memoria durante l'addestramento: 615.19921875 MiB\n",
      "\n",
      " Tempo di inferenza: 1.441871166229248 secondi\n",
      "Uso della memoria durante l'inferenza: 615.20703125 MiB\n",
      "Early stopping at iteration 12\n",
      "\n",
      " Tempo di inferenza sul test: 1.5843160152435303 secondi\n",
      "Uso della memoria durante l'inferenza sul test: 608.421875 MiB\n",
      "{'Accuracy': 0.97, 'Precision': 0.971, 'Recall': 0.885, 'F1': 0.926, 'MCC': 0.909, 'AUC_PR': 0.969, 'AUC_ROC': 0.99, 'PREC_N_SCORES': 0.912}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from memory_profiler import memory_usage\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pyod.models.xgbod import XGBOD\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.lof import LOF\n",
    "from pyod.models.abod import ABOD\n",
    "from pyod.models.ocsvm import OCSVM\n",
    "\n",
    "# Definizione dei modelli unsupervised\n",
    "unsupervised_models = [KNN(), LOF(), ABOD(), OCSVM()]\n",
    "\n",
    "# Divisione del dataset di allenamento per avere un set di validazione\n",
    "X_train_sub, X_val, y_train_sub, y_val = train_test_split(X_train_scaled, y_train, test_size=0.2, random_state=SEED)\n",
    "\n",
    "# Inizializzazione del modello\n",
    "model = XGBOD(estimator_list=unsupervised_models, n_estimators=50, max_depth=3, learning_rate=0.2, n_jobs=-1, random_state=SEED)\n",
    "\n",
    "best_score = -np.inf\n",
    "patience = 10       # Numero di volte che il modello cercherà di migliorarsi\n",
    "patience_counter = 0\n",
    "n_iterations = 100  # Numero massimo di cicli dell'allenamento\n",
    "\n",
    "for i in range(n_iterations):  # Numero massimo di iterazioni\n",
    "    start_time = time.time()\n",
    "    mem_usage = memory_usage((model.fit, (X_train_sub, y_train_sub)))\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"\\n Tempo di addestramento: {training_time} secondi\")\n",
    "    print(f\"Uso della memoria durante l'addestramento: {max(mem_usage)} MiB\")\n",
    "    \n",
    "    # Predizione sul set di validazione\n",
    "    start_time = time.time()\n",
    "    mem_usage_inference = memory_usage((model.predict, (X_val,)))\n",
    "    inference_time = time.time() - start_time\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    print(f\"\\n Tempo di inferenza: {inference_time} secondi\")\n",
    "    print(f\"Uso della memoria durante l'inferenza: {max(mem_usage_inference)} MiB\")\n",
    "\n",
    "    val_score = accuracy_score(y_val, y_val_pred)\n",
    "    \n",
    "    # Controllo early stopping\n",
    "    if val_score > best_score:\n",
    "        best_score = val_score\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at iteration {i}\")\n",
    "            break\n",
    "    model.n_estimators += 1  # Incrementa il numero di stimatori per la prossima iterazione\n",
    "\n",
    "# Predizione sul set di test\n",
    "start_time = time.time()\n",
    "mem_usage_inference_test = memory_usage((model.predict, (X_test_scaled,)))\n",
    "inference_time_test = time.time() - start_time\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "print(f\"\\n Tempo di inferenza sul test: {inference_time_test} secondi\")\n",
    "print(f\"Uso della memoria durante l'inferenza sul test: {max(mem_usage_inference_test)} MiB\")\n",
    "\n",
    "y_predicted_score = model.decision_function(X_test_scaled)\n",
    "\n",
    "# Eseguiamo la valutazione delle metriche con le nuove metriche di efficientamento\n",
    "metrics = evaluate_metrics(y_test, y_pred, y_predicted_score,)\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ca29c4",
   "metadata": {},
   "source": [
    "### XGBOD + ESN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "78bf95b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Reservoir-38: 100%|██████████| 1594/1594 [00:00<00:00, 2836.66it/s]\n",
      "Running Reservoir-38: 100%|██████████| 1594/1594 [00:00<00:00, 2651.93it/s]\n",
      "Running Reservoir-38: 100%|██████████| 529/529 [00:00<00:00, 2925.92it/s]\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [17:10:29] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: XGBOD(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "   colsample_bytree=1,\n",
      "   estimator_list=[KNN(algorithm='auto', contamination=0.1, leaf_size=30, method='largest',\n",
      "  metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "  radius=1.0), LOF(algorithm='auto', contamination=0.1, leaf_size=30, metric='minkowski',\n",
      "  metric_params=None, n_jobs=1, n_neighbors=20, n...3, gamma='auto',\n",
      "   kernel='rbf', max_iter=-1, nu=0.5, shrinking=True, tol=0.001,\n",
      "   verbose=False)],\n",
      "   gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "   min_child_weight=1, n_estimators=50, n_jobs=-1, nthread=None,\n",
      "   objective='binary:logistic', random_state=42, reg_alpha=0, reg_lambda=1,\n",
      "   scale_pos_weight=1, silent=True,\n",
      "   standardization_flag_list=[True, True, True, True], subsample=1)\n",
      "Metrics: {'Accuracy': 0.951, 'Precision': 0.922, 'Recall': 0.841, 'F1': 0.88, 'MCC': 0.85, 'AUC_PR': 0.958, 'AUC_ROC': 0.986, 'PREC_N_SCORES': 0.876}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from reservoirpy.nodes import Reservoir, Ridge\n",
    "from pyod.models.xgbod import XGBOD\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.lof import LOF\n",
    "from pyod.models.abod import ABOD\n",
    "from pyod.models.ocsvm import OCSVM\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Definizione dei modelli unsupervised\n",
    "unsupervised_models = [\n",
    "    KNN(),\n",
    "    LOF(),\n",
    "    ABOD(),\n",
    "    OCSVM()\n",
    "]\n",
    "\n",
    "# Creazione del reservoir\n",
    "reservoir = Reservoir(units=1000, sr=0.95)  # sr: raggio spettrale\n",
    "# Creazione del nodo di output per il readout\n",
    "readout = Ridge(ridge=1e-5)\n",
    "# Connessione del reservoir al readout per creare l'ESN\n",
    "reservoir >> readout\n",
    "\n",
    "# Pipeline di preprocessing\n",
    "preprocessor = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('reservoir', reservoir)\n",
    "])\n",
    "\n",
    "# Trasformazione dei dati di addestramento e test con ESN\n",
    "# Addestramento del modello\n",
    "readout.fit(reservoir.run(X_train_scaled), X_train_scaled)  # Si allena il readout sugli stati del reservoir\n",
    "\n",
    "# Predizione per il rilevamento di anomalie\n",
    "X_train_transformed = reservoir.run(X_train_scaled)\n",
    "X_test_transformed = reservoir.run(X_test_scaled)\n",
    "\n",
    "# Creazione del modello XGBOD con parametri specificati\n",
    "model = XGBOD(estimator_list=unsupervised_models, n_estimators=50, max_depth=3, learning_rate=0.1, n_jobs=-1, random_state=42)\n",
    "# Uso le trasformazioni di ESN con il modello XGBOD\n",
    "model.fit(X_train_transformed, y_train)\n",
    "\n",
    "# Predizione sui dati di test\n",
    "y_pred = model.predict(X_test_transformed)\n",
    "y_predicted_score = model.decision_function(X_test_transformed)\n",
    "\n",
    "# Valutazione delle metriche\n",
    "metrics = evaluate_metrics(y_test, y_pred, y_predicted_score)\n",
    "\n",
    "# Stampa i risultati\n",
    "print(f\"Model: {model}\")\n",
    "print(f\"Metrics: {metrics}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5bdcef",
   "metadata": {},
   "source": [
    "### Batch Processing\n",
    "Addestra il modello su piccole porzioni migliorando il carico sulla memoria e la velocità"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "86269535",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:57: FutureWarning: 'Series.swapaxes' is deprecated and will be removed in a future version. Please use 'Series.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [17:10:43] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [17:10:43] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [17:10:43] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [17:10:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [17:10:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [17:10:45] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [17:10:45] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [17:10:45] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [17:10:45] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [17:10:46] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'Accuracy': 0.749, 'Precision': 0.455, 'Recall': 0.885, 'F1': 0.601, 'MCC': 0.496, 'AUC_PR': 0.878, 'AUC_ROC': 0.918, 'PREC_N_SCORES': 0.796}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pyod.models.xgbod import XGBOD\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.lof import LOF\n",
    "from pyod.models.abod import ABOD\n",
    "from pyod.models.ocsvm import OCSVM\n",
    "\n",
    "# Dividi il dataset in batch\n",
    "n_batches = 10  # Specifica il numero di batch che vuoi\n",
    "X_train_batches = np.array_split(X_train_scaled, n_batches)\n",
    "y_train_batches = np.array_split(y_train, n_batches)\n",
    "\n",
    "# Definizione dei modelli unsupervised\n",
    "unsupervised_models = [ KNN(),\n",
    "                       LOF(),\n",
    "                       ABOD(),\n",
    "                        OCSVM()\n",
    "                    ]\n",
    "\n",
    "# Inizializza i modelli per ciascun batch\n",
    "models = []\n",
    "for X_batch, y_batch in zip(X_train_batches, y_train_batches):\n",
    "    # Inizializza e addestra il modello\n",
    "    model = XGBOD(estimator_list=unsupervised_models,\n",
    "                  n_estimators=100,\n",
    "                  max_depth=3,\n",
    "                  learning_rate=0.2,\n",
    "                  n_jobs=-1,\n",
    "                  random_state=SEED\n",
    "                )\n",
    "    model.fit(X_batch, y_batch)\n",
    "    models.append(model)\n",
    "\n",
    "# Prevedi gli outlier nel dataset di test e combinalo\n",
    "y_pred_scores = np.zeros_like(X_test_scaled[:, 0], dtype=float)\n",
    "for model in models:\n",
    "    y_pred_scores += model.decision_function(X_test_scaled)\n",
    "\n",
    "# Media dei punteggi di decisione\n",
    "y_pred_scores /= n_batches\n",
    "y_pred = (y_pred_scores > np.mean(y_pred_scores)).astype(int)\n",
    "\n",
    "# Esegui la valutazione delle metriche\n",
    "metrics = evaluate_metrics(y_test, y_pred, y_pred_scores)\n",
    "print(\"\")\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c88b806",
   "metadata": {},
   "source": [
    "#### Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1c25cf88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validation scores: [nan nan nan nan nan]\n",
      "Mean ROC AUC score: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [17:11:24] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBOD(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "   colsample_bytree=1,\n",
      "   estimator_list=[KNN(algorithm='auto', contamination=0.1, leaf_size=30, method='largest',\n",
      "  metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
      "  radius=1.0), LOF(algorithm='auto', contamination=0.1, leaf_size=30, metric='minkowski',\n",
      "  metric_params=None, n_jobs=1, n_neighbors=1, no...ax_features=1.0,\n",
      "    max_samples='auto', n_estimators=200, n_jobs=1, random_state=0,\n",
      "    verbose=0)],\n",
      "   gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "   min_child_weight=1, n_estimators=50, n_jobs=1, nthread=None,\n",
      "   objective='binary:logistic', random_state=0, reg_alpha=0, reg_lambda=1,\n",
      "   scale_pos_weight=1, silent=True,\n",
      "   standardization_flag_list=[True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False],\n",
      "   subsample=1) {'Accuracy': 0.964, 'Precision': 0.943, 'Recall': 0.885, 'F1': 0.913, 'MCC': 0.891, 'AUC_PR': 0.971, 'AUC_ROC': 0.991, 'PREC_N_SCORES': 0.903}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from pyod.models.xgbod import XGBOD\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "# Preprocessing and model pipeline\n",
    "preprocessor = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('feature_selection', SelectKBest(score_func=f_classif, k=10)),\n",
    "    ('classifier', XGBOD(n_estimators=50, max_depth=3, learning_rate=0.1))\n",
    "])\n",
    "\n",
    "# Cross-validation with pipeline\n",
    "scores = cross_val_score(pipeline, X_train_scaled, y_train, cv=5, scoring='roc_auc', n_jobs=-1)\n",
    "print(f\"Cross-validation scores: {scores}\")\n",
    "print(f\"Mean ROC AUC score: {np.mean(scores)}\")\n",
    "\n",
    "# Train and evaluate model\n",
    "pipeline.fit(X_train_scaled, y_train)\n",
    "y_pred = pipeline.predict(X_test_scaled)\n",
    "y_predicted_score = pipeline.decision_function(X_test_scaled)\n",
    "\n",
    "metrics = evaluate_metrics(y_test, y_pred, y_predicted_score)\n",
    "print(pipeline.named_steps['classifier'], metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bcc0b0",
   "metadata": {},
   "source": [
    "#### XGBOD con ricerca iperparametri con \"grid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "70c125a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:307: UserWarning: The total space of parameters 8 is smaller than n_iter=10. Running 8 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [17:12:24] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'n_estimators': 50, 'max_depth': 3, 'learning_rate': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\xgboost\\core.py:160: UserWarning: [17:12:30] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0b3782d1791676daf-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:742: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBOD(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "   colsample_bytree=1,\n",
      "   estimator_list=[KNN(algorithm='auto', contamination=0.1, leaf_size=30, method='largest',\n",
      "  metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
      "  radius=1.0), LOF(algorithm='auto', contamination=0.1, leaf_size=30, metric='minkowski',\n",
      "  metric_params=None, n_jobs=1, n_neighbors=1, no...ax_features=1.0,\n",
      "    max_samples='auto', n_estimators=200, n_jobs=1, random_state=0,\n",
      "    verbose=0)],\n",
      "   gamma=0, learning_rate=0.01, max_delta_step=0, max_depth=3,\n",
      "   min_child_weight=1, n_estimators=50, n_jobs=1, nthread=None,\n",
      "   objective='binary:logistic', random_state=0, reg_alpha=0, reg_lambda=1,\n",
      "   scale_pos_weight=1, silent=True,\n",
      "   standardization_flag_list=[True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False],\n",
      "   subsample=1) {'Accuracy': 0.947, 'Precision': 0.989, 'Recall': 0.761, 'F1': 0.86, 'MCC': 0.839, 'AUC_PR': 0.898, 'AUC_ROC': 0.945, 'PREC_N_SCORES': 0.967}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from pyod.models.xgbod import XGBOD\n",
    "import numpy as np\n",
    "\n",
    "# Definizione della griglia di parametri\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [3, 5],\n",
    "    'learning_rate': [0.01, 0.1]\n",
    "}\n",
    "\n",
    "# Inizializza il modello\n",
    "model = XGBOD()\n",
    "\n",
    "# Randomized search con meno iterazioni e parallelizzazione\n",
    "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=10, cv=3, scoring='roc_auc', random_state=42, n_jobs=-1)\n",
    "random_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Migliori parametri trovati\n",
    "best_params = random_search.best_params_\n",
    "print(f\"Best parameters found: {best_params}\")\n",
    "\n",
    "# Riaddestramento del modello con i migliori parametri\n",
    "model = XGBOD(**best_params)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Prevedi gli outlier nel dataset di test\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_predicted_score = model.decision_function(X_test_scaled)\n",
    "\n",
    "# Eseguiamo la valutazione delle metriche\n",
    "metrics = evaluate_metrics(y_test, y_pred, y_predicted_score)\n",
    "\n",
    "# Stampa i risultati\n",
    "print(model, metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b35867",
   "metadata": {},
   "source": [
    "### FCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d6ad2c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name '_v1_name_scope' from 'keras.backend' (c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\keras\\backend\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\keras\\api\\_v2\\keras\\__init__.py:13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_v2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __internal__\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_v2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_v2\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\keras\\api\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Public API for tf. namespace.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m module_wrapper \u001b[38;5;28;01mas\u001b[39;00m _module_wrapper\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(_sys\u001b[38;5;241m.\u001b[39mmodules[\u001b[38;5;18m__name__\u001b[39m], _module_wrapper\u001b[38;5;241m.\u001b[39mTFModuleWrapper):\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\keras\\api\\keras\\__init__.py:16\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m applications\n\u001b[1;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m callbacks\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m constraints\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\keras\\api\\keras\\backend\\__init__.py:8\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"Keras backend API.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01m_sys\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _v1_name_scope \u001b[38;5;28;01mas\u001b[39;00m name_scope\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28mabs\u001b[39m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28mall\u001b[39m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name '_v1_name_scope' from 'keras.backend' (c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\keras\\backend\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Definisci il modello FCNN\n",
    "model = Sequential([\n",
    "    Conv1D(64, 3, activation='relu', input_shape=(X_train_scaled.shape[1], 1)),\n",
    "    MaxPooling1D(2),\n",
    "    Conv1D(128, 3, activation='relu'),\n",
    "    MaxPooling1D(2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')  # Poiché si tratta di una classificazione binaria\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Addestra il modello\n",
    "model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, validation_data=(X_test_scaled, y_test))\n",
    "\n",
    "# Prevedi gli outlier nel dataset di test\n",
    "y_pred = (model.predict(X_test_scaled) > 0.5).astype(\"int32\")\n",
    "y_predicted_score = model.predict(X_test_scaled)\n",
    "\n",
    "metrics = evaluate_metrics(y_test, y_pred, y_predicted_score)\n",
    "\n",
    "# Stampa i risultati\n",
    "print(model, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f797308e",
   "metadata": {},
   "source": [
    "# Rocket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "83d67943",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Il numero di campioni non corrisponde!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m features \u001b[38;5;241m=\u001b[39m rocket\u001b[38;5;241m.\u001b[39mtransform(X_train_scaled)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Verifica che il numero di campioni di features e y_test sia lo stesso\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m features\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m y_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIl numero di campioni non corrisponde!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# 3. Rilevamento delle anomalie\u001b[39;00m\n\u001b[0;32m     15\u001b[0m model \u001b[38;5;241m=\u001b[39m XGBOD(contamination\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)  \u001b[38;5;66;03m# Modello non supervisionato\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Il numero di campioni non corrisponde!"
     ]
    }
   ],
   "source": [
    "from sktime.transformations.panel.rocket import Rocket\n",
    "from pyod.models.xgbod import XGBOD\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 2. Applica ROCKET\n",
    "rocket = Rocket(num_kernels=10000)\n",
    "rocket.fit(X_train_scaled, y_train)\n",
    "features = rocket.transform(X_train_scaled)\n",
    "# Verifica che il numero di campioni di features e y_test sia lo stesso\n",
    "assert features.shape[0] == y_test.shape[0], \"Il numero di campioni non corrisponde!\"\n",
    "\n",
    "\n",
    "# 3. Rilevamento delle anomalie\n",
    "model = XGBOD(contamination=0.01, random_state=42)  # Modello non supervisionato\n",
    "anomaly_scores = model.fit_predict(features, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1007c0",
   "metadata": {},
   "source": [
    "## Rilevamento di anomalie UNSUPERVISED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4273bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomalie rilevate nel training set: [1 0 0 ... 0 0 0]\n",
      "Anomalie rilevate nel test set: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 1 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione sul test set:\n",
      " {'Accuracy': 0.832, 'Precision': 0.962, 'Recall': 0.221, 'F1': 0.36, 'MCC': 0.415, 'AUC_PR': 0.726, 'AUC_ROC': 0.772, 'PREC_N_SCORES': 0.646}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numba import njit, prange\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef, average_precision_score, roc_auc_score\n",
    "\n",
    "# Funzioni già definite in precedenza\n",
    "def generate_kernels(input_length, num_kernels):\n",
    "    candidate_lengths = np.array((7, 9, 11), dtype=np.int32)\n",
    "    lengths = np.random.choice(candidate_lengths, num_kernels)\n",
    "\n",
    "    weights = np.zeros(lengths.sum(), dtype=np.float64)\n",
    "    biases = np.zeros(num_kernels, dtype=np.float64)\n",
    "    dilations = np.zeros(num_kernels, dtype=np.int32)\n",
    "    paddings = np.zeros(num_kernels, dtype=np.int32)\n",
    "\n",
    "    a1 = 0\n",
    "    for i in range(num_kernels):\n",
    "        _length = lengths[i]\n",
    "        _weights = np.random.normal(0, 1, _length)\n",
    "        b1 = a1 + _length\n",
    "        weights[a1:b1] = _weights - _weights.mean()\n",
    "        biases[i] = np.random.uniform(-1, 1)\n",
    "        dilation = 2 ** np.random.uniform(0, np.log2((input_length - 1) / (_length - 1)))\n",
    "        dilation = np.int32(dilation)\n",
    "        dilations[i] = dilation\n",
    "        padding = ((_length - 1) * dilation) // 2 if np.random.randint(2) == 1 else 0\n",
    "        paddings[i] = padding\n",
    "        a1 = b1\n",
    "\n",
    "    return weights, lengths, biases, dilations, paddings\n",
    "\n",
    "@njit(fastmath=True)\n",
    "def apply_kernel(X, weights, length, bias, dilation, padding):\n",
    "    input_length = len(X)\n",
    "    output_length = (input_length + (2 * padding)) - ((length - 1) * dilation)\n",
    "    _ppv = 0\n",
    "    _max = np.NINF\n",
    "    _mean_sum = 0  # Per calcolare la media\n",
    "    end = (input_length + padding) - ((length - 1) * dilation)\n",
    "    for i in range(-padding, end):\n",
    "        _sum = bias\n",
    "        index = i\n",
    "        for j in range(length):\n",
    "            if index > -1 and index < input_length:\n",
    "                _sum += weights[j] * X[index]\n",
    "            index += dilation\n",
    "        _mean_sum += _sum  # Aggiungi al totale per la media\n",
    "        if _sum > _max:\n",
    "            _max = _sum\n",
    "        if _sum > 0:\n",
    "            _ppv += 1\n",
    "    mean_response = _mean_sum / output_length  # Calcola la media\n",
    "    return _ppv / output_length, _max, mean_response\n",
    "\n",
    "@njit(\"float64[:,:](float64[:,:],Tuple((float64[::1],int32[:],float64[:],int32[:],int32[:])))\", parallel=True, fastmath=True)\n",
    "def apply_kernels(X, kernels):\n",
    "    weights, lengths, biases, dilations, paddings = kernels\n",
    "    num_examples, _ = X.shape\n",
    "    num_kernels = len(lengths)\n",
    "    _X = np.zeros((num_examples, num_kernels * 3), dtype=np.float64)  # 3 features per kernel\n",
    "    for i in prange(num_examples):\n",
    "        a1 = 0  # Per i pesi\n",
    "        a2 = 0  # Per le caratteristiche\n",
    "        for j in range(num_kernels):\n",
    "            b1 = a1 + lengths[j]\n",
    "            b2 = a2 + 3\n",
    "            _X[i, a2:b2] = apply_kernel(\n",
    "                X[i], weights[a1:b1], lengths[j], biases[j], dilations[j], paddings[j]\n",
    "            )\n",
    "            a1 = b1\n",
    "            a2 = b2\n",
    "    return _X\n",
    "\n",
    "def detect_anomalies_with_threshold(scores, threshold):\n",
    "    return (scores > threshold).astype(int)\n",
    "\n",
    "# Genera kernel convoluzionali casuali\n",
    "input_length = X_train.shape[1]\n",
    "num_kernels = 10000\n",
    "kernels = generate_kernels(input_length, num_kernels)\n",
    "\n",
    "# Applica i kernel alle serie temporali\n",
    "features_train = apply_kernels(X_train2, kernels)\n",
    "features_test = apply_kernels(X_test2, kernels)\n",
    "\n",
    "# Sintesi delle caratteristiche per esempio\n",
    "anomaly_scores_train = np.mean(features_train, axis=1)  # Media\n",
    "anomaly_scores_test = np.mean(features_test, axis=1)  # Media\n",
    "\n",
    "# Rilevamento delle anomalie\n",
    "threshold = np.percentile(anomaly_scores_train , 95)\n",
    "anomaly_labels_train = detect_anomalies_with_threshold(anomaly_scores_train , threshold)\n",
    "anomaly_labels_test = detect_anomalies_with_threshold(anomaly_scores_test , threshold)\n",
    "\n",
    "# Visualizzazione dei risultati\n",
    "print(\"Anomalie rilevate nel training set:\", anomaly_labels_train)\n",
    "print(\"Anomalie rilevate nel test set:\", anomaly_labels_test)\n",
    "\n",
    "# Eseguiamo la valutazione delle metriche\n",
    "metrics = evaluate_metrics(y_test, anomaly_labels_test, y_proba=anomaly_scores_test)\n",
    "print(\"Metriche di valutazione sul test set:\\n\", metrics)\n",
    "# {'Accuracy': 0.832, 'Precision': 0.962, 'Recall': 0.221, 'F1': 0.36, 'MCC': 0.415, 'AUC_PR': 0.726, 'AUC_ROC': 0.772, 'PREC_N_SCORES': 0.646}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2409f2d8",
   "metadata": {},
   "source": [
    "## Rilevamento di anomalie SUPERVISED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "53ffb220",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 131\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m# Addestramento del modello supervisionato\u001b[39;00m\n\u001b[0;32m    130\u001b[0m model \u001b[38;5;241m=\u001b[39m XGBOD(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, random_state\u001b[38;5;241m=\u001b[39mSEED)\n\u001b[1;32m--> 131\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;66;03m# Predizione delle anomalie nei dati di test\u001b[39;00m\n\u001b[0;32m    134\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(features_test)\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\pyod\\models\\xgbod.py:317\u001b[0m, in \u001b[0;36mXGBOD.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m ind, estimator \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator_list):\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstandardization_flag_list[ind]:\n\u001b[1;32m--> 317\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_norm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    318\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX_train_add_[:, ind] \u001b[38;5;241m=\u001b[39m estimator\u001b[38;5;241m.\u001b[39mdecision_scores_\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\pyod\\models\\ocsvm.py:168\u001b[0m, in \u001b[0;36mOCSVM.fit\u001b[1;34m(self, X, y, sample_weight, **params)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetector_\u001b[38;5;241m.\u001b[39mfit(X\u001b[38;5;241m=\u001b[39mX, y\u001b[38;5;241m=\u001b[39my, sample_weight\u001b[38;5;241m=\u001b[39msample_weight,\n\u001b[0;32m    164\u001b[0m                    \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m    166\u001b[0m \u001b[38;5;66;03m# invert decision_scores_. Outliers comes with higher outlier scores\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecision_scores_ \u001b[38;5;241m=\u001b[39m invert_order(\n\u001b[1;32m--> 168\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetector_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecision_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_decision_scores()\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\sklearn\\svm\\_classes.py:1833\u001b[0m, in \u001b[0;36mOneClassSVM.decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1818\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecision_function\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m   1819\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Signed distance to the separating hyperplane.\u001b[39;00m\n\u001b[0;32m   1820\u001b[0m \n\u001b[0;32m   1821\u001b[0m \u001b[38;5;124;03m    Signed distance is positive for an inlier and negative for an outlier.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1831\u001b[0m \u001b[38;5;124;03m        Returns the decision function of the samples.\u001b[39;00m\n\u001b[0;32m   1832\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1833\u001b[0m     dec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decision_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mravel()\n\u001b[0;32m   1834\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dec\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\sklearn\\svm\\_base.py:538\u001b[0m, in \u001b[0;36mBaseLibSVM._decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    536\u001b[0m     dec_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse_decision_function(X)\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 538\u001b[0m     dec_func \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dense_decision_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    540\u001b[0m \u001b[38;5;66;03m# In binary case, we need to flip the sign of coef, intercept and\u001b[39;00m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;66;03m# decision function.\u001b[39;00m\n\u001b[0;32m    542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_impl \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mc_svc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnu_svc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\OPS-SAT-AD-main\\.venv\\Lib\\site-packages\\sklearn\\svm\\_base.py:554\u001b[0m, in \u001b[0;36mBaseLibSVM._dense_decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(kernel):\n\u001b[0;32m    552\u001b[0m     kernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecomputed\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 554\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlibsvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecision_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    555\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    556\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msupport_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    557\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msupport_vectors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    558\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_n_support\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    559\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dual_coef_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    560\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_intercept_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    561\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_probA\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    562\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_probB\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43msvm_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLIBSVM_IMPL\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_impl\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    564\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkernel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdegree\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdegree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcoef0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoef0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    569\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyod.models.xgbod import XGBOD\n",
    "from numba import njit, prange\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef, average_precision_score, roc_auc_score\n",
    "\n",
    "@njit(\"Tuple((float64[:],int32[:],float64[:],int32[:],int32[:]))(int64,int64)\")\n",
    "def generate_kernels(input_length, num_kernels):\n",
    "\n",
    "    candidate_lengths = np.array((7, 9, 11), dtype = np.int32)\n",
    "    lengths = np.random.choice(candidate_lengths, num_kernels)\n",
    "\n",
    "    weights = np.zeros(lengths.sum(), dtype = np.float64)\n",
    "    biases = np.zeros(num_kernels, dtype = np.float64)\n",
    "    dilations = np.zeros(num_kernels, dtype = np.int32)\n",
    "    paddings = np.zeros(num_kernels, dtype = np.int32)\n",
    "\n",
    "    a1 = 0\n",
    "\n",
    "    for i in range(num_kernels):\n",
    "\n",
    "        _length = lengths[i]\n",
    "\n",
    "        _weights = np.random.normal(0, 1, _length)\n",
    "\n",
    "        b1 = a1 + _length\n",
    "        weights[a1:b1] = _weights - _weights.mean()\n",
    "\n",
    "        biases[i] = np.random.uniform(-1, 1)\n",
    "\n",
    "        dilation = 2 ** np.random.uniform(0, np.log2((input_length - 1) / (_length - 1)))\n",
    "        dilation = np.int32(dilation)\n",
    "        dilations[i] = dilation\n",
    "\n",
    "        padding = ((_length - 1) * dilation) // 2 if np.random.randint(2) == 1 else 0\n",
    "        paddings[i] = padding\n",
    "\n",
    "        a1 = b1\n",
    "\n",
    "    return weights, lengths, biases, dilations, paddings\n",
    "\n",
    "@njit(fastmath = True)\n",
    "def apply_kernel(X, weights, length, bias, dilation, padding):\n",
    "\n",
    "    input_length = len(X)\n",
    "\n",
    "    output_length = (input_length + (2 * padding)) - ((length - 1) * dilation)\n",
    "\n",
    "    _ppv = 0\n",
    "    _max = np.NINF\n",
    "\n",
    "    end = (input_length + padding) - ((length - 1) * dilation)\n",
    "\n",
    "    for i in range(-padding, end):\n",
    "\n",
    "        _sum = bias\n",
    "\n",
    "        index = i\n",
    "\n",
    "        for j in range(length):\n",
    "\n",
    "            if index > -1 and index < input_length:\n",
    "\n",
    "                _sum = _sum + weights[j] * X[index]\n",
    "\n",
    "            index = index + dilation\n",
    "\n",
    "        if _sum > _max:\n",
    "            _max = _sum\n",
    "\n",
    "        if _sum > 0:\n",
    "            _ppv += 1\n",
    "\n",
    "    return _ppv / output_length, _max\n",
    "\n",
    "@njit(\"float64[:,:](float64[:,:],Tuple((float64[::1],int32[:],float64[:],int32[:],int32[:])))\", parallel = True, fastmath = True)\n",
    "def apply_kernels(X, kernels):\n",
    "\n",
    "    weights, lengths, biases, dilations, paddings = kernels\n",
    "\n",
    "    num_examples, _ = X.shape\n",
    "    num_kernels = len(lengths)\n",
    "\n",
    "    _X = np.zeros((num_examples, num_kernels * 2), dtype = np.float64) # 2 features per kernel\n",
    "\n",
    "    for i in prange(num_examples):\n",
    "\n",
    "        a1 = 0 # for weights\n",
    "        a2 = 0 # for features\n",
    "\n",
    "        for j in range(num_kernels):\n",
    "\n",
    "            b1 = a1 + lengths[j]\n",
    "            b2 = a2 + 2\n",
    "\n",
    "            _X[i, a2:b2] = \\\n",
    "            apply_kernel(X[i], weights[a1:b1], lengths[j], biases[j], dilations[j], paddings[j])\n",
    "\n",
    "            a1 = b1\n",
    "            a2 = b2\n",
    "\n",
    "    return _X\n",
    "    weights, lengths, biases, dilations, paddings = kernels\n",
    "    num_examples, _ = X.shape\n",
    "    num_kernels = len(lengths)\n",
    "    _X = np.zeros((num_examples, num_kernels * 3), dtype=np.float64)  # 3 features per kernel\n",
    "    for i in prange(num_examples):\n",
    "        a1 = 0  # Per i pesi\n",
    "        a2 = 0  # Per le caratteristiche\n",
    "        for j in range(num_kernels):\n",
    "            b1 = a1 + lengths[j]\n",
    "            b2 = a2 + 3\n",
    "            _X[i, a2:b2] = apply_kernel(\n",
    "                X[i], weights[a1:b1], lengths[j], biases[j], dilations[j], paddings[j]\n",
    "            )\n",
    "            a1 = b1\n",
    "            a2 = b2\n",
    "    return _X\n",
    "\n",
    "# Genera kernel convoluzionali casuali\n",
    "input_length = X_train.shape[1]\n",
    "num_kernels = 1000 # Valore standard\n",
    "kernels = generate_kernels(input_length, num_kernels)\n",
    "\n",
    "# Applica i kernel alle serie temporali\n",
    "features_train = apply_kernels(X_train_scaled, kernels)\n",
    "features_test = apply_kernels(X_test_scaled, kernels)\n",
    "\n",
    "# Addestramento del modello supervisionato\n",
    "model = XGBOD(n_estimators=100, max_depth=3, learning_rate=0.1, random_state=SEED)\n",
    "model.fit(features_train, y_train)\n",
    "\n",
    "# Predizione delle anomalie nei dati di test\n",
    "y_pred = model.predict(features_test)\n",
    "y_proba = model.predict_proba(features_test)\n",
    "\n",
    "# Visualizzazione dei risultati\n",
    "print(\"Predizioni nel test set:\", y_pred)\n",
    "\n",
    "# Eseguiamo la valutazione delle metriche\n",
    "metrics = evaluate_metrics(y_test, y_pred, y_proba=y_proba)\n",
    "print(\"Metriche di valutazione:\\n\", metrics)\n",
    "\n",
    "# Scaled -> {'Accuracy': 0.6, 'Precision': 0.7, 'Recall': 0.583, 'F1': 0.636, 'MCC': 0.204, 'AUC_PR': 0.632, 'AUC_ROC': 0.542}\n",
    "# Non Scaled -> {'Accuracy': 0.7, 'Precision': 0.75, 'Recall': 0.75, 'F1': 0.75, 'MCC': 0.375, 'AUC_PR': 0.712, 'AUC_ROC': 0.656}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8036570a",
   "metadata": {},
   "source": [
    "Con standard Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4756a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predizioni nel test set: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 1 0 1 0 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 1 0\n",
      " 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 1 0 1 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.845, 'Precision': 0.763, 'Recall': 0.398, 'F1': 0.523, 'MCC': 0.475, 'AUC_PR': 0.619, 'AUC_ROC': 0.811, 'PREC_N_SCORES': 0.54}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyod.models.iforest import IsolationForest\n",
    "from pyod.models.knn import KNN\n",
    "from numba import njit, prange\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef, average_precision_score, roc_auc_score\n",
    "\n",
    "@njit(\"Tuple((float64[:],int32[:],float64[:],int32[:],int32[:]))(int64,int64)\")\n",
    "def generate_kernels(input_length, num_kernels):\n",
    "\n",
    "    candidate_lengths = np.array((7, 9, 11), dtype = np.int32)\n",
    "    lengths = np.random.choice(candidate_lengths, num_kernels)\n",
    "\n",
    "    weights = np.zeros(lengths.sum(), dtype = np.float64)\n",
    "    biases = np.zeros(num_kernels, dtype = np.float64)\n",
    "    dilations = np.zeros(num_kernels, dtype = np.int32)\n",
    "    paddings = np.zeros(num_kernels, dtype = np.int32)\n",
    "\n",
    "    a1 = 0\n",
    "\n",
    "    for i in range(num_kernels):\n",
    "\n",
    "        _length = lengths[i]\n",
    "\n",
    "        _weights = np.random.normal(0, 1, _length)\n",
    "\n",
    "        b1 = a1 + _length\n",
    "        weights[a1:b1] = _weights - _weights.mean()\n",
    "\n",
    "        biases[i] = np.random.uniform(-1, 1)\n",
    "\n",
    "        dilation = 2 ** np.random.uniform(0, np.log2((input_length - 1) / (_length - 1)))\n",
    "        dilation = np.int32(dilation)\n",
    "        dilations[i] = dilation\n",
    "\n",
    "        padding = ((_length - 1) * dilation) // 2 if np.random.randint(2) == 1 else 0\n",
    "        paddings[i] = padding\n",
    "\n",
    "        a1 = b1\n",
    "\n",
    "    return weights, lengths, biases, dilations, paddings\n",
    "\n",
    "@njit(fastmath = True)\n",
    "def apply_kernel(X, weights, length, bias, dilation, padding):\n",
    "\n",
    "    input_length = len(X)\n",
    "\n",
    "    output_length = (input_length + (2 * padding)) - ((length - 1) * dilation)\n",
    "\n",
    "    _ppv = 0\n",
    "    _max = np.NINF\n",
    "\n",
    "    end = (input_length + padding) - ((length - 1) * dilation)\n",
    "\n",
    "    for i in range(-padding, end):\n",
    "\n",
    "        _sum = bias\n",
    "\n",
    "        index = i\n",
    "\n",
    "        for j in range(length):\n",
    "\n",
    "            if index > -1 and index < input_length:\n",
    "\n",
    "                _sum = _sum + weights[j] * X[index]\n",
    "\n",
    "            index = index + dilation\n",
    "\n",
    "        if _sum > _max:\n",
    "            _max = _sum\n",
    "\n",
    "        if _sum > 0:\n",
    "            _ppv += 1\n",
    "\n",
    "    return _ppv / output_length, _max\n",
    "\n",
    "@njit(\"float64[:,:](float64[:,:],Tuple((float64[::1],int32[:],float64[:],int32[:],int32[:])))\", parallel = True, fastmath = True)\n",
    "def apply_kernels(X, kernels):\n",
    "\n",
    "    weights, lengths, biases, dilations, paddings = kernels\n",
    "\n",
    "    num_examples, _ = X.shape\n",
    "    num_kernels = len(lengths)\n",
    "\n",
    "    _X = np.zeros((num_examples, num_kernels * 2), dtype = np.float64) # 2 features per kernel\n",
    "\n",
    "    for i in prange(num_examples):\n",
    "\n",
    "        a1 = 0 # for weights\n",
    "        a2 = 0 # for features\n",
    "\n",
    "        for j in range(num_kernels):\n",
    "\n",
    "            b1 = a1 + lengths[j]\n",
    "            b2 = a2 + 2\n",
    "\n",
    "            _X[i, a2:b2] = \\\n",
    "            apply_kernel(X[i], weights[a1:b1], lengths[j], biases[j], dilations[j], paddings[j])\n",
    "\n",
    "            a1 = b1\n",
    "            a2 = b2\n",
    "\n",
    "    return _X\n",
    "\n",
    "# Genera kernel convoluzionali casuali\n",
    "input_length = X_train.shape[1]\n",
    "num_kernels = 1000\n",
    "kernels = generate_kernels(input_length, num_kernels)\n",
    "\n",
    "# Applica i kernel alle serie temporali\n",
    "features_train = apply_kernels(X_train2, kernels)\n",
    "features_test = apply_kernels(X_test2, kernels)\n",
    "\n",
    "\n",
    "# Addestramento del modello supervisionato\n",
    "model = KNN()\n",
    "model.fit(features_train)\n",
    "\n",
    "# Predizione delle anomalie nei dati di test\n",
    "y_pred = model.predict(features_test)\n",
    "y_proba = model.decision_function(features_test)\n",
    "\n",
    "# Visualizzazione dei risultati\n",
    "print(\"Predizioni nel test set:\", y_pred)\n",
    "\n",
    "# Eseguiamo la valutazione delle metriche\n",
    "metrics = evaluate_metrics(y_test, y_pred, y_proba)\n",
    "print(\"Metriche di valutazione:\\n\", metrics)\n",
    "# {'Accuracy': 0.845, 'Precision': 0.763, 'Recall': 0.398, 'F1': 0.523, 'MCC': 0.475, 'AUC_PR': 0.619, 'AUC_ROC': 0.811, 'PREC_N_SCORES': 0.54}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0eed09",
   "metadata": {},
   "source": [
    "### Regressione Logistica -> Classificatore lineare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2b43ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predizioni nel test set: [0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1\n",
      " 0 0 1 0 0 1 1 0 1 1 1 0 1 0 0 0 0 0 0 1 1 1 0 1 0 0 1 0 1 1 1 0 1 1 1 1 1\n",
      " 0 1 0 1 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 1 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1 0 1 1 0 0 0\n",
      " 0 0 1 0 1 0 0 1 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0\n",
      " 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 1 1 1 1 0 0 0 0 0 0\n",
      " 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 1 0 1 0 1 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0\n",
      " 0 1 0 1 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.977, 'Precision': 0.972, 'Recall': 0.92, 'F1': 0.945, 'MCC': 0.932, 'AUC_PR': 0.962, 'AUC_ROC': 0.984, 'PREC_N_SCORES': 0.929}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from pyod.models.knn import KNN\n",
    "from numba import njit, prange\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef, average_precision_score, roc_auc_score\n",
    "\n",
    "@njit(\"Tuple((float64[:],int32[:],float64[:],int32[:],int32[:]))(int64,int64)\")\n",
    "def generate_kernels(input_length, num_kernels):\n",
    "\n",
    "    candidate_lengths = np.array((7, 9, 11), dtype = np.int32)\n",
    "    lengths = np.random.choice(candidate_lengths, num_kernels)\n",
    "\n",
    "    weights = np.zeros(lengths.sum(), dtype = np.float64)\n",
    "    biases = np.zeros(num_kernels, dtype = np.float64)\n",
    "    dilations = np.zeros(num_kernels, dtype = np.int32)\n",
    "    paddings = np.zeros(num_kernels, dtype = np.int32)\n",
    "\n",
    "    a1 = 0\n",
    "\n",
    "    for i in range(num_kernels):\n",
    "\n",
    "        _length = lengths[i]\n",
    "\n",
    "        _weights = np.random.normal(0, 1, _length)\n",
    "\n",
    "        b1 = a1 + _length\n",
    "        weights[a1:b1] = _weights - _weights.mean()\n",
    "\n",
    "        biases[i] = np.random.uniform(-1, 1)\n",
    "\n",
    "        dilation = 2 ** np.random.uniform(0, np.log2((input_length - 1) / (_length - 1)))\n",
    "        dilation = np.int32(dilation)\n",
    "        dilations[i] = dilation\n",
    "\n",
    "        padding = ((_length - 1) * dilation) // 2 if np.random.randint(2) == 1 else 0\n",
    "        paddings[i] = padding\n",
    "\n",
    "        a1 = b1\n",
    "\n",
    "    return weights, lengths, biases, dilations, paddings\n",
    "\n",
    "@njit(fastmath = True)\n",
    "def apply_kernel(X, weights, length, bias, dilation, padding):\n",
    "\n",
    "    input_length = len(X)\n",
    "\n",
    "    output_length = (input_length + (2 * padding)) - ((length - 1) * dilation)\n",
    "\n",
    "    _ppv = 0\n",
    "    _max = np.NINF\n",
    "\n",
    "    end = (input_length + padding) - ((length - 1) * dilation)\n",
    "\n",
    "    for i in range(-padding, end):\n",
    "\n",
    "        _sum = bias\n",
    "\n",
    "        index = i\n",
    "\n",
    "        for j in range(length):\n",
    "\n",
    "            if index > -1 and index < input_length:\n",
    "\n",
    "                _sum = _sum + weights[j] * X[index]\n",
    "\n",
    "            index = index + dilation\n",
    "\n",
    "        if _sum > _max:\n",
    "            _max = _sum\n",
    "\n",
    "        if _sum > 0:\n",
    "            _ppv += 1\n",
    "\n",
    "    return _ppv / output_length, _max\n",
    "\n",
    "@njit(\"float64[:,:](float64[:,:],Tuple((float64[::1],int32[:],float64[:],int32[:],int32[:])))\", parallel = True, fastmath = True)\n",
    "def apply_kernels(X, kernels):\n",
    "\n",
    "    weights, lengths, biases, dilations, paddings = kernels\n",
    "\n",
    "    num_examples, _ = X.shape\n",
    "    num_kernels = len(lengths)\n",
    "\n",
    "    _X = np.zeros((num_examples, num_kernels * 2), dtype = np.float64) # 2 features per kernel\n",
    "\n",
    "    for i in prange(num_examples):\n",
    "\n",
    "        a1 = 0 # for weights\n",
    "        a2 = 0 # for features\n",
    "\n",
    "        for j in range(num_kernels):\n",
    "\n",
    "            b1 = a1 + lengths[j]\n",
    "            b2 = a2 + 2\n",
    "\n",
    "            _X[i, a2:b2] = \\\n",
    "            apply_kernel(X[i], weights[a1:b1], lengths[j], biases[j], dilations[j], paddings[j])\n",
    "\n",
    "            a1 = b1\n",
    "            a2 = b2\n",
    "\n",
    "    return _X\n",
    "\n",
    "# Genera kernel convoluzionali casuali\n",
    "input_length = X_train.shape[1]\n",
    "num_kernels = 1000\n",
    "kernels = generate_kernels(input_length, num_kernels)\n",
    "\n",
    "# Applica i kernel alle serie temporali\n",
    "features_train = apply_kernels(X_train2, kernels)\n",
    "features_test = apply_kernels(X_test2, kernels)\n",
    "\n",
    "\n",
    "# Addestramento del modello supervisionato\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(features_train, y_train)\n",
    "\n",
    "# Predizione delle anomalie nei dati di test\n",
    "y_pred = model.predict(features_test)\n",
    "y_proba = model.decision_function(features_test)\n",
    "\n",
    "# Visualizzazione dei risultati\n",
    "print(\"Predizioni nel test set:\", y_pred)\n",
    "\n",
    "# Eseguiamo la valutazione delle metriche\n",
    "metrics = evaluate_metrics(y_test, y_pred, y_proba)\n",
    "print(\"Metriche di valutazione:\\n\", metrics)\n",
    "# {'Accuracy': 0.977, 'Precision': 0.972, 'Recall': 0.92, 'F1': 0.945, 'MCC': 0.932, 'AUC_PR': 0.962, 'AUC_ROC': 0.984, 'PREC_N_SCORES': 0.929}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a539711",
   "metadata": {},
   "source": [
    "### Prova con Dettagli dal GitHub del Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ee46bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predizioni nel test set: [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 1 0 1 1 1 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 1 1 1 0 0 0 1 1 1\n",
      " 0 1 0 1 0 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 1 0 0 0\n",
      " 0 0 1 0 0 0 0 0 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1 1 1 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.888, 'Precision': 0.966, 'Recall': 0.496, 'F1': 0.655, 'MCC': 0.644, 'AUC_PR': 0.922, 'AUC_ROC': 0.962, 'PREC_N_SCORES': 0.912}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from numba import njit, prange\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef, average_precision_score, roc_auc_score\n",
    "\n",
    "@njit(\"Tuple((float64[:],int32[:],float64[:],int32[:],int32[:]))(int64,int64)\")\n",
    "def generate_kernels(input_length, num_kernels):\n",
    "\n",
    "    candidate_lengths = np.array((7, 9, 11), dtype = np.int32)\n",
    "    lengths = np.random.choice(candidate_lengths, num_kernels)\n",
    "\n",
    "    weights = np.zeros(lengths.sum(), dtype = np.float64)\n",
    "    biases = np.zeros(num_kernels, dtype = np.float64)\n",
    "    dilations = np.zeros(num_kernels, dtype = np.int32)\n",
    "    paddings = np.zeros(num_kernels, dtype = np.int32)\n",
    "\n",
    "    a1 = 0\n",
    "\n",
    "    for i in range(num_kernels):\n",
    "\n",
    "        _length = lengths[i]\n",
    "\n",
    "        _weights = np.random.normal(0, 1, _length)\n",
    "\n",
    "        b1 = a1 + _length\n",
    "        weights[a1:b1] = _weights - _weights.mean()\n",
    "\n",
    "        biases[i] = np.random.uniform(-1, 1)\n",
    "\n",
    "        dilation = 2 ** np.random.uniform(0, np.log2((input_length - 1) / (_length - 1)))\n",
    "        dilation = np.int32(dilation)\n",
    "        dilations[i] = dilation\n",
    "\n",
    "        padding = ((_length - 1) * dilation) // 2 if np.random.randint(2) == 1 else 0\n",
    "        paddings[i] = padding\n",
    "\n",
    "        a1 = b1\n",
    "\n",
    "    return weights, lengths, biases, dilations, paddings\n",
    "\n",
    "@njit(fastmath = True)\n",
    "def apply_kernel(X, weights, length, bias, dilation, padding):\n",
    "\n",
    "    input_length = len(X)\n",
    "\n",
    "    output_length = (input_length + (2 * padding)) - ((length - 1) * dilation)\n",
    "\n",
    "    _ppv = 0\n",
    "    _max = np.NINF\n",
    "\n",
    "    end = (input_length + padding) - ((length - 1) * dilation)\n",
    "\n",
    "    for i in range(-padding, end):\n",
    "\n",
    "        _sum = bias\n",
    "\n",
    "        index = i\n",
    "\n",
    "        for j in range(length):\n",
    "\n",
    "            if index > -1 and index < input_length:\n",
    "\n",
    "                _sum = _sum + weights[j] * X[index]\n",
    "\n",
    "            index = index + dilation\n",
    "\n",
    "        if _sum > _max:\n",
    "            _max = _sum\n",
    "\n",
    "        if _sum > 0:\n",
    "            _ppv += 1\n",
    "\n",
    "    return _ppv / output_length, _max\n",
    "\n",
    "@njit(\"float64[:,:](float64[:,:],Tuple((float64[::1],int32[:],float64[:],int32[:],int32[:])))\", parallel = True, fastmath = True)\n",
    "def apply_kernels(X, kernels):\n",
    "\n",
    "    weights, lengths, biases, dilations, paddings = kernels\n",
    "\n",
    "    num_examples, _ = X.shape\n",
    "    num_kernels = len(lengths)\n",
    "\n",
    "    _X = np.zeros((num_examples, num_kernels * 2), dtype = np.float64) # 2 features per kernel\n",
    "\n",
    "    for i in prange(num_examples):\n",
    "\n",
    "        a1 = 0 # for weights\n",
    "        a2 = 0 # for features\n",
    "\n",
    "        for j in range(num_kernels):\n",
    "\n",
    "            b1 = a1 + lengths[j]\n",
    "            b2 = a2 + 2\n",
    "\n",
    "            _X[i, a2:b2] = \\\n",
    "            apply_kernel(X[i], weights[a1:b1], lengths[j], biases[j], dilations[j], paddings[j])\n",
    "\n",
    "            a1 = b1\n",
    "            a2 = b2\n",
    "\n",
    "    return _X\n",
    "\n",
    "def detect_anomalies_with_threshold(scores, threshold):\n",
    "    return (scores > threshold).astype(int)\n",
    "\n",
    "\n",
    "# Genera kernel convoluzionali casuali\n",
    "input_length = X_train.shape[1]\n",
    "num_kernels = 1000\n",
    "kernels = generate_kernels(input_length, num_kernels)\n",
    "\n",
    "# Applica i kernel alle serie temporali\n",
    "features_train = apply_kernels(X_train2, kernels)\n",
    "features_test = apply_kernels(X_test2, kernels)\n",
    "\n",
    "\n",
    "# Addestramento del modello supervisionato\n",
    "model = Ridge(alpha=1.0)\n",
    "model.fit(features_train, y_train)\n",
    "\n",
    "# Predizione delle anomalie nei dati di test\n",
    "anomaly_scores_test = model.predict(features_test)\n",
    "anomaly_scores_train = model.predict(features_train)\n",
    "\n",
    "# Rilevamento delle anomalie\n",
    "threshold = np.percentile(anomaly_scores_train , 95)\n",
    "anomaly_labels_train = detect_anomalies_with_threshold(anomaly_scores_train , threshold)\n",
    "anomaly_labels_test = detect_anomalies_with_threshold(anomaly_scores_test , threshold)\n",
    "\n",
    "# Visualizzazione dei risultati\n",
    "print(\"Predizioni nel test set:\", anomaly_labels_test)\n",
    "\n",
    "# Eseguiamo la valutazione delle metriche\n",
    "metrics = evaluate_metrics(y_test, anomaly_labels_test, y_proba=anomaly_scores_test)\n",
    "print(\"Metriche di valutazione:\\n\", metrics)\n",
    "#  {'Accuracy': 0.888, 'Precision': 0.966, 'Recall': 0.496, 'F1': 0.655, 'MCC': 0.644, 'AUC_PR': 0.922, 'AUC_ROC': 0.962, 'PREC_N_SCORES': 0.912}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
