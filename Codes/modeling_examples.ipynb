{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e613346-c162-4bf8-908d-8e6a5cff0746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, matthews_corrcoef, average_precision_score\n",
    "from pyod.utils.data import precision_n_scores\n",
    "from pyod.models.iforest import IForest\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# Per l'uso della memoria degli algoritmi\n",
    "from memory_profiler import memory_usage\n",
    "# Per la metrica sul tempo di Addestramento e Inferenza\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba180484-282d-4605-990e-bf354cf53bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_metrics(y_test, y_pred, y_proba=None, digits=3):\n",
    "    res = {\"Accuracy\": round(accuracy_score(y_test, y_pred), digits),\n",
    "           \"Precision\": precision_score(y_test, y_pred).round(digits),\n",
    "           \"Recall\": recall_score(y_test, y_pred).round(digits),\n",
    "           \"F1\": f1_score(y_test, y_pred).round(digits),\n",
    "           \"MCC\": round(matthews_corrcoef(y_test, y_pred), ndigits=digits)}\n",
    "    if y_proba is not None:\n",
    "        res[\"AUC_PR\"] = average_precision_score(y_test, y_proba).round(digits)\n",
    "        res[\"AUC_ROC\"] = roc_auc_score(y_test, y_proba).round(digits)\n",
    "        res[\"PREC_N_SCORES\"] = precision_n_scores(y_test, y_proba).round(digits)\n",
    "    return res\n",
    "\n",
    "\n",
    "def set_seed_numpy(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc90bc09-5125-4641-bf52-cd0d4cc7a122",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\n",
    "    \"mean\", \"var\", \"std\", \"len\", \"duration\", \"len_weighted\", \"gaps_squared\", \"n_peaks\",\n",
    "    \"smooth10_n_peaks\", \"smooth20_n_peaks\", \"var_div_duration\", \"var_div_len\",\n",
    "    \"diff_peaks\", \"diff2_peaks\", \"diff_var\", \"diff2_var\", \"kurtosis\", \"skew\",\n",
    "]\n",
    "SEED = 2137"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3fffebd2-36a4-4c5f-82e0-3b9e4f6c3ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "segment\n",
      "1       1\n",
      "2       1\n",
      "3       1\n",
      "4       1\n",
      "6       0\n",
      "       ..\n",
      "2118    0\n",
      "2120    0\n",
      "2121    0\n",
      "2122    0\n",
      "2123    1\n",
      "Name: anomaly, Length: 1594, dtype: int64\n",
      "X_train (1594, 18)\n",
      "X_test (529, 18)\n",
      "X_train2 (1594, 18)\n",
      "X_test2 (529, 18)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/dataset.csv\", index_col=\"segment\")\n",
    "\n",
    "X_train, y_train = df.loc[df.train==1, features], df.loc[df.train==1, \"anomaly\"]\n",
    "print(y_train)\n",
    "X_test, y_test = df.loc[df.train==0, features], df.loc[df.train==0, \"anomaly\"]\n",
    "X_train_nominal = df.loc[(df.anomaly==0)&(df.train==1), features]\n",
    "\n",
    "prep = StandardScaler()\n",
    "X_train_nominal2 = prep.fit_transform(X_train_nominal)\n",
    "X_train2 = prep.transform(X_train)\n",
    "X_test2 = prep.transform(X_test)\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"X_train\", X_train.shape)\n",
    "print(\"X_test\", X_test.shape)\n",
    "print(\"X_train2\", X_train2.shape)\n",
    "print(\"X_test2\", X_test2.shape)\n",
    "\n",
    "# # Dataset NASA -> SOlar Orbiter\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# featuresNASA = [\n",
    "#     \"Radial Distance from Sun(AU)\", \n",
    "#     \"Electronic Box Temperature(DegC)\",\n",
    "#     \"Out Board Sensor Temperature(DegC)\",\n",
    "#     \"In Board Sensor Temperature(DegC)\",\n",
    "#     \"Search Coil Magnetometers Temperature(DegC)\",\n",
    "#     \"Solar Array Angle(Deg)\",\n",
    "#     \"High Gain Antenna azimuth(Deg)\",\n",
    "#     \"High Gain Antenna Elevation(Deg)\",\n",
    "#     \"IBS_R\",\n",
    "#     \"IBS_T\",\n",
    "#     \"IBS_N\",\n",
    "#     \"IBS_time\",\n",
    "#     \"OBS_R\",\n",
    "#     \"OBS_T\",\n",
    "#     \"OBS_N\"\n",
    "# ]\n",
    "\n",
    "# dNASA = pd.read_csv(\"data/Solar_Orbiter.csv\", index_col=\"Date\")\n",
    "\n",
    "# # Filtraggio delle colonne specificate\n",
    "# X = dNASA[featuresNASA]\n",
    "\n",
    "# # Rimozione delle righe con valori mancanti (opzionale)\n",
    "# X_cleaned = X.dropna()\n",
    "\n",
    "# # Divisione dei dati in training e test set\n",
    "# X_trainNASA, X_testNASA = train_test_split(X, test_size=0.2, random_state=42)\n",
    "# # Output dei risultati\n",
    "# print(\"Train set:\", X_trainNASA.shape)\n",
    "# print(\"Test set:\", X_testNASA.shape)\n",
    "\n",
    "# X_trainNASA = X_trainNASA.fillna(X_trainNASA.mean())\n",
    "# X_testNASA = X_testNASA.fillna(X_trainNASA.mean())\n",
    "\n",
    "# prep.fit(X_trainNASA)  # Calcolo delle statistiche (media, deviazione standard)\n",
    "# X_trainNASA2 = prep.transform(X_trainNASA)\n",
    "# X_testNASA2 = prep.transform(X_testNASA)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5cb2f65-dba5-431f-a7c1-dc4db5d1fb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed_numpy(SEED) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938333f9",
   "metadata": {},
   "source": [
    "# Supervised Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "396d74e0-6e0c-40c2-beaf-ea856b6a22d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoostClassifier(random_state=2137) \n",
      " {'Accuracy': 0.934, 'Precision': 0.89, 'Recall': 0.788, 'F1': 0.836, 'MCC': 0.797, 'AUC_PR': 0.923, 'AUC_ROC': 0.962, 'PREC_N_SCORES': 0.841}\n"
     ]
    }
   ],
   "source": [
    "model = AdaBoostClassifier(random_state=SEED)\n",
    "model.fit(X_train2, y_train)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb2e8e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=3, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=50, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=2137, ...) \n",
      " {'Accuracy': 0.957, 'Precision': 0.959, 'Recall': 0.832, 'F1': 0.891, 'MCC': 0.867, 'AUC_PR': 0.961, 'AUC_ROC': 0.986, 'PREC_N_SCORES': 0.876}\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "y_train_np = y_train\n",
    "\n",
    "model = xgb.XGBClassifier (\n",
    "    n_estimators=50,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    random_state=SEED\n",
    ")\n",
    "model.fit(X_train, y_train)\n",
    "# Previsioni e probabilità di previsione\n",
    "y_predicted = model.predict(X_test)\n",
    "y_predicted_score = model.predict_proba(X_test)[:, 1]  # Probabilità per la classe positiva\n",
    "# Questa è la probabilità che la classificazione sia corretta\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8aa6aa90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
      "              colsample_bylevel=None, colsample_bynode=None,\n",
      "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
      "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
      "              gamma=None, grow_policy=None, importance_type=None,\n",
      "              interaction_constraints=None, learning_rate=0.1, max_bin=None,\n",
      "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
      "              max_delta_step=None, max_depth=3, max_leaves=None,\n",
      "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
      "              multi_strategy=None, n_estimators=50, n_jobs=None,\n",
      "              num_parallel_tree=None, random_state=2137, ...) \n",
      " {'Accuracy': 0.953, 'Precision': 0.94, 'Recall': 0.832, 'F1': 0.883, 'MCC': 0.856, 'AUC_PR': 0.949, 'AUC_ROC': 0.976, 'PREC_N_SCORES': 0.867}\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "y_train_np = y_train\n",
    "\n",
    "model = xgb.XGBClassifier (\n",
    "    n_estimators=50,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    random_state=SEED\n",
    ")\n",
    "model.fit(X_train_scaled, y_train)\n",
    "# Previsioni e probabilità di previsione\n",
    "y_predicted = model.predict(X_test_scaled)\n",
    "y_predicted_score = model.predict_proba(X_test_scaled)[:, 1]  # Probabilità per la classe positiva\n",
    "# Questa è la probabilità che la classificazione sia corretta\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f258319c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': 0.928, 'Precision': 0.921, 'Recall': 0.726, 'F1': 0.812, 'MCC': 0.777, 'AUC_PR': 0.949, 'AUC_ROC': 0.976, 'PREC_N_SCORES': 0.867}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Inizializza e addestra il modello\n",
    "model = LinearSVC()\n",
    "model.fit(X_train2, y_train)\n",
    "\n",
    "# Predizione\n",
    "y_test_scores = model.decision_function(X_test2)\n",
    "# Previsioni e probabilità di previsione\n",
    "y_predicted = model.predict(X_test2)\n",
    "\n",
    "# Questa è la probabilità che la classificazione sia corretta\n",
    "print(evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d90807a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': 0.924, 'Precision': 0.92, 'Recall': 0.708, 'F1': 0.8, 'MCC': 0.764, 'AUC_PR': 0.949, 'AUC_ROC': 0.976, 'PREC_N_SCORES': 0.867}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Inizializza e addestra il modello\n",
    "model = LogisticRegression(max_iter=500)\n",
    "model.fit(X_train2, y_train)\n",
    "\n",
    "# Predizione\n",
    "y_test_scores = model.decision_function(X_test2)\n",
    "# Previsioni e probabilità di previsione\n",
    "y_predicted = model.predict(X_test2)\n",
    "\n",
    "# Questa è la probabilità che la classificazione sia corretta\n",
    "print(evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c5f6a6",
   "metadata": {},
   "source": [
    "## Unsupervised Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b5d242",
   "metadata": {},
   "source": [
    "MO_GAAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76d742e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyod\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmo_gaal\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MO_GAAL\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      3\u001b[0m os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTF_USE_LEGACY_KERAS\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrue\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\mo_gaal.py:13\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 13\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplease install torch first\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\__init__.py:2016\u001b[0m\n\u001b[0;32m   2009\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_compile\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _disable_dynamo  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[0;32m   2011\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[0;32m   2012\u001b[0m \u001b[38;5;66;03m# Import interface functions defined in Python\u001b[39;00m\n\u001b[0;32m   2013\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[0;32m   2014\u001b[0m \n\u001b[0;32m   2015\u001b[0m \u001b[38;5;66;03m# needs to be after the above ATen bindings so we can overwrite from Python side\u001b[39;00m\n\u001b[1;32m-> 2016\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _VF \u001b[38;5;28;01mas\u001b[39;00m _VF, functional \u001b[38;5;28;01mas\u001b[39;00m functional  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[0;32m   2017\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# usort: skip # noqa: F403\u001b[39;00m\n\u001b[0;32m   2019\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[0;32m   2020\u001b[0m \u001b[38;5;66;03m# Remove unnecessary members\u001b[39;00m\n\u001b[0;32m   2021\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\functional.py:7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Any, List, Optional, Sequence, Tuple, TYPE_CHECKING, Union\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _VF, Tensor\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _add_docstr\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\nn\\__init__.py:8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# mypy: allow-untyped-defs\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparameter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     Buffer \u001b[38;5;28;01mas\u001b[39;00m Buffer,\n\u001b[0;32m      4\u001b[0m     Parameter \u001b[38;5;28;01mas\u001b[39;00m Parameter,\n\u001b[0;32m      5\u001b[0m     UninitializedBuffer \u001b[38;5;28;01mas\u001b[39;00m UninitializedBuffer,\n\u001b[0;32m      6\u001b[0m     UninitializedParameter \u001b[38;5;28;01mas\u001b[39;00m UninitializedParameter,\n\u001b[0;32m      7\u001b[0m )\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# usort: skip # noqa: F403\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     attention \u001b[38;5;28;01mas\u001b[39;00m attention,\n\u001b[0;32m     11\u001b[0m     functional \u001b[38;5;28;01mas\u001b[39;00m functional,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m     utils \u001b[38;5;28;01mas\u001b[39;00m utils,\n\u001b[0;32m     17\u001b[0m )\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparallel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataParallel \u001b[38;5;28;01mas\u001b[39;00m DataParallel\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodule\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Module  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Bilinear, Identity, LazyLinear, Linear  \u001b[38;5;66;03m# usort: skip\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      4\u001b[0m     CELU,\n\u001b[0;32m      5\u001b[0m     ELU,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     32\u001b[0m     Threshold,\n\u001b[0;32m     33\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_prims_common\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DeviceLikeType\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparameter\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Buffer, Parameter\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_python_dispatch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_traceable_wrapper_subclass\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhooks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BackwardHook, RemovableHandle\n\u001b[0;32m     33\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregister_module_forward_pre_hook\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregister_module_forward_hook\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     42\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     43\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\utils\\__init__.py:8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mweakref\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      9\u001b[0m     backcompat \u001b[38;5;28;01mas\u001b[39;00m backcompat,\n\u001b[0;32m     10\u001b[0m     collect_env \u001b[38;5;28;01mas\u001b[39;00m collect_env,\n\u001b[0;32m     11\u001b[0m     data \u001b[38;5;28;01mas\u001b[39;00m data,\n\u001b[0;32m     12\u001b[0m     deterministic \u001b[38;5;28;01mas\u001b[39;00m deterministic,\n\u001b[0;32m     13\u001b[0m     hooks \u001b[38;5;28;01mas\u001b[39;00m hooks,\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend_registration\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     16\u001b[0m     generate_methods_for_privateuse1_backend,\n\u001b[0;32m     17\u001b[0m     rename_privateuse1_backend,\n\u001b[0;32m     18\u001b[0m )\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcpp_backtrace\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_cpp_backtrace\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\utils\\data\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataloader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     _DatasetKind,\n\u001b[0;32m      3\u001b[0m     DataLoader,\n\u001b[0;32m      4\u001b[0m     default_collate,\n\u001b[0;32m      5\u001b[0m     default_convert,\n\u001b[0;32m      6\u001b[0m     get_worker_info,\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_decorator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      9\u001b[0m     argument_validation,\n\u001b[0;32m     10\u001b[0m     functional_datapipe,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     runtime_validation_disabled,\n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipe\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     17\u001b[0m     DataChunk,\n\u001b[0;32m     18\u001b[0m     DFIterDataPipe,\n\u001b[0;32m     19\u001b[0m     IterDataPipe,\n\u001b[0;32m     20\u001b[0m     MapDataPipe,\n\u001b[0;32m     21\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mdist\u001b[39;00m\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph_settings\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExceptionWrapper\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _utils\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\utils\\data\\graph_settings.py:8\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping_extensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deprecated\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01miter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msharding\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      9\u001b[0m     _ShardingIterDataPipe,\n\u001b[0;32m     10\u001b[0m     SHARDING_PRIORITIES,\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraph\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataPipe, DataPipeGraph, traverse_dps\n\u001b[0;32m     15\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply_random_seed\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply_sharding\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mget_all_graph_pipes\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     21\u001b[0m ]\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\utils\\data\\datapipes\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dataframe \u001b[38;5;28;01mas\u001b[39;00m dataframe, \u001b[38;5;28miter\u001b[39m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;28miter\u001b[39m, \u001b[38;5;28mmap\u001b[39m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;28mmap\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\utils\\data\\datapipes\\iter\\__init__.py:5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01miter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallable\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     CollatorIterDataPipe \u001b[38;5;28;01mas\u001b[39;00m Collator,\n\u001b[0;32m      3\u001b[0m     MapperIterDataPipe \u001b[38;5;28;01mas\u001b[39;00m Mapper,\n\u001b[0;32m      4\u001b[0m )\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01miter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombinatorics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      6\u001b[0m     SamplerIterDataPipe \u001b[38;5;28;01mas\u001b[39;00m Sampler,\n\u001b[0;32m      7\u001b[0m     ShufflerIterDataPipe \u001b[38;5;28;01mas\u001b[39;00m Shuffler,\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01miter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcombining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     ConcaterIterDataPipe \u001b[38;5;28;01mas\u001b[39;00m Concater,\n\u001b[0;32m     11\u001b[0m     DemultiplexerIterDataPipe \u001b[38;5;28;01mas\u001b[39;00m Demultiplexer,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     ZipperIterDataPipe \u001b[38;5;28;01mas\u001b[39;00m Zipper,\n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatapipes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01miter\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfilelister\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     17\u001b[0m     FileListerIterDataPipe \u001b[38;5;28;01mas\u001b[39;00m FileLister,\n\u001b[0;32m     18\u001b[0m )\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1322\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1262\u001b[0m, in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1528\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1502\u001b[0m, in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1601\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:147\u001b[0m, in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pyod.models.mo_gaal import MO_GAAL\n",
    "import os\n",
    "os.environ['TF_USE_LEGACY_KERAS'] = 'True'\n",
    "\n",
    "model = MO_GAAL()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))\n",
    " # {'Accuracy': 0.896, 'Precision': 0.939, 'Recall': 0.549, 'F1': 0.693, 'MCC': 0.669, 'AUC_PR': 0.771, 'AUC_ROC': 0.849, 'PREC_N_SCORES': 0.699}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5629a030",
   "metadata": {},
   "source": [
    "ANO-GAAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af5e4c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train iter: 1\n",
      "Train iter: 2\n",
      "Train iter: 3\n",
      "Train iter: 4\n",
      "Train iter: 5\n",
      "Train iter: 6\n",
      "Train iter: 7\n",
      "Train iter: 8\n",
      "Train iter: 9\n",
      "Train iter: 10\n",
      "Train iter: 11\n",
      "Train iter: 12\n",
      "Train iter: 13\n",
      "Train iter: 14\n",
      "Train iter: 15\n",
      "Train iter: 16\n",
      "Train iter: 17\n",
      "Train iter: 18\n",
      "Train iter: 19\n",
      "Train iter: 20\n",
      "Train iter: 21\n",
      "Train iter: 22\n",
      "Train iter: 23\n",
      "Train iter: 24\n",
      "Train iter: 25\n",
      "Train iter: 26\n",
      "Train iter: 27\n",
      "Train iter: 28\n",
      "Train iter: 29\n",
      "Train iter: 30\n",
      "Train iter: 31\n",
      "Train iter: 32\n",
      "Train iter: 33\n",
      "Train iter: 34\n",
      "Train iter: 35\n",
      "Train iter: 36\n",
      "Train iter: 37\n",
      "Train iter: 38\n",
      "Train iter: 39\n",
      "Train iter: 40\n",
      "Train iter: 41\n",
      "Train iter: 42\n",
      "Train iter: 43\n",
      "Train iter: 44\n",
      "Train iter: 45\n",
      "Train iter: 46\n",
      "Train iter: 47\n",
      "Train iter: 48\n",
      "Train iter: 49\n",
      "Train iter: 50\n",
      "Train iter: 51\n",
      "Train iter: 52\n",
      "Train iter: 53\n",
      "Train iter: 54\n",
      "Train iter: 55\n",
      "Train iter: 56\n",
      "Train iter: 57\n",
      "Train iter: 58\n",
      "Train iter: 59\n",
      "Train iter: 60\n",
      "Train iter: 61\n",
      "Train iter: 62\n",
      "Train iter: 63\n",
      "Train iter: 64\n",
      "Train iter: 65\n",
      "Train iter: 66\n",
      "Train iter: 67\n",
      "Train iter: 68\n",
      "Train iter: 69\n",
      "Train iter: 70\n",
      "Train iter: 71\n",
      "Train iter: 72\n",
      "Train iter: 73\n",
      "Train iter: 74\n",
      "Train iter: 75\n",
      "Train iter: 76\n",
      "Train iter: 77\n",
      "Train iter: 78\n",
      "Train iter: 79\n",
      "Train iter: 80\n",
      "Train iter: 81\n",
      "Train iter: 82\n",
      "Train iter: 83\n",
      "Train iter: 84\n",
      "Train iter: 85\n",
      "Train iter: 86\n",
      "Train iter: 87\n",
      "Train iter: 88\n",
      "Train iter: 89\n",
      "Train iter: 90\n",
      "Train iter: 91\n",
      "Train iter: 92\n",
      "Train iter: 93\n",
      "Train iter: 94\n",
      "Train iter: 95\n",
      "Train iter: 96\n",
      "Train iter: 97\n",
      "Train iter: 98\n",
      "Train iter: 99\n",
      "Train iter: 100\n",
      "Train iter: 101\n",
      "Train iter: 102\n",
      "Train iter: 103\n",
      "Train iter: 104\n",
      "Train iter: 105\n",
      "Train iter: 106\n",
      "Train iter: 107\n",
      "Train iter: 108\n",
      "Train iter: 109\n",
      "Train iter: 110\n",
      "Train iter: 111\n",
      "Train iter: 112\n",
      "Train iter: 113\n",
      "Train iter: 114\n",
      "Train iter: 115\n",
      "Train iter: 116\n",
      "Train iter: 117\n",
      "Train iter: 118\n",
      "Train iter: 119\n",
      "Train iter: 120\n",
      "Train iter: 121\n",
      "Train iter: 122\n",
      "Train iter: 123\n",
      "Train iter: 124\n",
      "Train iter: 125\n",
      "Train iter: 126\n",
      "Train iter: 127\n",
      "Train iter: 128\n",
      "Train iter: 129\n",
      "Train iter: 130\n",
      "Train iter: 131\n",
      "Train iter: 132\n",
      "Train iter: 133\n",
      "Train iter: 134\n",
      "Train iter: 135\n",
      "Train iter: 136\n",
      "Train iter: 137\n",
      "Train iter: 138\n",
      "Train iter: 139\n",
      "Train iter: 140\n",
      "Train iter: 141\n",
      "Train iter: 142\n",
      "Train iter: 143\n",
      "Train iter: 144\n",
      "Train iter: 145\n",
      "Train iter: 146\n",
      "Train iter: 147\n",
      "Train iter: 148\n",
      "Train iter: 149\n",
      "Train iter: 150\n",
      "Train iter: 151\n",
      "Train iter: 152\n",
      "Train iter: 153\n",
      "Train iter: 154\n",
      "Train iter: 155\n",
      "Train iter: 156\n",
      "Train iter: 157\n",
      "Train iter: 158\n",
      "Train iter: 159\n",
      "Train iter: 160\n",
      "Train iter: 161\n",
      "Train iter: 162\n",
      "Train iter: 163\n",
      "Train iter: 164\n",
      "Train iter: 165\n",
      "Train iter: 166\n",
      "Train iter: 167\n",
      "Train iter: 168\n",
      "Train iter: 169\n",
      "Train iter: 170\n",
      "Train iter: 171\n",
      "Train iter: 172\n",
      "Train iter: 173\n",
      "Train iter: 174\n",
      "Train iter: 175\n",
      "Train iter: 176\n",
      "Train iter: 177\n",
      "Train iter: 178\n",
      "Train iter: 179\n",
      "Train iter: 180\n",
      "Train iter: 181\n",
      "Train iter: 182\n",
      "Train iter: 183\n",
      "Train iter: 184\n",
      "Train iter: 185\n",
      "Train iter: 186\n",
      "Train iter: 187\n",
      "Train iter: 188\n",
      "Train iter: 189\n",
      "Train iter: 190\n",
      "Train iter: 191\n",
      "Train iter: 192\n",
      "Train iter: 193\n",
      "Train iter: 194\n",
      "Train iter: 195\n",
      "Train iter: 196\n",
      "Train iter: 197\n",
      "Train iter: 198\n",
      "Train iter: 199\n",
      "Train iter: 200\n",
      "Train iter: 201\n",
      "Train iter: 202\n",
      "Train iter: 203\n",
      "Train iter: 204\n",
      "Train iter: 205\n",
      "Train iter: 206\n",
      "Train iter: 207\n",
      "Train iter: 208\n",
      "Train iter: 209\n",
      "Train iter: 210\n",
      "Train iter: 211\n",
      "Train iter: 212\n",
      "Train iter: 213\n",
      "Train iter: 214\n",
      "Train iter: 215\n",
      "Train iter: 216\n",
      "Train iter: 217\n",
      "Train iter: 218\n",
      "Train iter: 219\n",
      "Train iter: 220\n",
      "Train iter: 221\n",
      "Train iter: 222\n",
      "Train iter: 223\n",
      "Train iter: 224\n",
      "Train iter: 225\n",
      "Train iter: 226\n",
      "Train iter: 227\n",
      "Train iter: 228\n",
      "Train iter: 229\n",
      "Train iter: 230\n",
      "Train iter: 231\n",
      "Train iter: 232\n",
      "Train iter: 233\n",
      "Train iter: 234\n",
      "Train iter: 235\n",
      "Train iter: 236\n",
      "Train iter: 237\n",
      "Train iter: 238\n",
      "Train iter: 239\n",
      "Train iter: 240\n",
      "Train iter: 241\n",
      "Train iter: 242\n",
      "Train iter: 243\n",
      "Train iter: 244\n",
      "Train iter: 245\n",
      "Train iter: 246\n",
      "Train iter: 247\n",
      "Train iter: 248\n",
      "Train iter: 249\n",
      "Train iter: 250\n",
      "Train iter: 251\n",
      "Train iter: 252\n",
      "Train iter: 253\n",
      "Train iter: 254\n",
      "Train iter: 255\n",
      "Train iter: 256\n",
      "Train iter: 257\n",
      "Train iter: 258\n",
      "Train iter: 259\n",
      "Train iter: 260\n",
      "Train iter: 261\n",
      "Train iter: 262\n",
      "Train iter: 263\n",
      "Train iter: 264\n",
      "Train iter: 265\n",
      "Train iter: 266\n",
      "Train iter: 267\n",
      "Train iter: 268\n",
      "Train iter: 269\n",
      "Train iter: 270\n",
      "Train iter: 271\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m AnoGAN(verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# per stampare più cose\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m y_predicted \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test2)\n\u001b[0;32m     12\u001b[0m y_predicted_score \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdecision_function(X_test2)\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\anogan.py:314\u001b[0m, in \u001b[0;36mAnoGAN.fit\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    310\u001b[0m latent_noise \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(X_train_sel\u001b[38;5;241m.\u001b[39msize(\n\u001b[0;32m    311\u001b[0m     \u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatent_dim_G, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    313\u001b[0m generated_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerator(latent_noise)\n\u001b[1;32m--> 314\u001b[0m real_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiscriminator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_sel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    315\u001b[0m fake_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiscriminator(generated_data\u001b[38;5;241m.\u001b[39mdetach())\n\u001b[0;32m    317\u001b[0m loss_D_real \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCELoss()(real_output, torch\u001b[38;5;241m.\u001b[39mones_like(\n\u001b[0;32m    318\u001b[0m     real_output) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.9\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\anogan.py:87\u001b[0m, in \u001b[0;36mDiscriminator.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 87\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\activation.py:327\u001b[0m, in \u001b[0;36mSigmoid.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msigmoid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TF_USE_LEGACY_KERAS\"] = \"True\"\n",
    "\n",
    "# Ora importa PyOD e usa AnoGAN come prima\n",
    "from pyod.models.anogan import AnoGAN\n",
    "import tensorflow as tf\n",
    "\n",
    "model = AnoGAN(verbose=1) # per stampare più cose\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34682324",
   "metadata": {},
   "source": [
    "SO_GAAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e3d631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensione X_train: (1594, 18)\n",
      "Dimensione y_train: (1594,)\n",
      "Dimensione X_test: (529, 18)\n",
      "Dimensione y_test: (529,)\n",
      "Epoch 1 of 60\n",
      "Epoch 2 of 60\n",
      "Epoch 3 of 60\n",
      "Epoch 4 of 60\n",
      "Epoch 5 of 60\n",
      "Epoch 6 of 60\n",
      "Epoch 7 of 60\n",
      "Epoch 8 of 60\n",
      "Epoch 9 of 60\n",
      "Epoch 10 of 60\n",
      "Epoch 11 of 60\n",
      "Epoch 12 of 60\n",
      "Epoch 13 of 60\n",
      "Epoch 14 of 60\n",
      "Epoch 15 of 60\n",
      "Epoch 16 of 60\n",
      "Epoch 17 of 60\n",
      "Epoch 18 of 60\n",
      "Epoch 19 of 60\n",
      "Epoch 20 of 60\n",
      "Epoch 21 of 60\n",
      "Epoch 22 of 60\n",
      "Epoch 23 of 60\n",
      "Epoch 24 of 60\n",
      "Epoch 25 of 60\n",
      "Epoch 26 of 60\n",
      "Epoch 27 of 60\n",
      "Epoch 28 of 60\n",
      "Epoch 29 of 60\n",
      "Epoch 30 of 60\n",
      "Epoch 31 of 60\n",
      "Epoch 32 of 60\n",
      "Epoch 33 of 60\n",
      "Epoch 34 of 60\n",
      "Epoch 35 of 60\n",
      "Epoch 36 of 60\n",
      "Epoch 37 of 60\n",
      "Epoch 38 of 60\n",
      "Epoch 39 of 60\n",
      "Epoch 40 of 60\n",
      "Epoch 41 of 60\n",
      "Epoch 42 of 60\n",
      "Epoch 43 of 60\n",
      "Epoch 44 of 60\n",
      "Epoch 45 of 60\n",
      "Epoch 46 of 60\n",
      "Epoch 47 of 60\n",
      "Epoch 48 of 60\n",
      "Epoch 49 of 60\n",
      "Epoch 50 of 60\n",
      "Epoch 51 of 60\n",
      "Epoch 52 of 60\n",
      "Epoch 53 of 60\n",
      "Epoch 54 of 60\n",
      "Epoch 55 of 60\n",
      "Epoch 56 of 60\n",
      "Epoch 57 of 60\n",
      "Epoch 58 of 60\n",
      "Epoch 59 of 60\n",
      "Epoch 60 of 60\n",
      "SO_GAAL(contamination=0.1, lr_d=0.01, lr_g=0.0001, momentum=0.9,\n",
      "    stop_epochs=20) \n",
      " {'Accuracy': 0.887, 'Precision': 0.934, 'Recall': 0.504, 'F1': 0.655, 'MCC': 0.635, 'AUC_PR': 0.757, 'AUC_ROC': 0.839, 'PREC_N_SCORES': 0.681}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.so_gaal import SO_GAAL\n",
    "\n",
    "# Verifica le dimensioni dei dati generati\n",
    "print(\"Dimensione X_train:\", X_train.shape)\n",
    "print(\"Dimensione y_train:\", y_train.shape)\n",
    "print(\"Dimensione X_test:\", X_test.shape)\n",
    "print(\"Dimensione y_test:\", y_test.shape)\n",
    "\n",
    "model = SO_GAAL()\n",
    "model.fit(X_train2[:len(X_train2) // 500 * 500])\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "# Valutazione del modello\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fb4017",
   "metadata": {},
   "source": [
    "RF+ICCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b046e91",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Inizializza e addestra il modello\u001b[39;00m\n\u001b[0;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(\u001b[43mX_train\u001b[49m, y_train)\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Previsioni e probabilità di previsione\u001b[39;00m\n\u001b[0;32m      8\u001b[0m y_predicted \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Inizializza e addestra il modello\n",
    "model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Previsioni e probabilità di previsione\n",
    "y_predicted = model.predict(X_test)\n",
    "# Predizione\n",
    "y_test_scores = model.predict_proba(X_test)\n",
    "\n",
    "# Questa è la probabilità che la classificazione sia corretta\n",
    "print(evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ae849e",
   "metadata": {},
   "source": [
    "Linear+L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ece267d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Accuracy': 0.902, 'Precision': 0.969, 'Recall': 0.558, 'F1': 0.708, 'MCC': 0.69, 'AUC_PR': 0.889, 'AUC_ROC': 0.95, 'PREC_N_SCORES': 0.814}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeClassifier\n",
    "\n",
    "# Inizializza e addestra il modello Ridge Classifier (Linear + L2)\n",
    "model = RidgeClassifier(alpha=1.0)  # 'alpha' è il parametro di regolarizzazione L2\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predizione delle etichette di classe\n",
    "y_predicted = model.predict(X_test)\n",
    "\n",
    "# Ottieni le probabilità della classe positiva per AUC (si utilizza decision_function per ottenere punteggi di decisione)\n",
    "y_test_scores = model.decision_function(X_test)\n",
    "\n",
    "# Calcola e stampa le metriche\n",
    "metrics = evaluate_metrics(y_test, y_predicted, y_test_scores)\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21ed48a",
   "metadata": {},
   "source": [
    "Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a69cca-f485-4810-b146-d00d216c01cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IForest(behaviour='old', bootstrap=False, contamination=0.2, max_features=1.0,\n",
      "    max_samples='auto', n_estimators=100, n_jobs=1, random_state=2137,\n",
      "    verbose=0) \n",
      " {'Accuracy': 0.701, 'Precision': 0.297, 'Recall': 0.292, 'F1': 0.295, 'MCC': 0.105, 'AUC_PR': 0.347, 'AUC_ROC': 0.635, 'PREC_N_SCORES': 0.301}\n"
     ]
    }
   ],
   "source": [
    "model = IForest(random_state=SEED, contamination=.2)\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7f31c8",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3608e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN(algorithm='auto', contamination=0.1, leaf_size=30, method='largest',\n",
      "  metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "  radius=1.0) \n",
      " {'Accuracy': 0.849, 'Precision': 0.78, 'Recall': 0.407, 'F1': 0.535, 'MCC': 0.489, 'AUC_PR': 0.658, 'AUC_ROC': 0.852, 'PREC_N_SCORES': 0.593}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.knn import KNN\n",
    "\n",
    "model = KNN()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28682eb3",
   "metadata": {},
   "source": [
    "OCSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77bba73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OCSVM(cache_size=200, coef0=0.0, contamination=0.1, degree=3, gamma='auto',\n",
      "   kernel='rbf', max_iter=-1, nu=0.5, shrinking=True, tol=0.001,\n",
      "   verbose=False) \n",
      " {'Accuracy': 0.837, 'Precision': 0.721, 'Recall': 0.389, 'F1': 0.506, 'MCC': 0.447, 'AUC_PR': 0.659, 'AUC_ROC': 0.788, 'PREC_N_SCORES': 0.655}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.ocsvm import OCSVM\n",
    "\n",
    "model = OCSVM()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f40d2e",
   "metadata": {},
   "source": [
    "ABOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3e3a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ABOD(contamination=0.1, method='fast', n_neighbors=5) \n",
      " {'Accuracy': 0.845, 'Precision': 0.782, 'Recall': 0.381, 'F1': 0.512, 'MCC': 0.472, 'AUC_PR': 0.644, 'AUC_ROC': 0.843, 'PREC_N_SCORES': 0.584}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.abod import ABOD\n",
    "\n",
    "model = ABOD()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03191a05",
   "metadata": {},
   "source": [
    "INNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19321ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INNE(contamination=0.1, max_samples='auto', n_estimators=200,\n",
      "   random_state=None) \n",
      " {'Accuracy': 0.832, 'Precision': 0.694, 'Recall': 0.381, 'F1': 0.491, 'MCC': 0.427, 'AUC_PR': 0.636, 'AUC_ROC': 0.805, 'PREC_N_SCORES': 0.655}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.inne import INNE\n",
    "\n",
    "model = INNE()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b5366a",
   "metadata": {},
   "source": [
    "ALAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671e0764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALAD(activation_hidden_disc='tanh', activation_hidden_gen='tanh',\n",
      "   add_disc_zz_loss=True, add_recon_loss=False, batch_size=32,\n",
      "   contamination=0.1, dec_layers=[5, 10, 25], device=device(type='cpu'),\n",
      "   disc_xx_layers=[25, 10, 5], disc_xz_layers=[25, 10, 5],\n",
      "   disc_zz_layers=[25, 10, 5], dropout_rate=0.2, enc_layers=[25, 10, 5],\n",
      "   epochs=200, lambda_recon_loss=0.1, latent_dim=2,\n",
      "   learning_rate_disc=0.0001, learning_rate_gen=0.0001,\n",
      "   output_activation=None, preprocessing=False,\n",
      "   spectral_normalization=False, verbose=0) \n",
      " {'Accuracy': 0.783, 'Precision': 0.485, 'Recall': 0.283, 'F1': 0.358, 'MCC': 0.25, 'AUC_PR': 0.426, 'AUC_ROC': 0.626, 'PREC_N_SCORES': 0.407}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.alad import ALAD\n",
    "\n",
    "model = ALAD()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff966da",
   "metadata": {},
   "source": [
    "LMDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10b166f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LMDD(contamination=0.1, dis_measure='aad', n_iter=50, random_state=None) \n",
      " {'Accuracy': 0.822, 'Precision': 1.0, 'Recall': 0.168, 'F1': 0.288, 'MCC': 0.37, 'AUC_PR': 0.624, 'AUC_ROC': 0.765, 'PREC_N_SCORES': 0.663}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.lmdd import LMDD\n",
    "\n",
    "model = LMDD()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfca5e0",
   "metadata": {},
   "source": [
    "SOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f82998",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOD(alpha=0.8, contamination=0.1, n_neighbors=20, ref_set=10) \n",
      " {'Accuracy': 0.826, 'Precision': 0.611, 'Recall': 0.513, 'F1': 0.558, 'MCC': 0.453, 'AUC_PR': 0.621, 'AUC_ROC': 0.797, 'PREC_N_SCORES': 0.549}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.sod import SOD\n",
    "\n",
    "model = SOD()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42df12fb",
   "metadata": {},
   "source": [
    "COF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578deac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COF(contamination=0.1, method='fast', n_neighbors=20) \n",
      " {'Accuracy': 0.834, 'Precision': 0.667, 'Recall': 0.442, 'F1': 0.532, 'MCC': 0.449, 'AUC_PR': 0.603, 'AUC_ROC': 0.774, 'PREC_N_SCORES': 0.593}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.cof import COF\n",
    "\n",
    "model = COF()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35130602",
   "metadata": {},
   "source": [
    "LODA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12782922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LODA(contamination=0.1, n_bins=10, n_random_cuts=100) \n",
      " {'Accuracy': 0.83, 'Precision': 0.689, 'Recall': 0.372, 'F1': 0.483, 'MCC': 0.418, 'AUC_PR': 0.549, 'AUC_ROC': 0.692, 'PREC_N_SCORES': 0.522}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.loda import LODA\n",
    "\n",
    "model = LODA()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b9f73c",
   "metadata": {},
   "source": [
    "LUNAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7b6391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LUNAR(contamination=0.1, epsilon=0.1, lr=0.001, model_type='WEIGHT',\n",
      "   n_epochs=200, n_neighbours=5, negative_sampling='MIXED', proportion=1.0,\n",
      "   scaler=MinMaxScaler(), val_size=0.1, verbose=0, wd=0.1) \n",
      " {'Accuracy': 0.815, 'Precision': 0.742, 'Recall': 0.204, 'F1': 0.319, 'MCC': 0.322, 'AUC_PR': 0.539, 'AUC_ROC': 0.796, 'PREC_N_SCORES': 0.451}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.lunar import LUNAR\n",
    "\n",
    "model = LUNAR()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134d43ae",
   "metadata": {},
   "source": [
    "CBLOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d31d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBLOF(alpha=0.9, beta=5, check_estimator=False, clustering_estimator=None,\n",
      "   contamination=0.1, n_clusters=8, n_jobs=None, random_state=None,\n",
      "   use_weights=False) \n",
      " {'Accuracy': 0.802, 'Precision': 0.569, 'Recall': 0.292, 'F1': 0.386, 'MCC': 0.304, 'AUC_PR': 0.45, 'AUC_ROC': 0.574, 'PREC_N_SCORES': 0.372}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.cblof import CBLOF\n",
    "\n",
    "model = CBLOF()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e78d538",
   "metadata": {},
   "source": [
    "DIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664e4d40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DIF(batch_size=1000, contamination=0.1, device=device(type='cpu'),\n",
      "  hidden_activation='tanh', hidden_neurons=[500, 100], max_samples=256,\n",
      "  n_ensemble=50, n_estimators=6, random_state=None, representation_dim=20,\n",
      "  skip_connection=False) \n",
      " {'Accuracy': 0.786, 'Precision': 0.5, 'Recall': 0.009, 'F1': 0.017, 'MCC': 0.043, 'AUC_PR': 0.541, 'AUC_ROC': 0.836, 'PREC_N_SCORES': 0.584}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.dif import DIF\n",
    "\n",
    "model = DIF()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.predict_proba(X_test2)[:,1]\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d12a8b",
   "metadata": {},
   "source": [
    "VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322caf2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 30/30 [00:11<00:00,  2.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAE(batch_norm=False, batch_size=32, beta=1.0, capacity=0.0,\n",
      "  compile_mode='default', contamination=0.1,\n",
      "  decoder_neuron_list=[32, 64, 128], device=device(type='cpu'),\n",
      "  dropout_rate=0.2, encoder_neuron_list=[128, 64, 32], epoch_num=30,\n",
      "  hidden_activation_name='relu', latent_dim=2, lr=0.001,\n",
      "  optimizer_name='adam', optimizer_params={'weight_decay': 1e-05},\n",
      "  output_activation_name='sigmoid', preprocessing=True, random_state=42,\n",
      "  use_compile=False, verbose=1) \n",
      " {'Accuracy': 0.794, 'Precision': 0.532, 'Recall': 0.292, 'F1': 0.377, 'MCC': 0.283, 'AUC_PR': 0.446, 'AUC_ROC': 0.687, 'PREC_N_SCORES': 0.513}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.vae import VAE\n",
    "\n",
    "model = VAE()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2842c38a",
   "metadata": {},
   "source": [
    "GMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b603e181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GMM(contamination=0.1, covariance_type='full', init_params='kmeans',\n",
      "  max_iter=100, means_init=None, n_components=1, n_init=1,\n",
      "  precisions_init=None, random_state=None, reg_covar=1e-06, tol=0.001,\n",
      "  warm_start=False, weights_init=None) \n",
      " {'Accuracy': 0.783, 'Precision': 0.482, 'Recall': 0.239, 'F1': 0.32, 'MCC': 0.225, 'AUC_PR': 0.426, 'AUC_ROC': 0.713, 'PREC_N_SCORES': 0.389}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.gmm import GMM\n",
    "\n",
    "model = GMM()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c8a4e4",
   "metadata": {},
   "source": [
    "DeepSVDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f094a07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 36.17359483242035\n",
      "Epoch 2/100, Loss: 36.19166633486748\n",
      "Epoch 3/100, Loss: 36.2466336786747\n",
      "Epoch 4/100, Loss: 36.13528761267662\n",
      "Epoch 5/100, Loss: 36.165921211242676\n",
      "Epoch 6/100, Loss: 36.13916572928429\n",
      "Epoch 7/100, Loss: 36.189294904470444\n",
      "Epoch 8/100, Loss: 36.17238187789917\n",
      "Epoch 9/100, Loss: 36.2117395401001\n",
      "Epoch 10/100, Loss: 36.185857594013214\n",
      "Epoch 11/100, Loss: 36.13321906328201\n",
      "Epoch 12/100, Loss: 36.1584706902504\n",
      "Epoch 13/100, Loss: 36.17630282044411\n",
      "Epoch 14/100, Loss: 36.17380636930466\n",
      "Epoch 15/100, Loss: 36.25334322452545\n",
      "Epoch 16/100, Loss: 36.1712027490139\n",
      "Epoch 17/100, Loss: 36.12485006451607\n",
      "Epoch 18/100, Loss: 36.4436274766922\n",
      "Epoch 19/100, Loss: 36.22374951839447\n",
      "Epoch 20/100, Loss: 36.2115415930748\n",
      "Epoch 21/100, Loss: 36.16678577661514\n",
      "Epoch 22/100, Loss: 36.20809951424599\n",
      "Epoch 23/100, Loss: 36.228652626276016\n",
      "Epoch 24/100, Loss: 36.154085248708725\n",
      "Epoch 25/100, Loss: 36.138443648815155\n",
      "Epoch 26/100, Loss: 36.5161928832531\n",
      "Epoch 27/100, Loss: 36.136161506175995\n",
      "Epoch 28/100, Loss: 36.181707948446274\n",
      "Epoch 29/100, Loss: 36.141745775938034\n",
      "Epoch 30/100, Loss: 36.1334473490715\n",
      "Epoch 31/100, Loss: 36.193426355719566\n",
      "Epoch 32/100, Loss: 36.15622678399086\n",
      "Epoch 33/100, Loss: 36.199489802122116\n",
      "Epoch 34/100, Loss: 36.11734637618065\n",
      "Epoch 35/100, Loss: 36.160643100738525\n",
      "Epoch 36/100, Loss: 36.1936252117157\n",
      "Epoch 37/100, Loss: 36.16784855723381\n",
      "Epoch 38/100, Loss: 36.19024500250816\n",
      "Epoch 39/100, Loss: 36.2072534263134\n",
      "Epoch 40/100, Loss: 36.19248494505882\n",
      "Epoch 41/100, Loss: 36.18511536717415\n",
      "Epoch 42/100, Loss: 36.156825214624405\n",
      "Epoch 43/100, Loss: 36.18466040492058\n",
      "Epoch 44/100, Loss: 36.14989456534386\n",
      "Epoch 45/100, Loss: 36.18341547250748\n",
      "Epoch 46/100, Loss: 36.13255634903908\n",
      "Epoch 47/100, Loss: 36.44247457385063\n",
      "Epoch 48/100, Loss: 36.20795226097107\n",
      "Epoch 49/100, Loss: 36.16933789849281\n",
      "Epoch 50/100, Loss: 36.155869632959366\n",
      "Epoch 51/100, Loss: 36.17461675405502\n",
      "Epoch 52/100, Loss: 36.14994007349014\n",
      "Epoch 53/100, Loss: 36.176823407411575\n",
      "Epoch 54/100, Loss: 36.16330271959305\n",
      "Epoch 55/100, Loss: 36.18516033887863\n",
      "Epoch 56/100, Loss: 36.17514684796333\n",
      "Epoch 57/100, Loss: 36.11868315935135\n",
      "Epoch 58/100, Loss: 36.16933134198189\n",
      "Epoch 59/100, Loss: 36.193585991859436\n",
      "Epoch 60/100, Loss: 36.30585631728172\n",
      "Epoch 61/100, Loss: 36.124624133110046\n",
      "Epoch 62/100, Loss: 36.41590037941933\n",
      "Epoch 63/100, Loss: 36.16250681877136\n",
      "Epoch 64/100, Loss: 36.13125276565552\n",
      "Epoch 65/100, Loss: 36.290554732084274\n",
      "Epoch 66/100, Loss: 36.19485479593277\n",
      "Epoch 67/100, Loss: 36.192596822977066\n",
      "Epoch 68/100, Loss: 36.19311338663101\n",
      "Epoch 69/100, Loss: 36.15330824255943\n",
      "Epoch 70/100, Loss: 36.15977245569229\n",
      "Epoch 71/100, Loss: 36.17040690779686\n",
      "Epoch 72/100, Loss: 36.20549160242081\n",
      "Epoch 73/100, Loss: 36.14463156461716\n",
      "Epoch 74/100, Loss: 36.23132652044296\n",
      "Epoch 75/100, Loss: 36.14879962801933\n",
      "Epoch 76/100, Loss: 36.246677339076996\n",
      "Epoch 77/100, Loss: 36.14988797903061\n",
      "Epoch 78/100, Loss: 36.13583964109421\n",
      "Epoch 79/100, Loss: 36.220797061920166\n",
      "Epoch 80/100, Loss: 36.12322652339935\n",
      "Epoch 81/100, Loss: 36.13682180643082\n",
      "Epoch 82/100, Loss: 36.136348247528076\n",
      "Epoch 83/100, Loss: 36.21580085158348\n",
      "Epoch 84/100, Loss: 36.154904037714005\n",
      "Epoch 85/100, Loss: 36.17132344841957\n",
      "Epoch 86/100, Loss: 36.27107375860214\n",
      "Epoch 87/100, Loss: 36.149519234895706\n",
      "Epoch 88/100, Loss: 36.13255423307419\n",
      "Epoch 89/100, Loss: 36.17941099405289\n",
      "Epoch 90/100, Loss: 36.24904045462608\n",
      "Epoch 91/100, Loss: 36.19086942076683\n",
      "Epoch 92/100, Loss: 36.16237214207649\n",
      "Epoch 93/100, Loss: 36.12625986337662\n",
      "Epoch 94/100, Loss: 36.16925394535065\n",
      "Epoch 95/100, Loss: 36.25732374191284\n",
      "Epoch 96/100, Loss: 36.15719136595726\n",
      "Epoch 97/100, Loss: 36.21809810400009\n",
      "Epoch 98/100, Loss: 36.19173404574394\n",
      "Epoch 99/100, Loss: 36.41532385349274\n",
      "Epoch 100/100, Loss: 36.2065212726593\n",
      "DeepSVDD(batch_size=32, c=0.0, contamination=0.1, dropout_rate=0.2,\n",
      "     epochs=100, hidden_activation='relu', hidden_neurons=[64, 32],\n",
      "     l2_regularizer=0.1, n_features=18, optimizer='adam',\n",
      "     output_activation='sigmoid', preprocessing=True, random_state=None,\n",
      "     use_ae=False, validation_size=0.1, verbose=1) \n",
      " {'Accuracy': 0.76, 'Precision': 0.394, 'Recall': 0.23, 'F1': 0.291, 'MCC': 0.166, 'AUC_PR': 0.333, 'AUC_ROC': 0.598, 'PREC_N_SCORES': 0.319}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.deep_svdd import DeepSVDD\n",
    "\n",
    "# Determina il numero di feature\n",
    "n_features = X_train2.shape[1]\n",
    "\n",
    "model = DeepSVDD(n_features=n_features)\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba48687c",
   "metadata": {},
   "source": [
    "PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711d99bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA(contamination=0.1, copy=True, iterated_power='auto', n_components=None,\n",
      "  n_selected_components=None, random_state=None, standardization=True,\n",
      "  svd_solver='auto', tol=0.0, weighted=True, whiten=False) \n",
      " {'Accuracy': 0.779, 'Precision': 0.464, 'Recall': 0.23, 'F1': 0.308, 'MCC': 0.21, 'AUC_PR': 0.373, 'AUC_ROC': 0.612, 'PREC_N_SCORES': 0.363}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.pca import PCA\n",
    "\n",
    "model = PCA()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9980f4ce",
   "metadata": {},
   "source": [
    "COPOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac4e4963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COPOD(contamination=0.1, n_jobs=1) \n",
      " {'Accuracy': 0.767, 'Precision': 0.4, 'Recall': 0.177, 'F1': 0.245, 'MCC': 0.147, 'AUC_PR': 0.328, 'AUC_ROC': 0.627, 'PREC_N_SCORES': 0.257}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.copod import COPOD\n",
    "\n",
    "model = COPOD()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b78adc",
   "metadata": {},
   "source": [
    "SOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aea353c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SOS(contamination=0.1, eps=1e-05, metric='euclidean', perplexity=4.5) \n",
      " {'Accuracy': 0.758, 'Precision': 0.364, 'Recall': 0.177, 'F1': 0.238, 'MCC': 0.125, 'AUC_PR': 0.308, 'AUC_ROC': 0.524, 'PREC_N_SCORES': 0.274}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.sos import SOS\n",
    "\n",
    "model = SOS()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d203a345",
   "metadata": {},
   "source": [
    "ECOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31e207f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ECOD(contamination=0.1, n_jobs=1) \n",
      " {'Accuracy': 0.767, 'Precision': 0.396, 'Recall': 0.168, 'F1': 0.236, 'MCC': 0.14, 'AUC_PR': 0.34, 'AUC_ROC': 0.637, 'PREC_N_SCORES': 0.345}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.ecod import ECOD\n",
    "\n",
    "model = ECOD()\n",
    "model.fit(X_train2)\n",
    "\n",
    "y_predicted = model.predict(X_test2)\n",
    "y_predicted_score = model.decision_function(X_test2)\n",
    "\n",
    "print(model, '\\n', evaluate_metrics(y_test, y_predicted, y_predicted_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140e0d61",
   "metadata": {},
   "source": [
    "## XGBOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cf9dbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [21:32:28] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBOD(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "   colsample_bytree=1,\n",
      "   estimator_list=[KNN(algorithm='auto', contamination=0.1, leaf_size=30, method='largest',\n",
      "  metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
      "  radius=1.0), LOF(algorithm='auto', contamination=0.1, leaf_size=30, metric='minkowski',\n",
      "  metric_params=None, n_jobs=1, n_neighbors=1, no...ax_features=1.0,\n",
      "    max_samples='auto', n_estimators=200, n_jobs=1, random_state=0,\n",
      "    verbose=0)],\n",
      "   gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "   min_child_weight=1, n_estimators=100, n_jobs=1, nthread=None,\n",
      "   objective='binary:logistic', random_state=0, reg_alpha=0, reg_lambda=1,\n",
      "   scale_pos_weight=1, silent=True,\n",
      "   standardization_flag_list=[True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False],\n",
      "   subsample=1) {'Accuracy': 0.968, 'Precision': 0.953, 'Recall': 0.894, 'F1': 0.922, 'MCC': 0.903, 'AUC_PR': 0.969, 'AUC_ROC': 0.99, 'PREC_N_SCORES': 0.912}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.xgbod import XGBOD\n",
    "\n",
    "# Inizializza e addestra XGBOD\n",
    "model = XGBOD()\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Prevedi gli outlier nel dataset di test\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_predicted_score = model.decision_function(X_test_scaled)\n",
    "# Eseguiamo la valutazione delle metriche\n",
    "metrics = evaluate_metrics(y_test, y_pred, y_predicted_score)\n",
    "\n",
    "# Stampa i risultati\n",
    "print(model, metrics)\n",
    "\n",
    "#n_estimators=50,\n",
    "#max_depth=3,\n",
    "#learning_rate=0.1,\n",
    "#random_state=SEED"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3705d61e",
   "metadata": {},
   "source": [
    "#### Con metiche di Memoria e Tempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82d45b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from memory_profiler import memory_usage\n",
    "from pyod.models.xgbod import XGBOD\n",
    "\n",
    "# Inizializza e addestra XGBOD\n",
    "model = XGBOD(n_estimators=50, max_depth=3, learning_rate=0.1, random_state=SEED)\n",
    "\n",
    "def train_model():\n",
    "    start_time = time.time()\n",
    "    mem_usage = memory_usage((model.fit, (X_train_scaled, y_train)))\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"\\n Tempo di addestramento: {training_time} secondi\")\n",
    "    print(f\"Uso della memoria durante l'addestramento: {max(mem_usage)} MiB\")\n",
    "    return training_time, mem_usage\n",
    "\n",
    "def inference_model():\n",
    "    start_time = time.time()\n",
    "    mem_usage_inference = memory_usage((model.predict, (X_test_scaled,)))\n",
    "    inference_time = time.time() - start_time\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    print(f\"\\n Tempo di inferenza: {inference_time} secondi\")\n",
    "    print(f\"Uso della memoria durante l'inferenza: {max(mem_usage_inference)} MiB\")\n",
    "    return y_pred, inference_time, mem_usage_inference\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5595e73b",
   "metadata": {},
   "source": [
    "### XGBOD più modelli unsupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b09d455",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:09:07] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBOD(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "   colsample_bytree=1,\n",
      "   estimator_list=[KNN(algorithm='auto', contamination=0.1, leaf_size=30, method='largest',\n",
      "  metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "  radius=1.0), LOF(algorithm='auto', contamination=0.1, leaf_size=30, metric='minkowski',\n",
      "  metric_params=None, n_jobs=1, n_neighbors=20, n...3, gamma='auto',\n",
      "   kernel='rbf', max_iter=-1, nu=0.5, shrinking=True, tol=0.001,\n",
      "   verbose=False)],\n",
      "   gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "   min_child_weight=1, n_estimators=100, n_jobs=1, nthread=None,\n",
      "   objective='binary:logistic', random_state=0, reg_alpha=0, reg_lambda=1,\n",
      "   scale_pos_weight=1, silent=True,\n",
      "   standardization_flag_list=[True, True, True, True], subsample=1) {'Accuracy': 0.968, 'Precision': 0.944, 'Recall': 0.903, 'F1': 0.923, 'MCC': 0.903, 'AUC_PR': 0.974, 'AUC_ROC': 0.991, 'PREC_N_SCORES': 0.92}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.xgbod import XGBOD\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.iforest import IForest\n",
    "from pyod.models.lof import LOF\n",
    "from pyod.models.abod import ABOD\n",
    "from pyod.models.ocsvm import OCSVM\n",
    "\n",
    "# Definizione dei modelli unsupervised\n",
    "unsupervised_models = [ KNN(),\n",
    "                       LOF(),\n",
    "                       ABOD(),\n",
    "                        OCSVM()\n",
    "                    ]\n",
    "# Inizializza e addestra XGBOD\n",
    "model = XGBOD(estimator_list=unsupervised_models)\n",
    "\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Prevedi gli outlier nel dataset di test\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_predicted_score = model.decision_function(X_test_scaled)\n",
    "# Eseguiamo la valutazione delle metriche\n",
    "metrics = evaluate_metrics(y_test, y_pred, y_predicted_score)\n",
    "\n",
    "# Stampa i risultati\n",
    "print(model, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c6d685",
   "metadata": {},
   "source": [
    "#### Con Metriche di Tempo e Memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00d2966",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:13:57] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Tempo di addestramento: 2.3419463634490967 secondi\n",
      "Uso della memoria durante l'addestramento: 815.8125 MiB\n",
      "\n",
      " Tempo di inferenza: 1.605494499206543 secondi\n",
      "Uso della memoria durante l'inferenza: 815.79296875 MiB\n",
      "XGBOD(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "   colsample_bytree=1,\n",
      "   estimator_list=[KNN(algorithm='auto', contamination=0.1, leaf_size=30, method='largest',\n",
      "  metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "  radius=1.0), LOF(algorithm='auto', contamination=0.1, leaf_size=30, metric='minkowski',\n",
      "  metric_params=None, n_jobs=1, n_neighbors=20, n...3, gamma='auto',\n",
      "   kernel='rbf', max_iter=-1, nu=0.5, shrinking=True, tol=0.001,\n",
      "   verbose=False)],\n",
      "   gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
      "   min_child_weight=1, n_estimators=100, n_jobs=1, nthread=None,\n",
      "   objective='binary:logistic', random_state=0, reg_alpha=0, reg_lambda=1,\n",
      "   scale_pos_weight=1, silent=True,\n",
      "   standardization_flag_list=[True, True, True, True], subsample=1) {'Accuracy': 0.968, 'Precision': 0.944, 'Recall': 0.903, 'F1': 0.923, 'MCC': 0.903, 'AUC_PR': 0.974, 'AUC_ROC': 0.991, 'PREC_N_SCORES': 0.92}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from memory_profiler import memory_usage\n",
    "from pyod.models.xgbod import XGBOD\n",
    "\n",
    "# Definizione dei modelli unsupervised\n",
    "unsupervised_models = [ KNN(),\n",
    "                       LOF(),\n",
    "                       ABOD(),\n",
    "                        OCSVM()\n",
    "                    ]\n",
    "# Inizializza e addestra XGBOD\n",
    "model = XGBOD(estimator_list=unsupervised_models)\n",
    "\n",
    "def train_model():\n",
    "    start_time = time.time()\n",
    "    mem_usage = memory_usage((model.fit, (X_train_scaled, y_train)))\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"\\n Tempo di addestramento: {training_time} secondi\")\n",
    "    print(f\"Uso della memoria durante l'addestramento: {max(mem_usage)} MiB\")\n",
    "    return training_time, mem_usage\n",
    "\n",
    "def inference_model():\n",
    "    start_time = time.time()\n",
    "    mem_usage_inference = memory_usage((model.predict, (X_test_scaled,)))\n",
    "    inference_time = time.time() - start_time\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    print(f\"\\n Tempo di inferenza: {inference_time} secondi\")\n",
    "    print(f\"Uso della memoria durante l'inferenza: {max(mem_usage_inference)} MiB\")\n",
    "    return y_pred, inference_time, mem_usage_inference\n",
    "\n",
    "# Addestramento del modello e monitoraggio delle metriche di efficientamento\n",
    "training_time, mem_usage = train_model()\n",
    "\n",
    "# Inferenza del modello e monitoraggio delle metriche di efficientamento\n",
    "y_pred, inference_time, mem_usage_inference = inference_model()\n",
    "\n",
    "# Calcola i punteggi di decisione\n",
    "y_predicted_score = model.decision_function(X_test_scaled)\n",
    "\n",
    "# Eseguiamo la valutazione delle metriche con le nuove metriche di efficientamento\n",
    "metrics = evaluate_metrics(y_test, y_pred, y_predicted_score)\n",
    "\n",
    "# Stampa i risultati\n",
    "print(model, metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a13a44",
   "metadata": {},
   "source": [
    "### XGBOD più modelli unsupervised e Parametri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7899936c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:09:26] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "{'Accuracy': 0.97, 'Precision': 0.945, 'Recall': 0.912, 'F1': 0.928, 'MCC': 0.909, 'AUC_PR': 0.973, 'AUC_ROC': 0.992, 'PREC_N_SCORES': 0.92}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.xgbod import XGBOD\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.lof import LOF\n",
    "from pyod.models.abod import ABOD\n",
    "from pyod.models.ocsvm import OCSVM\n",
    "\n",
    "# Definizione dei modelli unsupervised\n",
    "unsupervised_models = [ KNN(),\n",
    "                       LOF(),\n",
    "                       ABOD(),\n",
    "                        OCSVM()\n",
    "                    ]\n",
    "\n",
    "# Inizializza e addestra XGBOD\n",
    "model = XGBOD(estimator_list=unsupervised_models,\n",
    "              n_estimators=100,\n",
    "              max_depth=3,\n",
    "              learning_rate=0.2,\n",
    "              n_jobs=-1,\n",
    "              random_state=SEED\n",
    "            )\n",
    "\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Prevedi gli outlier nel dataset di test\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_predicted_score = model.decision_function(X_test_scaled)\n",
    "\n",
    "# Eseguiamo la valutazione delle metriche\n",
    "metrics = evaluate_metrics(y_test, y_pred, y_predicted_score)\n",
    "print(\"\")\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796179ff",
   "metadata": {},
   "source": [
    "#### Con Metriche di Tempo e Memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "790f2d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:14:04] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Tempo di addestramento: 2.611022472381592 secondi\n",
      "Uso della memoria durante l'addestramento: 816.11328125 MiB\n",
      "\n",
      " Tempo di inferenza: 1.9620587825775146 secondi\n",
      "Uso della memoria durante l'inferenza: 816.078125 MiB\n",
      "XGBOD(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "   colsample_bytree=1,\n",
      "   estimator_list=[KNN(algorithm='auto', contamination=0.1, leaf_size=30, method='largest',\n",
      "  metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "  radius=1.0), LOF(algorithm='auto', contamination=0.1, leaf_size=30, metric='minkowski',\n",
      "  metric_params=None, n_jobs=1, n_neighbors=20, n...3, gamma='auto',\n",
      "   kernel='rbf', max_iter=-1, nu=0.5, shrinking=True, tol=0.001,\n",
      "   verbose=False)],\n",
      "   gamma=0, learning_rate=0.2, max_delta_step=0, max_depth=3,\n",
      "   min_child_weight=1, n_estimators=100, n_jobs=1, nthread=None,\n",
      "   objective='binary:logistic', random_state=2137, reg_alpha=0,\n",
      "   reg_lambda=1, scale_pos_weight=1, silent=True,\n",
      "   standardization_flag_list=[True, True, True, True], subsample=1) {'Accuracy': 0.97, 'Precision': 0.945, 'Recall': 0.912, 'F1': 0.928, 'MCC': 0.909, 'AUC_PR': 0.973, 'AUC_ROC': 0.992, 'PREC_N_SCORES': 0.92}\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from memory_profiler import memory_usage\n",
    "from pyod.models.xgbod import XGBOD\n",
    "\n",
    "# Definizione dei modelli unsupervised\n",
    "unsupervised_models = [ KNN(),\n",
    "                       LOF(),\n",
    "                       ABOD(),\n",
    "                        OCSVM()\n",
    "                    ]\n",
    "# Inizializza e addestra XGBOD\n",
    "model = XGBOD(estimator_list=unsupervised_models, n_estimators=100, max_depth=3, learning_rate=0.2, random_state=SEED)\n",
    "\n",
    "def train_model():\n",
    "    start_time = time.time()\n",
    "    mem_usage = memory_usage((model.fit, (X_train_scaled, y_train)))\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"\\n Tempo di addestramento: {training_time} secondi\")\n",
    "    print(f\"Uso della memoria durante l'addestramento: {max(mem_usage)} MiB\")\n",
    "    return training_time, mem_usage\n",
    "\n",
    "def inference_model():\n",
    "    start_time = time.time()\n",
    "    mem_usage_inference = memory_usage((model.predict, (X_test_scaled,)))\n",
    "    inference_time = time.time() - start_time\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    print(f\"\\n Tempo di inferenza: {inference_time} secondi\")\n",
    "    print(f\"Uso della memoria durante l'inferenza: {max(mem_usage_inference)} MiB\")\n",
    "    return y_pred, inference_time, mem_usage_inference\n",
    "\n",
    "# Addestramento del modello e monitoraggio delle metriche di efficientamento\n",
    "training_time, mem_usage = train_model()\n",
    "\n",
    "# Inferenza del modello e monitoraggio delle metriche di efficientamento\n",
    "y_pred, inference_time, mem_usage_inference = inference_model()\n",
    "\n",
    "# Calcola i punteggi di decisione\n",
    "y_predicted_score = model.decision_function(X_test_scaled)\n",
    "\n",
    "# Eseguiamo la valutazione delle metriche con le nuove metriche di efficientamento\n",
    "metrics = evaluate_metrics(y_test, y_pred, y_predicted_score)\n",
    "\n",
    "# Stampa i risultati\n",
    "print(model, metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39459cc5",
   "metadata": {},
   "source": [
    "### Early Stopping\n",
    "Termina l'esecuzione anticipatamente se per un numero prestabilito di round non migliorano più i parametri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0157b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:09:43] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:09:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:09:45] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:09:46] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:09:46] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:09:47] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:09:48] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:09:49] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:09:50] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:09:50] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:09:51] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:09:52] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [22:09:53] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at iteration 12\n",
      "\n",
      "{'Accuracy': 0.97, 'Precision': 0.971, 'Recall': 0.885, 'F1': 0.926, 'MCC': 0.909, 'AUC_PR': 0.969, 'AUC_ROC': 0.99, 'PREC_N_SCORES': 0.912}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pyod.models.xgbod import XGBOD\n",
    "from pyod.models.knn import KNN\n",
    "from pyod.models.iforest import IForest\n",
    "from pyod.models.lof import LOF\n",
    "from pyod.models.abod import ABOD\n",
    "from pyod.models.ocsvm import OCSVM\n",
    "\n",
    "# Definizione dei modelli unsupervised\n",
    "unsupervised_models = [ KNN(),\n",
    "                       LOF(),\n",
    "                       ABOD(),\n",
    "                        OCSVM()\n",
    "                    ]\n",
    "\n",
    "# Divisione del dataset di allenamento per avere un set di validazione\n",
    "X_train_sub, X_val, y_train_sub, y_val = train_test_split(X_train_scaled, y_train, test_size=0.2, random_state=SEED)\n",
    "\n",
    "# Inizializzazione del modello\n",
    "model = XGBOD(estimator_list=unsupervised_models, n_estimators=50, max_depth=3, learning_rate=0.2, n_jobs=-1, random_state=SEED)\n",
    "\n",
    "best_score = -np.inf\n",
    "patience = 10       # Numero di volte che il modello cercherà di migliorarsi\n",
    "patience_counter = 0\n",
    "n_iterations = 100      # Numero massimo di cicli del'allenamento\n",
    "\n",
    "for i in range(n_iterations):  # Numero massimo di iterazioni\n",
    "    model.fit(X_train_sub, y_train_sub)\n",
    "    \n",
    "    # Predizione sul set di validazione\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    val_score = accuracy_score(y_val, y_val_pred)\n",
    "    \n",
    "    # Controllo early stopping\n",
    "    if val_score > best_score:\n",
    "        best_score = val_score\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at iteration {i}\")\n",
    "            break\n",
    "    model.n_estimators += 1  # Incrementa il numero di stimatori per la prossima iterazione\n",
    "\n",
    "# Predizione sul set di test\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_predicted_score = model.decision_function(X_test_scaled)\n",
    "\n",
    "# Eseguiamo la valutazione delle metriche\n",
    "metrics = evaluate_metrics(y_test, y_pred, y_predicted_score)\n",
    "print(\"\")\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bcc0b0",
   "metadata": {},
   "source": [
    "### XGBOD con ricerca iperparametri con \"grid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c125a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:320: UserWarning: The total space of parameters 8 is smaller than n_iter=10. Running 8 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1103: UserWarning: One or more of the test scores are non-finite: [nan nan nan nan nan nan nan nan]\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:16:55] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\pyod\\models\\base.py:423: UserWarning: y should not be presented in unsupervised learning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found: {'n_estimators': 50, 'max_depth': 3, 'learning_rate': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [15:17:00] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-0c55ff5f71b100e98-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"silent\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBOD(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "   colsample_bytree=1,\n",
      "   estimator_list=[KNN(algorithm='auto', contamination=0.1, leaf_size=30, method='largest',\n",
      "  metric='minkowski', metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
      "  radius=1.0), LOF(algorithm='auto', contamination=0.1, leaf_size=30, metric='minkowski',\n",
      "  metric_params=None, n_jobs=1, n_neighbors=1, no...ax_features=1.0,\n",
      "    max_samples='auto', n_estimators=200, n_jobs=1, random_state=0,\n",
      "    verbose=0)],\n",
      "   gamma=0, learning_rate=0.01, max_delta_step=0, max_depth=3,\n",
      "   min_child_weight=1, n_estimators=50, n_jobs=1, nthread=None,\n",
      "   objective='binary:logistic', random_state=0, reg_alpha=0, reg_lambda=1,\n",
      "   scale_pos_weight=1, silent=True,\n",
      "   standardization_flag_list=[True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False, True, True, True, True, True, True, True, True, True, True, True, False, False, False, False, False, False, False],\n",
      "   subsample=1) {'Accuracy': 0.947, 'Precision': 0.989, 'Recall': 0.761, 'F1': 0.86, 'MCC': 0.839, 'AUC_PR': 0.898, 'AUC_ROC': 0.945, 'PREC_N_SCORES': 0.967}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from pyod.models.xgbod import XGBOD\n",
    "import numpy as np\n",
    "\n",
    "# Definizione della griglia di parametri\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [3, 5],\n",
    "    'learning_rate': [0.01, 0.1]\n",
    "}\n",
    "\n",
    "# Inizializza il modello\n",
    "model = XGBOD()\n",
    "\n",
    "# Randomized search con meno iterazioni e parallelizzazione\n",
    "random_search = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=10, cv=3, scoring='roc_auc', random_state=42, n_jobs=-1)\n",
    "random_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Migliori parametri trovati\n",
    "best_params = random_search.best_params_\n",
    "print(f\"Best parameters found: {best_params}\")\n",
    "\n",
    "# Riaddestramento del modello con i migliori parametri\n",
    "model = XGBOD(**best_params)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Prevedi gli outlier nel dataset di test\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "y_predicted_score = model.decision_function(X_test_scaled)\n",
    "\n",
    "# Eseguiamo la valutazione delle metriche\n",
    "metrics = evaluate_metrics(y_test, y_pred, y_predicted_score)\n",
    "\n",
    "# Stampa i risultati\n",
    "print(model, metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b35867",
   "metadata": {},
   "source": [
    "### FCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ad2c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.8006 - loss: 0.4877 - val_accuracy: 0.8885 - val_loss: 0.2546\n",
      "Epoch 2/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9154 - loss: 0.2390 - val_accuracy: 0.9244 - val_loss: 0.1969\n",
      "Epoch 3/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9334 - loss: 0.1862 - val_accuracy: 0.9168 - val_loss: 0.1949\n",
      "Epoch 4/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9408 - loss: 0.1831 - val_accuracy: 0.9452 - val_loss: 0.1793\n",
      "Epoch 5/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9474 - loss: 0.1629 - val_accuracy: 0.9471 - val_loss: 0.1570\n",
      "Epoch 6/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9424 - loss: 0.1595 - val_accuracy: 0.9546 - val_loss: 0.1572\n",
      "Epoch 7/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.9514 - loss: 0.1251 - val_accuracy: 0.9509 - val_loss: 0.1471\n",
      "Epoch 8/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9561 - loss: 0.1225 - val_accuracy: 0.9546 - val_loss: 0.1322\n",
      "Epoch 9/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9428 - loss: 0.1436 - val_accuracy: 0.9565 - val_loss: 0.1249\n",
      "Epoch 10/10\n",
      "\u001b[1m50/50\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.9612 - loss: 0.1117 - val_accuracy: 0.9622 - val_loss: 0.1135\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "\u001b[1m17/17\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "<Sequential name=sequential, built=True> {'Accuracy': 0.962, 'Precision': 0.927, 'Recall': 0.894, 'F1': 0.91, 'MCC': 0.886, 'AUC_PR': 0.966, 'AUC_ROC': 0.984, 'PREC_N_SCORES': 0.903}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Definisci il modello FCNN\n",
    "model = Sequential([\n",
    "    Conv1D(64, 3, activation='relu', input_shape=(X_train_scaled.shape[1], 1)),\n",
    "    MaxPooling1D(2),\n",
    "    Conv1D(128, 3, activation='relu'),\n",
    "    MaxPooling1D(2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')  # Poiché si tratta di una classificazione binaria\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Addestra il modello\n",
    "model.fit(X_train_scaled, y_train, epochs=10, batch_size=32, validation_data=(X_test_scaled, y_test))\n",
    "\n",
    "# Prevedi gli outlier nel dataset di test\n",
    "y_pred = (model.predict(X_test_scaled) > 0.5).astype(\"int32\")\n",
    "y_predicted_score = model.predict(X_test_scaled)\n",
    "\n",
    "metrics = evaluate_metrics(y_test, y_pred, y_predicted_score)\n",
    "\n",
    "# Stampa i risultati\n",
    "print(model, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f797308e",
   "metadata": {},
   "source": [
    "# Rocket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d67943",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Il numero di campioni non corrisponde!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m features \u001b[38;5;241m=\u001b[39m rocket\u001b[38;5;241m.\u001b[39mtransform(X_train_scaled)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Verifica che il numero di campioni di features e y_test sia lo stesso\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m features\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m y_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIl numero di campioni non corrisponde!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# 3. Rilevamento delle anomalie\u001b[39;00m\n\u001b[0;32m     15\u001b[0m model \u001b[38;5;241m=\u001b[39m XGBOD(contamination\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)  \u001b[38;5;66;03m# Modello non supervisionato\u001b[39;00m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Il numero di campioni non corrisponde!"
     ]
    }
   ],
   "source": [
    "from sktime.transformations.panel.rocket import Rocket\n",
    "from pyod.models.xgbod import XGBOD\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 2. Applica ROCKET\n",
    "rocket = Rocket(num_kernels=10000)\n",
    "rocket.fit(X_train_scaled, y_train)\n",
    "features = rocket.transform(X_train_scaled)\n",
    "# Verifica che il numero di campioni di features e y_test sia lo stesso\n",
    "assert features.shape[0] == y_test.shape[0], \"Il numero di campioni non corrisponde!\"\n",
    "\n",
    "\n",
    "# 3. Rilevamento delle anomalie\n",
    "model = XGBOD(contamination=0.01, random_state=42)  # Modello non supervisionato\n",
    "anomaly_scores = model.fit_predict(features, y_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e1007c0",
   "metadata": {},
   "source": [
    "## Rocket Normale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4273bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomalie rilevate nel training set: [1 0 0 ... 0 0 0]\n",
      "Anomalie rilevate nel test set: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 1 0 0 0 0 0 1 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione sul test set:\n",
      " {'Accuracy': 0.832, 'Precision': 0.962, 'Recall': 0.221, 'F1': 0.36, 'MCC': 0.415, 'AUC_PR': 0.726, 'AUC_ROC': 0.772, 'PREC_N_SCORES': 0.646}\n"
     ]
    }
   ],
   "source": [
    "from rocket_functions import generate_kernels, apply_kernels\n",
    "\n",
    "def detect_anomalies_with_threshold(scores, threshold):\n",
    "    return (scores > threshold).astype(int)\n",
    "\n",
    "# Genera kernel convoluzionali casuali\n",
    "input_length = X_train.shape[1]\n",
    "num_kernels = 10000\n",
    "kernels = generate_kernels(input_length, num_kernels)\n",
    "\n",
    "# Applica i kernel alle serie temporali\n",
    "features_train = apply_kernels(X_train2, kernels)\n",
    "features_test = apply_kernels(X_test2, kernels)\n",
    "\n",
    "# Sintesi delle caratteristiche per esempio\n",
    "anomaly_scores_train = np.mean(features_train, axis=1)  # Media\n",
    "anomaly_scores_test = np.mean(features_test, axis=1)  # Media\n",
    "\n",
    "# Rilevamento delle anomalie\n",
    "threshold = np.percentile(anomaly_scores_train , 95)\n",
    "anomaly_labels_train = detect_anomalies_with_threshold(anomaly_scores_train , threshold)\n",
    "anomaly_labels_test = detect_anomalies_with_threshold(anomaly_scores_test , threshold)\n",
    "\n",
    "# Visualizzazione dei risultati\n",
    "print(\"Anomalie rilevate nel training set:\", anomaly_labels_train)\n",
    "print(\"Anomalie rilevate nel test set:\", anomaly_labels_test)\n",
    "\n",
    "# Eseguiamo la valutazione delle metriche\n",
    "metrics = evaluate_metrics(y_test, anomaly_labels_test, y_proba=anomaly_scores_test)\n",
    "print(\"Metriche di valutazione sul test set:\\n\", metrics)\n",
    "# {'Accuracy': 0.832, 'Precision': 0.962, 'Recall': 0.221, 'F1': 0.36, 'MCC': 0.415, 'AUC_PR': 0.726, 'AUC_ROC': 0.772, 'PREC_N_SCORES': 0.646}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38ed2dc",
   "metadata": {},
   "source": [
    "## Rockad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e972c69",
   "metadata": {},
   "source": [
    "### Prima Prova singolo canale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de83dbb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\AppData\\Local\\Temp\\ipykernel_42688\\2480508613.py:71: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  y_test_final.append(y_testS[i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test:  [0 0 1 1 1 1 0 1 1 1 1 0 1 0 0]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.6, 'Precision': 0.6, 'Recall': 1.0, 'F1': 0.75, 'MCC': 0.0}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from RockadFunction import ROCKAD, NearestNeighborOCC\n",
    "\n",
    "features = [\"channel\", \"segment\", \"value\", \"anomaly\"]\n",
    "RANDOM_STATE = 42\n",
    "STEP = 250\n",
    "\n",
    "# Lista per memorizzare i segmenti di training\n",
    "X_train_final = []\n",
    "\n",
    "# Leggi il file CSV\n",
    "dfSegment = pd.read_csv(\"data/segments.csv\", index_col=\"timestamp\")\n",
    "channelFix = \"CADC0872\"\n",
    "\n",
    "# Itera su ogni segmento unico per il canale corrente\n",
    "for segment in dfSegment[dfSegment[\"channel\"] == channelFix][\"segment\"].unique():\n",
    "    mask = (dfSegment[\"train\"] == 1) & (dfSegment[\"channel\"] == channelFix) & (dfSegment[\"segment\"] == segment)\n",
    "\n",
    "    # Filtra i dati in base alla maschera\n",
    "    X_trainS = dfSegment.loc[mask, \"value\"] #.reset_index(drop=True).values  # Estrarre solo 'value'\n",
    "    # print(X_trainS.shape)\n",
    "    # Suddividi in sottoliste di STEP elementi\n",
    "    for i in range(0, len(X_trainS) - STEP + 1, STEP):\n",
    "        sublist = X_trainS[i:i + STEP]  # Estrarre una finestra di STEP elementi\n",
    "        X_train_final.append(sublist)\n",
    "\n",
    "# Converti la lista in un numpy array\n",
    "X_train = np.array(X_train_final)\n",
    "# print(X_train_final)\n",
    "\n",
    "# Reshape per ottenere la shape desiderata\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "# print(X_train_final.shape)\n",
    "# print(\"X_train_final:\", X_train_final)\n",
    "\n",
    "# y_train = dfSegment[dfSegment[\"train\"] == 1][\"anomaly\"].values[:X_train_final.shape[0]]\n",
    "\n",
    "# # Senza non torna perchè richiede che tutti abbiano una shape>0\n",
    "# X_train_filtered, y_train_filtered = zip(*[\n",
    "#     (x, y) for x, y in zip(X_train_final, y_train) if not np.any(x == 0)\n",
    "# ])\n",
    "# X_train_filtered = np.array(X_train_filtered)\n",
    "\n",
    "\n",
    "\n",
    "# X_normal_train = X_train_final[y_train == 0]\n",
    "#  print(\"Shape X_normal_train:\", X_normal_train.shape)\n",
    "\n",
    "# Inizializza e addestra il modello ROCKAD\n",
    "rockad = ROCKAD(n_estimators=10, n_kernels=10, random_state=RANDOM_STATE)\n",
    "rockad.fit(X_train)\n",
    "print(\"End Train\")\n",
    "\n",
    "# Predisposizione del test set\n",
    "test_data = dfSegment[dfSegment[\"train\"] == 0]\n",
    "\n",
    "# Predisposizione del test set\n",
    "X_test_final = []\n",
    "y_test_final = []\n",
    "\n",
    "for segment in test_data[test_data[\"channel\"] == channelFix][\"segment\"].unique():\n",
    "\n",
    "    mask = (test_data[\"channel\"] == channelFix) & (test_data[\"segment\"] == segment)\n",
    "    X_testS = test_data.loc[mask, \"value\"]#.reset_index(drop=True).values\n",
    "    y_testS = test_data.loc[mask, \"anomaly\"]#.reset_index(drop=True).values\n",
    "    \n",
    "    for i in range(0, len(X_testS) - STEP + 1, STEP):\n",
    "        X_test_final.append(X_testS[i:i + STEP])\n",
    "        y_test_final.append(y_testS[i])\n",
    "\n",
    "X_test = np.array(X_test_final)\n",
    "\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "# print(\"X_test: \",X_test)\n",
    "# X_test = np.array(X_test_final).reshape(len(X_test_final), STEP, 1)\n",
    "\n",
    "y_test = np.array(y_test_final)\n",
    "print(\"y_test: \",y_test)\n",
    "\n",
    "# Predict anomaly scores\n",
    "scores = rockad.predict_proba(X_test)\n",
    "# print(\"Score:\", scores)\n",
    "\n",
    "# Initialize and fit NearestNeigbor One Class Classifier\n",
    "decision_func = NearestNeighborOCC().fit(scores)\n",
    "predictions = decision_func.predict(scores)\n",
    "print(predictions)\n",
    "\n",
    "metrics = evaluate_metrics(y_test, predictions)\n",
    "print(\"Metriche di valutazione:\\n\", metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b7c494",
   "metadata": {},
   "source": [
    "### 2° Prova un canale -> miglioramento predizione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8a5ee4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: [[[-2.14870e-05]\n",
      "  [-2.05077e-05]\n",
      "  [-2.10633e-05]\n",
      "  ...\n",
      "  [-2.02867e-05]\n",
      "  [-2.14005e-05]\n",
      "  [-2.14015e-05]]\n",
      "\n",
      " [[-2.71730e-05]\n",
      "  [-2.67612e-05]\n",
      "  [-2.66298e-05]\n",
      "  ...\n",
      "  [ 0.00000e+00]\n",
      "  [ 0.00000e+00]\n",
      "  [ 0.00000e+00]]\n",
      "\n",
      " [[-3.75574e-05]\n",
      "  [-3.76894e-05]\n",
      "  [-3.83888e-05]\n",
      "  ...\n",
      "  [ 2.48018e-05]\n",
      "  [ 2.45232e-05]\n",
      "  [ 2.52121e-05]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-8.04761e-06]\n",
      "  [-8.60125e-06]\n",
      "  [-8.04855e-06]\n",
      "  ...\n",
      "  [ 1.35164e-05]\n",
      "  [ 1.28179e-05]\n",
      "  [ 1.33761e-05]]\n",
      "\n",
      " [[-1.61842e-05]\n",
      "  [-1.61858e-05]\n",
      "  [-1.54880e-05]\n",
      "  ...\n",
      "  [-7.26255e-06]\n",
      "  [-7.39873e-06]\n",
      "  [-7.96388e-06]]\n",
      "\n",
      " [[-4.61679e-05]\n",
      "  [-4.60268e-05]\n",
      "  [-4.64461e-05]\n",
      "  ...\n",
      "  [-3.19180e-05]\n",
      "  [-3.21974e-05]\n",
      "  [-3.23320e-05]]]\n",
      "End Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\AppData\\Local\\Temp\\ipykernel_20524\\238595853.py:71: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  y_test_final.append(y_testS[i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test:  [0 0 1 1 1 1 0 1 1 1 1 0 1 0 0]\n",
      "score_test:  (15,)\n",
      "[3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07]\n",
      "RISULTATI:  [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.6, 'Precision': 0.6, 'Recall': 1.0, 'F1': 0.75, 'MCC': 0.0, 'AUC_PR': 0.6, 'AUC_ROC': 0.5, 'PREC_N_SCORES': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from RockadFunction import ROCKAD, NearestNeighborOCC\n",
    "\n",
    "features = [\"channel\", \"segment\", \"value\", \"anomaly\"]\n",
    "RANDOM_STATE = 42\n",
    "STEP = 250\n",
    "\n",
    "# Lista per memorizzare i segmenti di training\n",
    "X_train_final = []\n",
    "\n",
    "# Leggi il file CSV\n",
    "dfSegment = pd.read_csv(\"data/segments.csv\", index_col=\"timestamp\")\n",
    "channelFix = \"CADC0872\"\n",
    "\n",
    "# Itera su ogni segmento unico per il canale corrente\n",
    "for segment in dfSegment[dfSegment[\"channel\"] == channelFix][\"segment\"].unique():\n",
    "    mask = (dfSegment[\"train\"] == 1) & (dfSegment[\"channel\"] == channelFix) & (dfSegment[\"segment\"] == segment)\n",
    "\n",
    "    # Filtra i dati in base alla maschera\n",
    "    X_trainS = dfSegment.loc[mask, \"value\"] #.reset_index(drop=True).values  # Estrarre solo 'value'\n",
    "    # print(X_trainS.shape)\n",
    "    # Suddividi in sottoliste di STEP elementi\n",
    "    for i in range(0, len(X_trainS) - STEP + 1, STEP):\n",
    "        sublist = X_trainS[i:i + STEP]  # Estrarre una finestra di STEP elementi\n",
    "        X_train_final.append(sublist)\n",
    "\n",
    "# Converti la lista in un numpy array\n",
    "X_train = np.array(X_train_final)\n",
    "# print(X_train_final)\n",
    "\n",
    "# Reshape per ottenere la shape desiderata\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "# print(X_train_final.shape)\n",
    "# print(\"X_train:\", X_train)\n",
    "\n",
    "# y_train = dfSegment[dfSegment[\"train\"] == 1][\"anomaly\"].values[:X_train_final.shape[0]]\n",
    "\n",
    "# # Senza non torna perchè richiede che tutti abbiano una shape>0\n",
    "# X_train_filtered, y_train_filtered = zip(*[\n",
    "#     (x, y) for x, y in zip(X_train_final, y_train) if not np.any(x == 0)\n",
    "# ])\n",
    "# X_train_filtered = np.array(X_train_filtered)\n",
    "\n",
    "\n",
    "\n",
    "# X_normal_train = X_train_final[y_train == 0]\n",
    "#  print(\"Shape X_normal_train:\", X_normal_train.shape)\n",
    "\n",
    "# Inizializza e addestra il modello ROCKAD\n",
    "rockad = ROCKAD(n_estimators=100, n_kernels=100, random_state=RANDOM_STATE)\n",
    "rockad.fit(X_train)\n",
    "print(\"End Train\")\n",
    "\n",
    "# Predisposizione del test set\n",
    "test_data = dfSegment[dfSegment[\"train\"] == 0]\n",
    "\n",
    "# Predisposizione del test set\n",
    "X_test_final = []\n",
    "y_test_final = []\n",
    "\n",
    "for segment in test_data[test_data[\"channel\"] == channelFix][\"segment\"].unique():\n",
    "\n",
    "    mask = (test_data[\"channel\"] == channelFix) & (test_data[\"segment\"] == segment)\n",
    "    X_testS = test_data.loc[mask, \"value\"]#.reset_index(drop=True).values\n",
    "    y_testS = test_data.loc[mask, \"anomaly\"]#.reset_index(drop=True).values\n",
    "    \n",
    "    for i in range(0, len(X_testS) - STEP + 1, STEP):\n",
    "        X_test_final.append(X_testS[i:i + STEP])\n",
    "        y_test_final.append(y_testS[i])\n",
    "\n",
    "\n",
    "X_test = np.array(X_test_final)\n",
    "\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "# print(\"X_test: \",X_test)\n",
    "# X_test = np.array(X_test_final).reshape(len(X_test_final), STEP, 1)\n",
    "\n",
    "y_test = np.array(y_test_final)\n",
    "print(\"y_test: \",y_test)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Predict anomaly scores\n",
    "score_train = rockad.predict_proba(X_train)\n",
    "# print(\"Score:\", scores)\n",
    "\n",
    "# Initialize and fit NearestNeigbor One Class Classifier\n",
    "\n",
    "decision_func = NearestNeighborOCC().fit(score_train)\n",
    "score_test = rockad.predict_proba(X_test)\n",
    "print(\"score_test: \", score_test.shape)\n",
    "print(score_test)\n",
    "\n",
    "result = decision_func.predict(score_test)\n",
    "# result2 = knn.predict(score_test)\n",
    "print(\"RISULTATI: \", result)\n",
    "#print(\"RISULTATI: \", result2)\n",
    "\n",
    "metrics = evaluate_metrics(y_test, result, score_test)\n",
    "print(\"Metriche di valutazione:\\n\", metrics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d749e55c",
   "metadata": {},
   "source": [
    "#### Prova con modifiche valori ingresso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfeab4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\AppData\\Local\\Temp\\ipykernel_20524\\661137002.py:40: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  y_test_final.append(y_testS[i])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input X contains infinity or a value too large for dtype('float32').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 55\u001b[0m\n\u001b[0;32m     50\u001b[0m y_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(y_test_final)\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# print(\"y_test: \",y_test)\u001b[39;00m\n\u001b[0;32m     52\u001b[0m \n\u001b[0;32m     53\u001b[0m \n\u001b[0;32m     54\u001b[0m \u001b[38;5;66;03m# Initialize and fit NearestNeigbor One Class Classifier\u001b[39;00m\n\u001b[1;32m---> 55\u001b[0m score_test \u001b[38;5;241m=\u001b[39m \u001b[43mrockad\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m decision_func \u001b[38;5;241m=\u001b[39m NearestNeighborOCC()\u001b[38;5;241m.\u001b[39mfit(score_test)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# print(score_test)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\RockadFunction.py:307\u001b[0m, in \u001b[0;36mROCKAD.predict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    302\u001b[0m     Xtp_scaled \u001b[38;5;241m=\u001b[39m Xt\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, bagger \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlist_baggers):\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;66;03m# Get scores from each estimator\u001b[39;00m\n\u001b[1;32m--> 307\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[43mbagger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXtp_scaled\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[0;32m    309\u001b[0m     y_scores[:, idx] \u001b[38;5;241m=\u001b[39m scores\n\u001b[0;32m    311\u001b[0m \u001b[38;5;66;03m# Average the scores to get the final score for each time series\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\RockadFunction.py:171\u001b[0m, in \u001b[0;36mNN.predict_proba\u001b[1;34m(self, X, y)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_proba\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 171\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkneighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    172\u001b[0m     scores \u001b[38;5;241m=\u001b[39m scores[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m scores\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\sklearn\\neighbors\\_base.py:825\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[1;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[0;32m    823\u001b[0m         X \u001b[38;5;241m=\u001b[39m _check_precomputed(X)\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m         X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    827\u001b[0m n_samples_fit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_samples_fit_\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_neighbors \u001b[38;5;241m>\u001b[39m n_samples_fit:\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\sklearn\\base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[0;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:1064\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1058\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1059\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1060\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[0;32m   1061\u001b[0m     )\n\u001b[0;32m   1063\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[1;32m-> 1064\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1065\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1066\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1067\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1068\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1069\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[0;32m   1072\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[0;32m   1073\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:123\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:172\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[0;32m    156\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[0;32m    158\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    159\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    170\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    171\u001b[0m     )\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from RockadFunction import ROCKAD, NearestNeighborOCC\n",
    "\n",
    "features = [\"channel\", \"segment\", \"value\", \"anomaly\"]\n",
    "RANDOM_STATE = 42\n",
    "STEP = 250\n",
    "\n",
    "#  print(X_train)\n",
    "\n",
    "# Converti la lista in un numpy array\n",
    "# X_train = np.array(X_train_final)\n",
    "# print(X_train_final)\n",
    "\n",
    "# Reshape per ottenere la shape desiderata\n",
    "# X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "# print(X_train)\n",
    "\n",
    "# Inizializza e addestra il modello ROCKAD\n",
    "rockad = ROCKAD(n_estimators=100, n_kernels=100, random_state=RANDOM_STATE, power_transform=False)\n",
    "rockad.fit(X_train)\n",
    "print(\"End Train\")\n",
    "\n",
    "# Predisposizione del test set\n",
    "test_data = dfSegment[dfSegment[\"train\"] == 0]\n",
    "\n",
    "# Predisposizione del test set\n",
    "X_test_final = []\n",
    "y_test_final = []\n",
    "\n",
    "for segment in test_data[test_data[\"channel\"] == channelFix][\"segment\"].unique():\n",
    "\n",
    "    mask = (test_data[\"channel\"] == channelFix) & (test_data[\"segment\"] == segment)\n",
    "    X_testS = test_data.loc[mask, \"value\"]#.reset_index(drop=True).values\n",
    "    y_testS = test_data.loc[mask, \"anomaly\"]#.reset_index(drop=True).values\n",
    "    \n",
    "    for i in range(0, len(X_testS) - STEP + 1, STEP):\n",
    "        X_test_final.append(X_testS[i:i + STEP])\n",
    "        y_test_final.append(y_testS[i])\n",
    "\n",
    "\n",
    "# X_test = np.array(X_test_final)\n",
    "X_test = pd.DataFrame(np.concatenate(X_test_final, axis=0))\n",
    "\n",
    "# X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "# print(\"X_test: \",X_test)\n",
    "# X_test = np.array(X_test_final).reshape(len(X_test_final), STEP, 1)\n",
    "\n",
    "y_test = np.array(y_test_final)\n",
    "# print(\"y_test: \",y_test)\n",
    "\n",
    "\n",
    "# Initialize and fit NearestNeigbor One Class Classifier\n",
    "score_test = rockad.predict_proba(X_test)\n",
    "decision_func = NearestNeighborOCC().fit(score_test)\n",
    "# print(score_test)\n",
    "\n",
    "\n",
    "result = decision_func.predict(score_test)\n",
    "# result2 = knn.predict(score_test)\n",
    "print(\"RISULTATI: \", result)\n",
    "#print(\"RISULTATI: \", result2)\n",
    "\n",
    "metrics = evaluate_metrics(y_test, result, score_test)\n",
    "print(\"Metriche di valutazione:\\n\", metrics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be9f115",
   "metadata": {},
   "source": [
    "### Più Canali e Miglioramento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9d81df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\AppData\\Local\\Temp\\ipykernel_22804\\3373671386.py:62: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  y_test_final.append(y_testS[i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test:  (130,)\n",
      "score_test:  (130,)\n",
      "[3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07]\n",
      "RISULTATI:  (130,)\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.477, 'Precision': 0.477, 'Recall': 1.0, 'F1': 0.646, 'MCC': 0.0, 'AUC_PR': 0.477, 'AUC_ROC': 0.5, 'PREC_N_SCORES': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyod.models.knn import KNN\n",
    "\n",
    "from RockadFunction import ROCKAD, NearestNeighborOCC\n",
    "\n",
    "features = [\"channel\", \"segment\", \"value\", \"anomaly\"]\n",
    "RANDOM_STATE = 42\n",
    "STEP = 250\n",
    "\n",
    "# Lista per memorizzare i segmenti di training\n",
    "X_train_final = []\n",
    "\n",
    "# Leggi il file CSV\n",
    "dfSegment = pd.read_csv(\"data/segments.csv\", index_col=\"timestamp\")\n",
    "channelFix = \"CADC0872\"\n",
    "\n",
    "for channel in dfSegment[\"channel\"].unique():\n",
    "    # Itera su ogni segmento unico per il canale corrente\n",
    "    for segment in dfSegment[dfSegment[\"channel\"] == channel][\"segment\"].unique():\n",
    "        mask = (dfSegment[\"train\"] == 1) & (dfSegment[\"channel\"] == channel) & (dfSegment[\"segment\"] == segment)\n",
    "\n",
    "        # Filtra i dati in base alla maschera\n",
    "        X_trainS = dfSegment.loc[mask, \"value\"] #.reset_index(drop=True).values  # Estrarre solo 'value'\n",
    "        # print(X_trainS.shape)\n",
    "        # Suddividi in sottoliste di STEP elementi\n",
    "        for i in range(0, len(X_trainS) - STEP + 1, STEP):\n",
    "            sublist = X_trainS[i:i + STEP]  # Estrarre una finestra di STEP elementi\n",
    "            X_train_final.append(sublist)\n",
    "\n",
    "# Converti la lista in un numpy array\n",
    "X_train = np.array(X_train_final)\n",
    "# print(X_train_final)\n",
    "\n",
    "# Reshape per ottenere la shape desiderata\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "# print(X_train_final.shape)\n",
    "# print(\"X_train_final:\", X_train_final)\n",
    "\n",
    "# Inizializza e addestra il modello ROCKAD\n",
    "rockad = ROCKAD(random_state=RANDOM_STATE)\n",
    "rockad.fit(X_train)\n",
    "print(\"End Train\")\n",
    "\n",
    "# Predisposizione del test set\n",
    "test_data = dfSegment[dfSegment[\"train\"] == 0]\n",
    "\n",
    "# Predisposizione del test set\n",
    "X_test_final = []\n",
    "y_test_final = []\n",
    "\n",
    "for channel in dfSegment[\"channel\"].unique():\n",
    "    for segment in test_data[test_data[\"channel\"] == channel][\"segment\"].unique():\n",
    "\n",
    "        mask = (test_data[\"channel\"] == channel) & (test_data[\"segment\"] == segment)\n",
    "        X_testS = test_data.loc[mask, \"value\"]#.reset_index(drop=True).values\n",
    "        y_testS = test_data.loc[mask, \"anomaly\"]#.reset_index(drop=True).values\n",
    "        \n",
    "        for i in range(0, len(X_testS) - STEP + 1, STEP):\n",
    "            X_test_final.append(X_testS[i:i + STEP])\n",
    "            y_test_final.append(y_testS[i])\n",
    "\n",
    "\n",
    "X_test = np.array(X_test_final)\n",
    "\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "# print(\"X_test: \",X_test)\n",
    "# X_test = np.array(X_test_final).reshape(len(X_test_final), STEP, 1)\n",
    "\n",
    "y_test = np.array(y_test_final)\n",
    "print(\"y_test: \",y_test.shape)\n",
    "\n",
    "# Predict anomaly scores\n",
    "score_train = rockad.predict_proba(X_train)\n",
    "# print(\"Score:\", scores)\n",
    "\n",
    "# Initialize and fit NearestNeigbor One Class Classifier\n",
    "\n",
    "decision_func = NearestNeighborOCC().fit(score_train)\n",
    "score_test = rockad.predict_proba(X_test)\n",
    "print(\"score_test: \", score_test.shape)\n",
    "print(score_test)\n",
    "\n",
    "result = decision_func.predict(score_test)\n",
    "# result2 = knn.predict(score_test)\n",
    "print(\"RISULTATI: \", result.shape)\n",
    "#print(\"RISULTATI: \", result2)\n",
    "\n",
    "metrics = evaluate_metrics(y_test, result, score_test)\n",
    "print(\"Metriche di valutazione:\\n\", metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c294f86e",
   "metadata": {},
   "source": [
    "#### Prova IForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06e5103a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\franc\\AppData\\Local\\Temp\\ipykernel_16452\\486147723.py:62: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  y_test_final.append(y_testS[i])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_test:  [0 0 1 1 1 1 0 1 1 1 1 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 1 1 1 1 1 0 0 1 1 0 1 1 1 0 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 0 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 1 0 0 1 0 1 0 1 0 0 0 0 0 0\n",
      " 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0]\n",
      "score_test:  (130,)\n",
      "[3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07 3.68062169e-07 3.68062169e-07\n",
      " 3.68062169e-07 3.68062169e-07]\n",
      "RISULTATI:  [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.523, 'Precision': 0.0, 'Recall': 0.0, 'F1': 0.0, 'MCC': 0.0, 'AUC_PR': 0.477, 'AUC_ROC': 0.5, 'PREC_N_SCORES': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from pyod.models.knn import KNN\n",
    "\n",
    "from RockadFunction import ROCKAD, NearestNeighborOCC\n",
    "\n",
    "features = [\"channel\", \"segment\", \"value\", \"anomaly\"]\n",
    "RANDOM_STATE = 42\n",
    "STEP = 250\n",
    "\n",
    "# Lista per memorizzare i segmenti di training\n",
    "X_train_final = []\n",
    "\n",
    "# Leggi il file CSV\n",
    "dfSegment = pd.read_csv(\"data/segments.csv\", index_col=\"timestamp\")\n",
    "channelFix = \"CADC0872\"\n",
    "\n",
    "for channel in dfSegment[\"channel\"].unique():\n",
    "    # Itera su ogni segmento unico per il canale corrente\n",
    "    for segment in dfSegment[dfSegment[\"channel\"] == channel][\"segment\"].unique():\n",
    "        mask = (dfSegment[\"train\"] == 1) & (dfSegment[\"channel\"] == channel) & (dfSegment[\"segment\"] == segment)\n",
    "\n",
    "        # Filtra i dati in base alla maschera\n",
    "        X_trainS = dfSegment.loc[mask, \"value\"] #.reset_index(drop=True).values  # Estrarre solo 'value'\n",
    "        # print(X_trainS.shape)\n",
    "        # Suddividi in sottoliste di STEP elementi\n",
    "        for i in range(0, len(X_trainS) - STEP + 1, STEP):\n",
    "            sublist = X_trainS[i:i + STEP]  # Estrarre una finestra di STEP elementi\n",
    "            X_train_final.append(sublist)\n",
    "\n",
    "# Converti la lista in un numpy array\n",
    "X_train = np.array(X_train_final)\n",
    "# print(X_train_final)\n",
    "\n",
    "# Reshape per ottenere la shape desiderata\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "# print(X_train_final.shape)\n",
    "# print(\"X_train_final:\", X_train_final)\n",
    "\n",
    "# Inizializza e addestra il modello ROCKAD\n",
    "rockad = ROCKAD(random_state=RANDOM_STATE)\n",
    "rockad.fit(X_train)\n",
    "print(\"End Train\")\n",
    "\n",
    "# Predisposizione del test set\n",
    "test_data = dfSegment[dfSegment[\"train\"] == 0]\n",
    "\n",
    "# Predisposizione del test set\n",
    "X_test_final = []\n",
    "y_test_final = []\n",
    "\n",
    "for channel in dfSegment[\"channel\"].unique():\n",
    "    for segment in test_data[test_data[\"channel\"] == channel][\"segment\"].unique():\n",
    "\n",
    "        mask = (test_data[\"channel\"] == channel) & (test_data[\"segment\"] == segment)\n",
    "        X_testS = test_data.loc[mask, \"value\"]#.reset_index(drop=True).values\n",
    "        y_testS = test_data.loc[mask, \"anomaly\"]#.reset_index(drop=True).values\n",
    "        \n",
    "        for i in range(0, len(X_testS) - STEP + 1, STEP):\n",
    "            X_test_final.append(X_testS[i:i + STEP])\n",
    "            y_test_final.append(y_testS[i])\n",
    "\n",
    "\n",
    "X_test = np.array(X_test_final)\n",
    "\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "# print(\"X_test: \",X_test)\n",
    "# X_test = np.array(X_test_final).reshape(len(X_test_final), STEP, 1)\n",
    "\n",
    "y_test = np.array(y_test_final)\n",
    "print(\"y_test: \",y_test)\n",
    "\n",
    "# Predict anomaly scores\n",
    "score_train = rockad.predict_proba(X_train)\n",
    "# print(\"Score:\", scores)\n",
    "\n",
    "# Initialize and fit NearestNeigbor One Class Classifier\n",
    "\n",
    "model = IForest(random_state=SEED, contamination=.2)\n",
    "score_train = score_train.reshape(-1, 1)  # Diventa (n_samples, 1)\n",
    "model.fit(score_train)\n",
    "\n",
    "score_test = rockad.predict_proba(X_test)\n",
    "\n",
    "print(\"score_test: \", score_test.shape)\n",
    "print(score_test)\n",
    "\n",
    "score_test = score_test.reshape(-1, 1)  # Assicura che sia 2D\n",
    "result = model.predict(score_test)\n",
    "decision_scores_iforest = model.decision_function(score_test)\n",
    "# result2 = knn.predict(score_test)\n",
    "print(\"RISULTATI: \", result)\n",
    "#print(\"RISULTATI: \", result2)\n",
    "\n",
    "metrics = evaluate_metrics(y_test, result, decision_scores_iforest)\n",
    "print(\"Metriche di valutazione:\\n\", metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fbeb98",
   "metadata": {},
   "source": [
    "### Non funzionante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4582cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "End Train\n",
      "(15, 20)\n",
      "    0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16  17  \\\n",
      "0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "1    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "2    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "3    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "4    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "5    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "6    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "7    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "8    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "9    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "10   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "11   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "12   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "13   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "14   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "\n",
      "    18  19  \n",
      "0    0   0  \n",
      "1    0   0  \n",
      "2    0   0  \n",
      "3    0   0  \n",
      "4    0   0  \n",
      "5    0   0  \n",
      "6    0   0  \n",
      "7    0   0  \n",
      "8    0   0  \n",
      "9    0   0  \n",
      "10   0   0  \n",
      "11   0   0  \n",
      "12   0   0  \n",
      "13   0   0  \n",
      "14   0   0  \n",
      "(15, 20)\n",
      "    0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16  17  \\\n",
      "0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "1    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "2    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "3    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "4    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "5    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "6    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "7    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "8    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "9    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "10   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "11   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "12   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "13   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "14   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "\n",
      "    18  19  \n",
      "0    0   0  \n",
      "1    0   0  \n",
      "2    0   0  \n",
      "3    0   0  \n",
      "4    0   0  \n",
      "5    0   0  \n",
      "6    0   0  \n",
      "7    0   0  \n",
      "8    0   0  \n",
      "9    0   0  \n",
      "10   0   0  \n",
      "11   0   0  \n",
      "12   0   0  \n",
      "13   0   0  \n",
      "14   0   0  \n",
      "(15, 20)\n",
      "    0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16  17  \\\n",
      "0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "1    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "2    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "3    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "4    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "5    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "6    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "7    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "8    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "9    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "10   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "11   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "12   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "13   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "14   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "\n",
      "    18  19  \n",
      "0    0   0  \n",
      "1    0   0  \n",
      "2    0   0  \n",
      "3    0   0  \n",
      "4    0   0  \n",
      "5    0   0  \n",
      "6    0   0  \n",
      "7    0   0  \n",
      "8    0   0  \n",
      "9    0   0  \n",
      "10   0   0  \n",
      "11   0   0  \n",
      "12   0   0  \n",
      "13   0   0  \n",
      "14   0   0  \n",
      "(15, 20)\n",
      "    0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16  17  \\\n",
      "0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "1    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "2    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "3    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "4    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "5    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "6    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "7    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "8    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "9    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "10   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "11   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "12   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "13   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "14   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "\n",
      "    18  19  \n",
      "0    0   0  \n",
      "1    0   0  \n",
      "2    0   0  \n",
      "3    0   0  \n",
      "4    0   0  \n",
      "5    0   0  \n",
      "6    0   0  \n",
      "7    0   0  \n",
      "8    0   0  \n",
      "9    0   0  \n",
      "10   0   0  \n",
      "11   0   0  \n",
      "12   0   0  \n",
      "13   0   0  \n",
      "14   0   0  \n",
      "(15, 20)\n",
      "    0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16  17  \\\n",
      "0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "1    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "2    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "3    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "4    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "5    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "6    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "7    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "8    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "9    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "10   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "11   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "12   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "13   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "14   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "\n",
      "    18  19  \n",
      "0    0   0  \n",
      "1    0   0  \n",
      "2    0   0  \n",
      "3    0   0  \n",
      "4    0   0  \n",
      "5    0   0  \n",
      "6    0   0  \n",
      "7    0   0  \n",
      "8    0   0  \n",
      "9    0   0  \n",
      "10   0   0  \n",
      "11   0   0  \n",
      "12   0   0  \n",
      "13   0   0  \n",
      "14   0   0  \n",
      "(15, 20)\n",
      "    0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16  17  \\\n",
      "0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "1    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "2    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "3    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "4    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "5    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "6    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "7    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "8    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "9    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "10   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "11   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "12   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "13   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "14   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "\n",
      "    18  19  \n",
      "0    0   0  \n",
      "1    0   0  \n",
      "2    0   0  \n",
      "3    0   0  \n",
      "4    0   0  \n",
      "5    0   0  \n",
      "6    0   0  \n",
      "7    0   0  \n",
      "8    0   0  \n",
      "9    0   0  \n",
      "10   0   0  \n",
      "11   0   0  \n",
      "12   0   0  \n",
      "13   0   0  \n",
      "14   0   0  \n",
      "(15, 20)\n",
      "    0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16  17  \\\n",
      "0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "1    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "2    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "3    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "4    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "5    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "6    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "7    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "8    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "9    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "10   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "11   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "12   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "13   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "14   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "\n",
      "    18  19  \n",
      "0    0   0  \n",
      "1    0   0  \n",
      "2    0   0  \n",
      "3    0   0  \n",
      "4    0   0  \n",
      "5    0   0  \n",
      "6    0   0  \n",
      "7    0   0  \n",
      "8    0   0  \n",
      "9    0   0  \n",
      "10   0   0  \n",
      "11   0   0  \n",
      "12   0   0  \n",
      "13   0   0  \n",
      "14   0   0  \n",
      "(15, 20)\n",
      "    0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16  17  \\\n",
      "0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "1    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "2    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "3    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "4    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "5    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "6    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "7    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "8    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "9    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "10   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "11   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "12   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "13   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "14   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "\n",
      "    18  19  \n",
      "0    0   0  \n",
      "1    0   0  \n",
      "2    0   0  \n",
      "3    0   0  \n",
      "4    0   0  \n",
      "5    0   0  \n",
      "6    0   0  \n",
      "7    0   0  \n",
      "8    0   0  \n",
      "9    0   0  \n",
      "10   0   0  \n",
      "11   0   0  \n",
      "12   0   0  \n",
      "13   0   0  \n",
      "14   0   0  \n",
      "(15, 20)\n",
      "    0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16  17  \\\n",
      "0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "1    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "2    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "3    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "4    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "5    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "6    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "7    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "8    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "9    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "10   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "11   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "12   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "13   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "14   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "\n",
      "    18  19  \n",
      "0    0   0  \n",
      "1    0   0  \n",
      "2    0   0  \n",
      "3    0   0  \n",
      "4    0   0  \n",
      "5    0   0  \n",
      "6    0   0  \n",
      "7    0   0  \n",
      "8    0   0  \n",
      "9    0   0  \n",
      "10   0   0  \n",
      "11   0   0  \n",
      "12   0   0  \n",
      "13   0   0  \n",
      "14   0   0  \n",
      "(15, 20)\n",
      "    0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16  17  \\\n",
      "0    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "1    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "2    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "3    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "4    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "5    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "6    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "7    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "8    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "9    0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "10   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "11   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "12   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "13   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "14   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   \n",
      "\n",
      "    18  19  \n",
      "0    0   0  \n",
      "1    0   0  \n",
      "2    0   0  \n",
      "3    0   0  \n",
      "4    0   0  \n",
      "5    0   0  \n",
      "6    0   0  \n",
      "7    0   0  \n",
      "8    0   0  \n",
      "9    0   0  \n",
      "10   0   0  \n",
      "11   0   0  \n",
      "12   0   0  \n",
      "13   0   0  \n",
      "14   0   0  \n",
      "Score: [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Incompatible dimension for X and Y matrices: X.shape[1] == 15 while Y.shape[1] == 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 45\u001b[0m\n\u001b[0;32m     43\u001b[0m decision_func \u001b[38;5;241m=\u001b[39m NearestNeighborOCC()\u001b[38;5;241m.\u001b[39mfit(scores)\n\u001b[0;32m     44\u001b[0m predictions \u001b[38;5;241m=\u001b[39m decision_func\u001b[38;5;241m.\u001b[39mpredict(scores)\n\u001b[1;32m---> 45\u001b[0m proba \u001b[38;5;241m=\u001b[39m \u001b[43mdecision_func\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Debugging shapes\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape y_test:\u001b[39m\u001b[38;5;124m\"\u001b[39m, y_test\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\RockadFunction.py:77\u001b[0m, in \u001b[0;36mNearestNeighborOCC.predict_score\u001b[1;34m(self, anomaly_score)\u001b[0m\n\u001b[0;32m     74\u001b[0m nearest_nearest_neighbor_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdist(nearest_neighbor_score_arr, _scores_train))\n\u001b[0;32m     75\u001b[0m nearest_nearest_neighbor_score \u001b[38;5;241m=\u001b[39m _scores_train[nearest_nearest_neighbor_idx][\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 77\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindicator_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     78\u001b[0m \u001b[43m    \u001b[49m\u001b[43manomaly_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnearest_neighbor_score\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnearest_nearest_neighbor_score\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prediction\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\RockadFunction.py:90\u001b[0m, in \u001b[0;36mNearestNeighborOCC.indicator_function\u001b[1;34m(self, z_score, nearest_score, nearest_of_nearest_score)\u001b[0m\n\u001b[0;32m     87\u001b[0m nearest_score_arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(nearest_score)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     88\u001b[0m nearest_of_nearest_score_arr \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(nearest_of_nearest_score)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 90\u001b[0m numerator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz_score_arr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnearest_score_arr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m denominator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdist(nearest_score_arr, nearest_of_nearest_score_arr)\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# error handling for corner cases\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:344\u001b[0m, in \u001b[0;36meuclidean_distances\u001b[1;34m(X, Y, Y_norm_squared, squared, X_norm_squared)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;129m@validate_params\u001b[39m(\n\u001b[0;32m    259\u001b[0m     {\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray-like\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    269\u001b[0m     X, Y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m, Y_norm_squared\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, squared\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, X_norm_squared\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    270\u001b[0m ):\n\u001b[0;32m    271\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;124;03m    Compute the distance matrix between each pair from a vector array X and Y.\u001b[39;00m\n\u001b[0;32m    273\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    342\u001b[0m \u001b[38;5;124;03m           [1.41421356]])\u001b[39;00m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 344\u001b[0m     X, Y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_pairwise_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    346\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m X_norm_squared \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    347\u001b[0m         X_norm_squared \u001b[38;5;241m=\u001b[39m check_array(X_norm_squared, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:214\u001b[0m, in \u001b[0;36mcheck_pairwise_arrays\u001b[1;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, ensure_2d, copy)\u001b[0m\n\u001b[0;32m    206\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    207\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecomputed metric requires shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    208\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(n_queries, n_indexed). Got (\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m indexed.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    210\u001b[0m         )\n\u001b[0;32m    211\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m ensure_2d \u001b[38;5;129;01mand\u001b[39;00m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m!=\u001b[39m Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;66;03m# Only check the number of features if 2d arrays are enforced. Otherwise,\u001b[39;00m\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;66;03m# validation is left to the user for custom metrics.\u001b[39;00m\n\u001b[1;32m--> 214\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    215\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncompatible dimension for X and Y matrices: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    216\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX.shape[1] == \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m while Y.shape[1] == \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], Y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    217\u001b[0m     )\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X, Y\n",
      "\u001b[1;31mValueError\u001b[0m: Incompatible dimension for X and Y matrices: X.shape[1] == 15 while Y.shape[1] == 1"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from RockadFunction import ROCKAD, NearestNeighborOCC\n",
    "\n",
    "features = [\"channel\", \"segment\", \"value\", \"anomaly\"]\n",
    "RANDOM_STATE = 42\n",
    "STEP = 250\n",
    "\n",
    "# Lista per memorizzare i segmenti di training\n",
    "X_train_final = []\n",
    "\n",
    "# Leggi il file CSV\n",
    "dfSegment = pd.read_csv(\"data/segments.csv\", index_col=\"timestamp\")\n",
    "for channel in dfSegment[\"channel\"].unique():\n",
    "    # Itera su ogni segmento unico per il canale corrente\n",
    "    for segment in dfSegment[dfSegment[\"channel\"] == channel][\"segment\"].unique():\n",
    "        mask = (dfSegment[\"train\"] == 1) & (dfSegment[\"channel\"] == channel) & (dfSegment[\"segment\"] == segment)\n",
    "\n",
    "        # Filtra i dati in base alla maschera\n",
    "        X_trainS = dfSegment.loc[mask, \"value\"].reset_index(drop=True).values  # Estrarre solo 'value'\n",
    "        \n",
    "        # Suddividi in sottoliste di STEP elementi\n",
    "        for i in range(0, len(X_trainS) - STEP + 1, STEP):\n",
    "            sublist = X_trainS[i:i + STEP]  # Estrarre una finestra di STEP elementi\n",
    "            X_train_final.append(sublist)\n",
    "\n",
    "# Converti la lista in un numpy array\n",
    "X_train_final = np.array(X_train_final)\n",
    "\n",
    "# Reshape per ottenere la shape desiderata\n",
    "X_train_final = X_train_final.reshape(X_train_final.shape[0], X_train_final.shape[1], 1)\n",
    "# print(\"X_train_final:\", X_train_final)\n",
    "\n",
    "y_train = dfSegment[dfSegment[\"train\"] == 1][\"anomaly\"].values[:X_train_final.shape[0]]\n",
    "\n",
    "# Senza non torna perchè richiede che tutti abbiano una shape>0\n",
    "X_train_filtered, y_train_filtered = zip(*[\n",
    "    (x, y) for x, y in zip(X_train_final, y_train) if not np.any(x == 0)\n",
    "])\n",
    "X_train_filtered = np.array(X_train_filtered)\n",
    "y_train_filtered = np.array(y_train_filtered)\n",
    "\n",
    "\n",
    "X_normal_train = X_train_final[y_train == 0]\n",
    "#  print(\"Shape X_normal_train:\", X_normal_train.shape)\n",
    "\n",
    "# Inizializza e addestra il modello ROCKAD\n",
    "rockad = ROCKAD(n_estimators=10, n_kernels=10, random_state=RANDOM_STATE, power_transform=False)\n",
    "rockad.fit(X_train_filtered)\n",
    "print(\"End Train\")\n",
    "\n",
    "# Predisposizione del test set\n",
    "test_data = dfSegment[dfSegment[\"train\"] == 0]\n",
    "\n",
    "# Predisposizione del test set\n",
    "X_test_final = []\n",
    "y_test_final = []\n",
    "\n",
    "for channel in dfSegment[\"channel\"].unique():\n",
    "    for segment in test_data[test_data[\"channel\"] == channel][\"segment\"].unique():\n",
    "        mask = (test_data[\"channel\"] == channel) & (test_data[\"segment\"] == segment)\n",
    "        X_testS = test_data.loc[mask, \"value\"].reset_index(drop=True).values\n",
    "        y_testS = test_data.loc[mask, \"anomaly\"].reset_index(drop=True).values\n",
    "        \n",
    "        for i in range(0, len(X_testS) - STEP + 1, STEP):\n",
    "            X_test_final.append(X_testS[i:i + STEP])\n",
    "            y_test_final.append(y_testS[i])\n",
    "\n",
    "X_test = np.array(X_test_final).reshape(len(X_test_final), STEP, 1)\n",
    "y_test = np.array(y_test_final)\n",
    "\n",
    "\n",
    "# Predict anomaly scores\n",
    "scores = rockad.predict_proba(X_test)\n",
    "print(\"Score:\", scores)\n",
    "\n",
    "# Initialize and fit NearestNeigbor One Class Classifier\n",
    "decision_func = NearestNeighborOCC().fit(scores)\n",
    "predictions = decision_func.predict(scores)\n",
    "proba = decision_func.predict_score(X_test)\n",
    "\n",
    "metrics = evaluate_metrics(y_test, predictions, proba)\n",
    "print(\"Metriche di valutazione:\\n\", metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2409f2d8",
   "metadata": {},
   "source": [
    "## Rilevamento di anomalie SUPERVISED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ffb220",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "No matching definition for argument type(s) pyobject, Tuple(array(float64, 1d, C), array(int32, 1d, C), array(float64, 1d, C), array(int32, 1d, C), array(int32, 1d, C))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Applica i kernel alle serie temporali\u001b[39;00m\n\u001b[0;32m     13\u001b[0m features_train \u001b[38;5;241m=\u001b[39m apply_kernels(X_train2, kernels)\n\u001b[1;32m---> 14\u001b[0m features_test \u001b[38;5;241m=\u001b[39m \u001b[43mapply_kernels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Addestramento del modello supervisionato\u001b[39;00m\n\u001b[0;32m     17\u001b[0m model \u001b[38;5;241m=\u001b[39m XGBOD(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.1\u001b[39m, random_state\u001b[38;5;241m=\u001b[39mSEED)\n",
      "File \u001b[1;32mc:\\Users\\franc\\OneDrive - University of Pisa\\Università\\Tesi\\OPS-SAT-AD\\Paper_OPS-SAT_Python\\.venv\\Lib\\site-packages\\numba\\core\\dispatcher.py:658\u001b[0m, in \u001b[0;36m_DispatcherBase._explain_matching_error\u001b[1;34m(self, *args, **kws)\u001b[0m\n\u001b[0;32m    655\u001b[0m args \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtypeof_pyval(a) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[0;32m    656\u001b[0m msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo matching definition for argument type(s) \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    657\u001b[0m        \u001b[38;5;241m%\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m, args)))\n\u001b[1;32m--> 658\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n",
      "\u001b[1;31mTypeError\u001b[0m: No matching definition for argument type(s) pyobject, Tuple(array(float64, 1d, C), array(int32, 1d, C), array(float64, 1d, C), array(int32, 1d, C), array(int32, 1d, C))"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pyod.models.xgbod import XGBOD\n",
    "\n",
    "from rocket_functions import generate_kernels, apply_kernels\n",
    "\n",
    "# Genera kernel convoluzionali casuali\n",
    "input_length = X_train.shape[1]\n",
    "num_kernels = 1000 # Valore standard\n",
    "kernels = generate_kernels(input_length, num_kernels)\n",
    "\n",
    "# Applica i kernel alle serie temporali\n",
    "features_train = apply_kernels(X_train2, kernels)\n",
    "features_test = apply_kernels(X_test, kernels)\n",
    "\n",
    "# Addestramento del modello supervisionato\n",
    "model = XGBOD(n_estimators=100, max_depth=3, learning_rate=0.1, random_state=SEED)\n",
    "model.fit(features_train, y_train)\n",
    "\n",
    "# Predizione delle anomalie nei dati di test\n",
    "y_pred = model.predict(features_test)\n",
    "y_proba = model.predict_proba(features_test)\n",
    "\n",
    "# Visualizzazione dei risultati\n",
    "print(\"Predizioni nel test set:\", y_pred)\n",
    "\n",
    "# Eseguiamo la valutazione delle metriche\n",
    "metrics = evaluate_metrics(y_test, y_pred, y_proba=y_proba)\n",
    "print(\"Metriche di valutazione:\\n\", metrics)\n",
    "\n",
    "# Scaled -> {'Accuracy': 0.6, 'Precision': 0.7, 'Recall': 0.583, 'F1': 0.636, 'MCC': 0.204, 'AUC_PR': 0.632, 'AUC_ROC': 0.542}\n",
    "# Non Scaled -> {'Accuracy': 0.7, 'Precision': 0.75, 'Recall': 0.75, 'F1': 0.75, 'MCC': 0.375, 'AUC_PR': 0.712, 'AUC_ROC': 0.656}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8036570a",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4756a12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predizioni nel test set: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 1 0 1 1 0 1 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 1 1 1 0 0 0 1 0\n",
      " 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1\n",
      " 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 1 0 1 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.843, 'Precision': 0.742, 'Recall': 0.407, 'F1': 0.526, 'MCC': 0.47, 'AUC_PR': 0.612, 'AUC_ROC': 0.806, 'PREC_N_SCORES': 0.531}\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.knn import KNN\n",
    "\n",
    "from rocket_functions import generate_kernels, apply_kernels\n",
    "\n",
    "# Genera kernel convoluzionali casuali\n",
    "input_length = X_train.shape[1]\n",
    "num_kernels = 1000\n",
    "kernels = generate_kernels(input_length, num_kernels)\n",
    "\n",
    "# Applica i kernel alle serie temporali\n",
    "features_train = apply_kernels(X_train2, kernels)\n",
    "features_test = apply_kernels(X_test2, kernels)\n",
    "\n",
    "\n",
    "# Addestramento del modello supervisionato\n",
    "model = KNN()\n",
    "model.fit(features_train)\n",
    "\n",
    "# Predizione delle anomalie nei dati di test\n",
    "y_pred = model.predict(features_test)\n",
    "y_proba = model.decision_function(features_test)\n",
    "\n",
    "# Visualizzazione dei risultati\n",
    "print(\"Predizioni nel test set:\", y_pred)\n",
    "\n",
    "# Eseguiamo la valutazione delle metriche\n",
    "metrics = evaluate_metrics(y_test, y_pred, y_proba)\n",
    "print(\"Metriche di valutazione:\\n\", metrics)\n",
    "# {'Accuracy': 0.845, 'Precision': 0.763, 'Recall': 0.398, 'F1': 0.523, 'MCC': 0.475, 'AUC_PR': 0.619, 'AUC_ROC': 0.811, 'PREC_N_SCORES': 0.54}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0eed09",
   "metadata": {},
   "source": [
    "### Regressione Logistica -> Classificatore lineare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2b43ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predizioni nel test set: [0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 0 1 0 0 1\n",
      " 0 0 1 0 0 1 1 0 1 1 1 0 1 0 0 0 0 0 0 1 1 1 1 1 0 0 1 0 1 1 1 0 1 1 1 1 1\n",
      " 0 1 0 1 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1 0 1 1 0 0 0\n",
      " 0 0 1 0 1 0 0 1 1 0 0 1 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0\n",
      " 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 1 1 1 1 0 0 0 0 1 0\n",
      " 0 1 0 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 1 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.972, 'Precision': 0.945, 'Recall': 0.92, 'F1': 0.933, 'MCC': 0.915, 'AUC_PR': 0.975, 'AUC_ROC': 0.993, 'PREC_N_SCORES': 0.92}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from rocket_functions import generate_kernels, apply_kernels\n",
    "\n",
    "# Genera kernel convoluzionali casuali\n",
    "input_length = X_train.shape[1]\n",
    "num_kernels = 1000\n",
    "kernels = generate_kernels(input_length, num_kernels)\n",
    "\n",
    "# Applica i kernel alle serie temporali\n",
    "features_train = apply_kernels(X_train2, kernels)\n",
    "features_test = apply_kernels(X_test2, kernels)\n",
    "\n",
    "\n",
    "# Addestramento del modello supervisionato\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(features_train, y_train)\n",
    "\n",
    "# Predizione delle anomalie nei dati di test\n",
    "y_pred = model.predict(features_test)\n",
    "y_proba = model.decision_function(features_test)\n",
    "\n",
    "# Visualizzazione dei risultati\n",
    "print(\"Predizioni nel test set:\", y_pred)\n",
    "\n",
    "# Eseguiamo la valutazione delle metriche\n",
    "metrics = evaluate_metrics(y_test, y_pred, y_proba)\n",
    "print(\"Metriche di valutazione:\\n\", metrics)\n",
    "# {'Accuracy': 0.977, 'Precision': 0.972, 'Recall': 0.92, 'F1': 0.945, 'MCC': 0.932, 'AUC_PR': 0.962, 'AUC_ROC': 0.984, 'PREC_N_SCORES': 0.929}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a539711",
   "metadata": {},
   "source": [
    "### Prova con Dettagli dal GitHub del Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5ee46bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predizioni nel test set: [0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 1 0 1 1 1 0 1 0 0 0 0 0 0 1 0 1 1 0 0 0 0 0 0 1 1 0 1 0 1 0 0\n",
      " 0 1 0 1 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0\n",
      " 0 0 1 0 0 0 0 1 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.879, 'Precision': 0.98, 'Recall': 0.442, 'F1': 0.61, 'MCC': 0.611, 'AUC_PR': 0.932, 'AUC_ROC': 0.965, 'PREC_N_SCORES': 0.92}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from rocket_functions import generate_kernels, apply_kernels\n",
    "\n",
    "def detect_anomalies_with_threshold(scores, threshold):\n",
    "    return (scores > threshold).astype(int)\n",
    "\n",
    "\n",
    "# Genera kernel convoluzionali casuali\n",
    "input_length = X_train.shape[1]\n",
    "num_kernels = 1000\n",
    "kernels = generate_kernels(input_length, num_kernels)\n",
    "\n",
    "# Applica i kernel alle serie temporali\n",
    "features_train = apply_kernels(X_train2, kernels)\n",
    "features_test = apply_kernels(X_test2, kernels)\n",
    "\n",
    "\n",
    "# Addestramento del modello supervisionato\n",
    "model = Ridge(alpha=1.0)\n",
    "model.fit(features_train, y_train)\n",
    "\n",
    "# Predizione delle anomalie nei dati di test\n",
    "anomaly_scores_test = model.predict(features_test)\n",
    "anomaly_scores_train = model.predict(features_train)\n",
    "\n",
    "# Rilevamento delle anomalie\n",
    "threshold = np.percentile(anomaly_scores_train , 95)\n",
    "anomaly_labels_train = detect_anomalies_with_threshold(anomaly_scores_train , threshold)\n",
    "anomaly_labels_test = detect_anomalies_with_threshold(anomaly_scores_test , threshold)\n",
    "\n",
    "# Visualizzazione dei risultati\n",
    "print(\"Predizioni nel test set:\", anomaly_labels_test)\n",
    "\n",
    "# Eseguiamo la valutazione delle metriche\n",
    "metrics = evaluate_metrics(y_test, anomaly_labels_test, y_proba=anomaly_scores_test)\n",
    "print(\"Metriche di valutazione:\\n\", metrics)\n",
    "#  {'Accuracy': 0.888, 'Precision': 0.966, 'Recall': 0.496, 'F1': 0.655, 'MCC': 0.644, 'AUC_PR': 0.922, 'AUC_ROC': 0.962, 'PREC_N_SCORES': 0.912}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4501c69b",
   "metadata": {},
   "source": [
    "## LogisticClassifierCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b5c5ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predizioni nel test set: [0 0 0 1 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 1 0 1 0 0 1\n",
      " 0 0 1 0 0 1 1 0 1 1 1 0 1 0 0 0 0 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 1\n",
      " 0 1 0 1 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      " 0 1 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 1 0 1 1 1 0 1 1 0 1 1 0 0 0\n",
      " 0 0 1 0 1 0 0 1 1 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 1 0 0 0 0 0\n",
      " 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 1 0 1 0 0 0 1 0\n",
      " 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 1 1 1 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Metriche di valutazione:\n",
      " {'Accuracy': 0.953, 'Precision': 0.958, 'Recall': 0.814, 'F1': 0.88, 'MCC': 0.856, 'AUC_PR': 0.951, 'AUC_ROC': 0.982, 'PREC_N_SCORES': 0.92}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from scipy.special import softmax\n",
    "\n",
    "from rocket_functions import generate_kernels, apply_kernels\n",
    "\n",
    "# Genera kernel convoluzionali casuali\n",
    "input_length = X_train.shape[1]\n",
    "num_kernels = 1000\n",
    "kernels = generate_kernels(input_length, num_kernels)\n",
    "\n",
    "# Applica i kernel alle serie temporali\n",
    "features_train = apply_kernels(X_train2, kernels)\n",
    "features_test = apply_kernels(X_test2, kernels)\n",
    "\n",
    "\n",
    "# Addestramento del modello supervisionato\n",
    "model = RidgeClassifierCV(alphas = np.logspace(-3, 3, 10))\n",
    "model.fit(features_train, y_train)\n",
    "\n",
    "# Predizione delle anomalie nei dati di test\n",
    "y_pred = model.predict(features_test)\n",
    "\n",
    "# Per separare multiclasse o monoclasse\n",
    "if  len(np.unique(y_test)) > 2:\n",
    "    y_proba = softmax(model.decision_function(features_test), axis=1)\n",
    "else:\n",
    "    y_proba = softmax(model.decision_function(features_test), axis=0)\n",
    "\n",
    "# Visualizzazione dei risultati\n",
    "print(\"Predizioni nel test set:\", y_pred)\n",
    "\n",
    "# Eseguiamo la valutazione delle metriche\n",
    "metrics = evaluate_metrics(y_test, y_pred, y_proba)\n",
    "print(\"Metriche di valutazione:\\n\", metrics)\n",
    "# {'Accuracy': 0.977, 'Precision': 0.972, 'Recall': 0.92, 'F1': 0.945, 'MCC': 0.932, 'AUC_PR': 0.962, 'AUC_ROC': 0.984, 'PREC_N_SCORES': 0.929}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6522d8c",
   "metadata": {},
   "source": [
    "# Test Rocket su NASA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a60ddcf",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12609375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentuale di anomalie rilevate: 5.442176870748299\n",
      "Predizioni nel test set: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "Predizioni nel test set: [ 27.39075515   6.73818551  30.98061762  21.36254965 118.34842727\n",
      "  12.07022744   4.98496023  15.30422813  11.66501984  28.08199317\n",
      "  15.84377621  19.2439204   15.15385179  26.62659354  32.54609538\n",
      "  44.63017342  46.03431525 201.55029622  14.89469191  24.63798363\n",
      "  39.08084855  10.79138293  13.23960034  25.4978436    9.74804133\n",
      "  17.665933    38.41646069 110.33655827  17.14146976  18.12435952\n",
      "  15.23798035 134.38722751   9.80981152  10.14225774  13.24224582\n",
      "  44.64392651  23.93383161  31.14978035  26.30618643  58.97880914\n",
      " 256.54798443   5.00440866  17.56927001  10.38270909  23.51199476\n",
      "  29.23675036  18.2885722    7.25394565  29.35229715  47.75608828\n",
      "  32.53691265  23.13234831  12.90875764  16.27963897  15.51566091\n",
      "  34.8221541   43.27771038  20.41988484   9.43933266  21.78916297\n",
      "  11.08380393  15.00102139  47.09357615  24.73439982  18.6808173\n",
      "  15.54429839  12.89903513  15.34119823  58.99959658  21.24731258\n",
      "  52.47152103   5.04404884  25.37441872  17.31002875   7.80313828\n",
      "  27.12066133  21.59329188  54.2720251    5.99765956  40.37541571\n",
      "  71.70672513  34.60341887  29.60177417  23.51241972  29.13324889\n",
      "   5.1670687   41.32155465  20.93160649  19.7450106   18.73722616\n",
      "   4.19552265  33.76765016  13.05573392   6.01490899  75.18196284\n",
      "  19.54929879  30.13474744  12.33259961  12.96383691  11.01478879\n",
      "  35.85475611  15.67839818   9.84445533  25.47453667  61.4445626\n",
      "  15.92541468  12.85212405  22.73149215  24.50339561   5.71785793\n",
      "  22.93328884  32.81111956  52.06910195  67.74476534  12.06244534\n",
      "  23.62886823   8.21759986  18.10202591  13.01418666  12.74664066\n",
      "  12.51575089  16.22201373  28.56556514   6.07506586  28.86648755\n",
      "  26.83282384  19.73917451  20.68428281  24.71082228  19.22723837\n",
      "  29.35821834  31.29230033  17.0386849   35.10177352  18.80669952\n",
      " 100.37102376  13.13668731  14.62999349  45.06436473  18.33637233\n",
      "  15.95079793   7.90718247  67.80369265  11.83782526  26.35865129\n",
      "  12.68149733  10.76208867]\n"
     ]
    }
   ],
   "source": [
    "from pyod.models.knn import KNN\n",
    "import numpy as np\n",
    "\n",
    "from rocket_functions import generate_kernels, apply_kernels\n",
    "\n",
    "# Genera kernel convoluzionali casuali\n",
    "input_length = X_train.shape[1]\n",
    "num_kernels = 1000\n",
    "kernels = generate_kernels(input_length, num_kernels)\n",
    "\n",
    "# Applica i kernel alle serie temporali\n",
    "features_train = apply_kernels(X_trainNASA2, kernels)\n",
    "features_test = apply_kernels(X_testNASA2, kernels)\n",
    "\n",
    "# RImozioni valori infiniti\n",
    "features_train = np.nan_to_num(features_train, nan=0.0, posinf=np.finfo(np.float32).max, neginf=np.finfo(np.float32).min)\n",
    "features_test= np.nan_to_num(features_test, nan=0.0, posinf=np.finfo(np.float32).max, neginf=np.finfo(np.float32).min)\n",
    "\n",
    "# Addestramento del modello supervisionato\n",
    "model = KNN()\n",
    "model.fit(features_train)\n",
    "\n",
    "# Predizione delle anomalie nei dati di test\n",
    "y_pred = model.predict(features_test)\n",
    "y_proba = model.decision_function(features_test)\n",
    "\n",
    "threshold = np.percentile(y_proba, 95)  # Soglia al 95° percentile\n",
    "predicted_anomalies = y_proba > threshold\n",
    "print(\"Percentuale di anomalie rilevate:\", predicted_anomalies.mean() * 100)\n",
    "\n",
    "# Visualizzazione dei risultati\n",
    "print(\"Predizioni nel test set:\", y_pred)\n",
    "print(\"Predizioni nel test set:\", y_proba)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a49c544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ci sono NaN: False\n",
      "Ci sono infiniti: True\n",
      "Massimo valore: 54.03164194612088\n",
      "Minimo valore: -inf\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"Ci sono NaN:\", np.isnan(features_train).any())\n",
    "print(\"Ci sono infiniti:\", np.isinf(features_train).any())\n",
    "print(\"Massimo valore:\", np.max(features_train))\n",
    "print(\"Minimo valore:\", np.min(features_train))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
