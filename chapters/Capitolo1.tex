\chapter{Introduzione}
\section{Rilevamento di Anomalie nei Satelliti}
Negli anni si è fatto sempre più presente il bisogno di esplorare lo spazio puntando sempre più lontano, studiando e comprendendo le regole e le strutture dell'universo partendo dall'osservazione di quest'ultimo. Con l'avanzare della tecnologia abbiamo potuto avvicinarci allo spazio tramite i satelliti posizionati in orbita intorno alla terra; questi sono circa 3500 attivi e si è fatta sempre più importante la richiesta di rilevare le anomalie di tali satelliti e comunicandole alla stazione di controllo, portando un aumento nello scambio di informazioni con un costo non irrilevante data la banda limitata.
Prima dell'avvento dell'intelligenza artificiale la rilevazione delle anomalie era lasciata ad un'analisi statica delle componenti, controllo di qualità con regole e soglie fisse, questo ovviamente portava ad avere un alta probabilità di ricevere tante segnalazioni di anomalie non rilevanti o date da cambiamenti nei rifermenti che creano un diverso ambiente vanificando i controlli statici.
Con l'intelligenza artificiale siamo riusciti ad avere un rilevamento adattivo in base all'ambiente che portava ad avere meno false anomalie ma anche questo approccio portava con se delle problematiche: l'algoritmo di intelligenza artificiale veniva allenato nel centro di controllo su dati reali e poi spedito sul satellite, ogni volta che bisognava cambiare i parametri dell'algoritmo bisognava ripetere tutto il processo dato che i satelliti all'interno hanno un processore molto poco prestante e quindi non sufficiente ad eseguire l'allenamento degli algoritmi, sprecando così tanta banda e molto tempo di trasmissione.

\section{NASA ed ESA}
Negli anni sia la NASA (National Aeronautics and Space Administration) che l'ESA (Agenzia Spaziale Europea) hanno pubblicato molteplici dataset contenenti dati reali di missioni e relativi benchmark sull'esecuzione di algoritmi di intelligenza artificiale per trovare il giusto compromesso tra efficacia e efficienza di un algoritmo, focalizzandosi maggiormente sul trovare l'algoritmo migliore per il rilevamento delle anomalie.

I dataset sono stati resi pubblici per incentivare la comunità a contribuire alla ricerca di nuove tecniche di monitoraggio e rilevamento delle anomalie, potendo usare dati reali.

\section{Obbiettivo}
Partendo dai dati proposti nei dataset e benchmark di ESA e NASA vogliamo analizzare quali test sono stati effettuati per capire come poter fare un ulteriore passo in avanti nel rilevamento continuo di anomalie cercando di rendere più efficiente un algoritmo già efficace in modo da poterlo allenare direttamente sul satellite, limitando così lo scambio di comunicazioni e restringendo ancora di più il rilevamento di false anomalie.

Tutto questo processo è dedito a trovare un algoritmo che abbia un giusto compromesso tra efficienza ed efficacia così che possa rilevare in modo corretto la maggioranza delle anomalie senza però avere un costo molto alto in termini di consumo di banda e di risorse del satellite.

Successivamente come prima cosa analizzeremo i dati a nostra disposizione relativi all'efficacia ed efficienza in termini di, rispettivamente, anomalie rilevate e tempo di esecuzione.
Dopo di ciò procederemo a scendere più nello specifico nell'implementazione di un \textbf{algoritmo "da scrivere il nome" } provando a renderlo più efficiente così da poter effettuare il training sui satelliti.
Per validare l'implementazione fatta portiamo a sostegno risultati ottenuti in primo luogo effettuando test e confronti tra un'implementazione standard e la nostra proposta mettendo a paragone le metriche di valutazione; in secondo luogo procederemo ad eseguire l'algoritmo su una macchina RaspberryPi, ovvero un mini-computer con un processore molto simile a quello di un satellite standard con poca capacità di calcolo, per avere un riscontro simile ad un'applicazione reale.